{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0ee00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0953333",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter bt pro roman'; color: rgb(0, 65, 75);\">\n",
    "<h1>\n",
    "Old GDP Revisions Datasets\n",
    "</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c323e5f0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter bt pro roman'; color: rgb(0, 65, 75);\">\n",
    "<h3>\n",
    "Documentation\n",
    "<br>\n",
    "____________________\n",
    "<br>\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc075bd7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'PT Serif Pro Book'; color: rgb(0, 65, 75); font-size: 16px;\">\n",
    "    Jason Cruz\n",
    "    <br>\n",
    "    <a href=\"mailto:jj.cruza@up.edu.pe\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">\n",
    "        jj.cruza@up.edu.pe\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0f23d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "This <span style=\"color: rgb(0, 65, 75);\">jupyter notebook</span> documents step-by-step the <b>construction of old datasets</b> for the project <b>'Revisions and Biases in Preliminary GDP Estimates in Peru'</b>.\n",
    "\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516ae6c",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Amaya; text-align: left; color: rgb(0, 65, 75); font-size:16px\">The following <b>outline is functional</b>. By utilising the provided buttons, users are able to enhance their experience by browsing this script.<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15589700",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #292929; padding: 10px; line-height: 1.5; font-family: 'PT Serif Pro Book';\">\n",
    "    <h2 style=\"text-align: left; color: #E0E0E0;\">\n",
    "        Outline\n",
    "    </h2>\n",
    "    <br>\n",
    "    <a href=\"#libraries\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        Libraries</a>\n",
    "    <br>\n",
    "    <a href=\"#setup\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        Initial set-up</a>\n",
    "    <br>\n",
    "    <a href=\"#1\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        1. </a>\n",
    "    <br>\n",
    "    <a href=\"#2\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        2. Data cleaning</a>\n",
    "    <br>\n",
    "    <a href=\"#2-1\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        2.1. Extracting tables and data cleanup.</a>\n",
    "    <br>\n",
    "    <a href=\"#2-2-1\" style=\"color: #94FFD8; font-size: 14px; margin-left: 40px;\">\n",
    "        2.2.1. Table 1. Extraction and cleaning of data from tables on monthly real GDP growth rates.</a>\n",
    "    <br>\n",
    "    <a href=\"#2-2-2\" style=\"color: #94FFD8; font-size: 14px; margin-left: 40px;\">\n",
    "        2.2.2. Table 2. Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.</a>\n",
    "    <br>\n",
    "    <a href=\"#3\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">3. Real-time data of Peru's GDP growth rates</a>\n",
    "    <br>\n",
    "    <a href=\"#3-1\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        3.1. Annual vintages concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-2\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        3.2. Quarterly vintages concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-3\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        3.3. Monthly vintages concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-4\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        3.4. Loading SQL.</a>\n",
    "    <br>\n",
    "    <a href=\"#4\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">4. GDP final revision dataset</a>\n",
    "    <br>\n",
    "    <a href=\"#4-1\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        4.1. Annual revisions.</a>\n",
    "    <br>\n",
    "    <a href=\"#4-2\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        4.2. Quarterly revisions.</a>\n",
    "    <br>\n",
    "    <a href=\"#4-3\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        4.3. Monthly revisions.</a>\n",
    "    <br>\n",
    "    <a href=\"#5\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">5. Uploading data to SQL</a>\n",
    "    <br>\n",
    "    <a href=\"#5-1\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        5.1. Vintages.</a>\n",
    "    <br>\n",
    "    <a href=\"#5-2\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        5.2. Revisions.</a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a7c4d6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    Any questions or issues regarding the coding, please email Jason Cruz <a href=\"mailto:jj.cruza@alum.up.edu.pe\" style=\"color: rgb(0, 153, 123); text-decoration: none;\"><span style=\"font-size: 24px;\">&#x2709;</span>\n",
    "    </a>.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff8772",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    If you don't have the libraries below, please use the following code (as example) to install the required libraries.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f517f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea21974a",
   "metadata": {},
   "source": [
    "<div id=\"libraries\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752d8fe",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    <h2>\n",
    "    Libraries\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00415459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data cleaning\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 1.1. A brief documentation on issus in the table information of the PDFs\n",
    "\n",
    "from PIL import Image  # Used for opening, manipulating, and saving image files.\n",
    "import matplotlib.pyplot as plt  # Used for creating static, animated, and interactive visualizations.\n",
    "\n",
    "# 1.2. Extracting tables and data cleanup\n",
    "\n",
    "\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import unicodedata  # For manipulating Unicode data\n",
    "import re  # For regular expressions operations\n",
    "from datetime import datetime  # For working with dates and times\n",
    "import locale  # For locale-specific formatting of numbers, dates, and currencies\n",
    "import numpy as np\n",
    "import unidecode\n",
    "\n",
    "# 1.2.1. Table 1. Extraction and cleaning of data from tables on monthly real GDP growth rates.\n",
    "\n",
    "import tabula  # Used to extract tables from PDF files into pandas DataFrames\n",
    "from tkinter import Tk, messagebox, TOP, YES, NO  # Used for creating graphical user interfaces\n",
    "from sqlalchemy import create_engine  # Used for connecting to and interacting with SQL databases\n",
    "\n",
    "# 1.2.2. Table 2. Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.\n",
    "\n",
    "import roman\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# 2. Real-time data of Peru's GDP growth rates\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import psycopg2  # For interacting with PostgreSQL databases\n",
    "from sqlalchemy import create_engine, text  # For creating and executing SQL queries using SQLAlchemy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8167e",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d73758",
   "metadata": {},
   "source": [
    "<div id=\"setup\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4fbf38",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark;\">\n",
    "    <h2>\n",
    "    Initial set-up\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb31bc",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\"> The following code lines will create folders in your current path, call them to import and export your outputs. <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1defdb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4457bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc19bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "450c067b",
   "metadata": {},
   "source": [
    "# Data clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdp_revisions_datasets_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc7602",
   "metadata": {},
   "source": [
    "# Exclusivas para tablas entregadas por el BCRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c61dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd86a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_nombres_columnas(df):\n",
    "    # Eliminar tildes y convertir a minúsculas los nombres de las columnas\n",
    "    df.columns = df.columns.str.lower()\n",
    "    # Only normalize string column names\n",
    "    df.columns = [unicodedata.normalize('NFKD', col).encode('ASCII', 'ignore').decode('utf-8') if isinstance(col, str) else col for col in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8061b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajustar_nombres_columnas(df):\n",
    "    # Comprobar que la primera observación de la primera columna sea NaN\n",
    "    if pd.isna(df.iloc[0, 0]) and pd.isna(df.iloc[0, -1]):\n",
    "        # Verificar los nombres de las columnas\n",
    "        if \"sectores economicos\" in df.columns[0] and \"economic sectors\" in df.columns[-1]:\n",
    "            # Reemplazar NaN por los nombres de las columnas correspondientes\n",
    "            df.iloc[0, 0] = \"sectores economicos\"\n",
    "            df.iloc[0, -1] = \"economic sectors\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e73d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounding_values(df, decimales=1):\n",
    "    # Iterar sobre todas las columnas del DataFrame\n",
    "    for col in df.columns:\n",
    "        # Verificar si la columna es de tipo float\n",
    "        if df[col].dtype == 'float64':\n",
    "            # Redondear los valores de la columna al número de decimales especificado\n",
    "            df[col] = df[col].round(decimales)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f3049",
   "metadata": {},
   "source": [
    "# Importando csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69dd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Substitua 'arquivo.csv' pelo caminho do seu arquivo CSV\n",
    "path = r'C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\coding_training\\old_dataset\\raw_data\\tabla 2\\2010\\ns-07-2010.csv'\n",
    "\n",
    "# Carrega o arquivo CSV para um DataFrame\n",
    "df = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "# Exibe as primeiras linhas do DataFrame para verificar se os dados foram carregados corretamente\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b387f",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ffb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Establecer la localización en español\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Diccionario para almacenar los DataFrames generados\n",
    "old_dataframes_dict_1 = {}\n",
    "\n",
    "# Ruta del archivo de registro de carpetas procesadas\n",
    "registro_path = 'dataframes_record/old_carpetas_procesadas_1.txt'\n",
    "\n",
    "# Función para corregir los nombres de los meses\n",
    "def corregir_nombre_mes(mes):\n",
    "    meses_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Agrega más mapeos si es necesario para otros nombres de meses\n",
    "    }\n",
    "    return meses_mapping.get(mes, mes)\n",
    "\n",
    "def registrar_carpeta_procesada(carpeta, num_archivos_procesados):\n",
    "    with open(registro_path, 'a') as file:\n",
    "        file.write(f\"{carpeta}:{num_archivos_procesados}\\n\")\n",
    "\n",
    "def carpeta_procesada(carpeta):\n",
    "    if not os.path.exists(registro_path):\n",
    "        return False\n",
    "    with open(registro_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(carpeta):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def obtener_fecha(df, engine):\n",
    "    id_ns = df['id_ns'].iloc[0]\n",
    "    year = df['year'].iloc[0]\n",
    "    query = f\"SELECT date FROM dates_growth_rates WHERE id_ns = '{id_ns}' AND year = '{year}';\"\n",
    "    fecha = pd.read_sql(query, engine)\n",
    "    return fecha.iloc[0, 0] if not fecha.empty else None\n",
    "\n",
    "def procesar_archivo_csv(csv_path, engine):\n",
    "    old_tables_dict_1 = {}  # Diccionario local para cada archivo CSV\n",
    "    table_counter = 1\n",
    "\n",
    "    filename = os.path.basename(csv_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No se encontraron coincidencias para id_ns y year en el nombre del archivo:\", filename)\n",
    "        return None, None, None, None\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(csv_path))[0].replace('-', '_')\n",
    "\n",
    "    df = pd.read_csv(csv_path, delimiter=';')\n",
    "    \n",
    "    nombre_dataframe = f\"{new_filename}_{table_counter}\"\n",
    "    old_tables_dict_1[nombre_dataframe] = df.copy()\n",
    "\n",
    "    # Aplicar las funciones de limpieza a una copia del DataFrame\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Funciones de limpieza (ajustar según sea necesario)\n",
    "    df_clean = limpiar_nombres_columnas(df_clean)\n",
    "    df_clean = ajustar_nombres_columnas(df_clean)\n",
    "    df_clean = drop_rare_caracter_row(df_clean)\n",
    "    df_clean = drop_nan_rows(df_clean)\n",
    "    df_clean = drop_nan_columns(df_clean)\n",
    "    df_clean = reset_index(df_clean)\n",
    "    df_clean = remove_digit_slash(df_clean)\n",
    "    df_clean = replace_var_perc_first_column(df_clean)\n",
    "    df_clean = replace_var_perc_last_columns(df_clean)\n",
    "    df_clean = replace_number_moving_average(df_clean)\n",
    "    df_clean = relocate_last_column(df_clean)\n",
    "    df_clean = clean_first_row(df_clean)\n",
    "    df_clean = find_year_column(df_clean)\n",
    "    year_columns = extract_years(df_clean)\n",
    "    df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "    df_clean = first_row_columns(df_clean)\n",
    "    df_clean = clean_columns_values(df_clean)\n",
    "    df_clean = convert_float(df_clean)\n",
    "    df_clean = replace_set_sep(df_clean)\n",
    "    df_clean = spaces_se_es(df_clean)\n",
    "    df_clean = replace_services(df_clean)\n",
    "    df_clean = rounding_values(df_clean, decimales=1)\n",
    "\n",
    "    # Añadir la columna 'year' al DataFrame limpio\n",
    "    df_clean.insert(0, 'year', year)\n",
    "    \n",
    "    # Añadir la columna 'id_ns' al DataFrame limpio\n",
    "    df_clean.insert(1, 'id_ns', id_ns)\n",
    "    \n",
    "    # Obtener la fecha correspondiente de la base de datos\n",
    "    fecha = obtener_fecha(df_clean, engine)\n",
    "    if fecha:\n",
    "        # Añadir la columna 'date' al DataFrame limpio\n",
    "        df_clean.insert(2, 'date', fecha)\n",
    "    else:\n",
    "        print(\"No se encontró fecha en la base de datos para id_ns:\", id_ns, \"y year:\", year)\n",
    "    \n",
    "    # Almacenar DataFrame limpio en old_dataframes_dict_1\n",
    "    old_dataframes_dict_1[nombre_dataframe] = df_clean\n",
    "\n",
    "    return id_ns, year, old_tables_dict_1\n",
    "\n",
    "def procesar_carpeta(carpeta, engine):\n",
    "    print(f\"Procesando la carpeta {os.path.basename(carpeta)}\")\n",
    "    csv_files = [os.path.join(carpeta, f) for f in os.listdir(carpeta) if f.endswith('.csv')]\n",
    "\n",
    "    num_csv_procesados = 0\n",
    "    num_dataframes_generados = 0\n",
    "\n",
    "    table_counter = 1  # Inicializar el contador de tabla aquí\n",
    "    old_tables_dict_1 = {}  # Declarar old_tables_dict_1 fuera del bucle principal\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        id_ns, year, tables_dict_temp = procesar_archivo_csv(csv_file, engine)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for nombre_df, df in tables_dict_temp.items():\n",
    "                nombre_archivo = os.path.splitext(os.path.basename(csv_file))[0].replace('-', '_')\n",
    "                nombre_df = f\"{nombre_archivo}_{table_counter}\"\n",
    "                \n",
    "                # Almacenar DataFrame sin procesar en old_tables_dict_1\n",
    "                old_tables_dict_1[nombre_df] = df.copy()\n",
    "                \n",
    "                # Procesar y limpiar el DataFrame\n",
    "                df_clean = df.copy()\n",
    "                \n",
    "                # Aplicar las funciones de limpieza a una copia del DataFrame\n",
    "                df_clean = limpiar_nombres_columnas(df_clean)\n",
    "                df_clean = ajustar_nombres_columnas(df_clean)\n",
    "                df_clean = drop_rare_caracter_row(df_clean)\n",
    "                df_clean = drop_nan_rows(df_clean)\n",
    "                df_clean = drop_nan_columns(df_clean)\n",
    "                df_clean = reset_index(df_clean)\n",
    "                df_clean = remove_digit_slash(df_clean)\n",
    "                df_clean = replace_var_perc_first_column(df_clean)\n",
    "                df_clean = replace_var_perc_last_columns(df_clean)\n",
    "                df_clean = replace_number_moving_average(df_clean)\n",
    "                df_clean = relocate_last_column(df_clean)\n",
    "                df_clean = clean_first_row(df_clean)\n",
    "                df_clean = find_year_column(df_clean)\n",
    "                year_columns = extract_years(df_clean)\n",
    "                df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "                df_clean = first_row_columns(df_clean)\n",
    "                df_clean = clean_columns_values(df_clean)\n",
    "                df_clean = convert_float(df_clean)\n",
    "                df_clean = replace_set_sep(df_clean)\n",
    "                df_clean = spaces_se_es(df_clean)\n",
    "                df_clean = replace_services(df_clean)\n",
    "                df_clean = rounding_values(df_clean, decimales=1)\n",
    "                \n",
    "                # Añadir la columna 'year' al DataFrame limpio\n",
    "                df_clean.insert(0, 'year', year)\n",
    "                \n",
    "                # Añadir la columna 'id_ns' al DataFrame limpio\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                \n",
    "                # Obtener la fecha correspondiente de la base de datos\n",
    "                fecha = obtener_fecha(df_clean, engine)\n",
    "                if fecha:\n",
    "                    # Añadir la columna 'date' al DataFrame limpio\n",
    "                    df_clean.insert(2, 'date', fecha)\n",
    "                else:\n",
    "                    print(\"No se encontró fecha en la base de datos para id_ns:\", id_ns, \"y year:\", year)\n",
    "                \n",
    "                # Almacenar DataFrame limpio en old_dataframes_dict_1\n",
    "                old_dataframes_dict_1[nombre_df] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. El dataframe generado para el archivo {csv_file} es: {nombre_df}')\n",
    "                num_dataframes_generados += 1\n",
    "                table_counter += 1  # Incrementar el contador de tabla aquí\n",
    "        \n",
    "        num_csv_procesados += 1  # Incrementar el número de CSVs procesados por cada archivo en la carpeta\n",
    "\n",
    "    return num_csv_procesados, num_dataframes_generados, old_tables_dict_1\n",
    "\n",
    "def procesar_carpetas():\n",
    "    base_folder = r'C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\coding_training\\old_dataset\\raw_data\\tabla 1'\n",
    "    carpetas = [os.path.join(base_folder, d) for d in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, d)) and re.match(r'\\d{4}', d)]\n",
    "    \n",
    "    old_tables_dict_1 = {}  # Inicializar old_tables_dict_1 aquí\n",
    "    \n",
    "    for carpeta in carpetas:\n",
    "        if carpeta_procesada(carpeta):\n",
    "            print(f\"La carpeta {carpeta} ya ha sido procesada.\")\n",
    "            continue\n",
    "        \n",
    "        num_csv_procesados, num_dataframes_generados, tables_dict_temp = procesar_carpeta(carpeta, engine)\n",
    "        \n",
    "        # Actualizar old_tables_dict_1 con los valores devueltos de procesar_carpeta()\n",
    "        old_tables_dict_1.update(tables_dict_temp)\n",
    "        \n",
    "        registrar_carpeta_procesada(carpeta, num_csv_procesados)\n",
    "\n",
    "        # Preguntar al usuario si desea continuar con la siguiente carpeta\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Para asegurar que la ventana esté en primer plano\n",
    "        \n",
    "        mensaje = f\"Se han generado {num_dataframes_generados} dataframes en la carpeta {carpeta}. ¿Deseas continuar con la siguiente carpeta?\"\n",
    "        continuar = messagebox.askyesno(\"Continuar\", mensaje)\n",
    "        root.destroy()\n",
    "\n",
    "        if not continuar:\n",
    "            print(\"Procesamiento detenido por el usuario.\")\n",
    "            break  # Romper el bucle for si el usuario decide no continuar\n",
    "\n",
    "    print(\"Procesamiento completado para todas las carpetas.\")  # Add a message to indicate completion\n",
    "\n",
    "    return old_tables_dict_1  # Devolver old_tables_dict_1 al final de la función\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get environment variables\n",
    "    user = os.environ.get('CIUP_SQL_USER')\n",
    "    password = os.environ.get('CIUP_SQL_PASS')\n",
    "    host = os.environ.get('CIUP_SQL_HOST')\n",
    "    port = 5432\n",
    "    database = 'gdp_revisions_datasets'\n",
    "\n",
    "    # Check if all environment variables are defined\n",
    "    if not all([host, user, password]):\n",
    "        raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "    # Create connection string\n",
    "    connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    old_tables_dict_1 = procesar_carpetas()  # Capturar el valor devuelto de procesar_carpetas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda07520",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tables_dict_1['ns_03_2010_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dataframes_dict_1['ns_03_2010_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e77290",
   "metadata": {},
   "source": [
    "# TABLA 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744448b",
   "metadata": {},
   "source": [
    "# Cargando desde excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d919b7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Substitua 'arquivo.csv' pelo caminho do seu arquivo CSV\n",
    "path = r'C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\coding_training\\old_dataset\\raw_data\\tabla 1\\table_1\\00_1.csv'\n",
    "\n",
    "# Carrega o arquivo CSV para um DataFrame\n",
    "df_clean = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "# Exibe as primeiras linhas do DataFrame para verificar se os dados foram carregados corretamente\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc75a9b",
   "metadata": {},
   "source": [
    "# Total by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90766688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_total_with_year(df):\n",
    "    # Reemplazar 'TOTAL' con 'AÑO' en la primera fila\n",
    "    df.iloc[0] = df.iloc[0].apply(lambda x: 'AÑO' if \"TOTAL\" in str(x) else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10036cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = replace_total_with_year(df_clean)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774512d6",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab041892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import locale\n",
    "from tkinter import Tk, messagebox, TOP, YES, NO\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Establecer la localización en español\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Diccionario para almacenar los DataFrames generados\n",
    "old_dataframes_dict_2 = {}\n",
    "\n",
    "# Ruta del archivo de registro de carpetas procesadas\n",
    "registro_path = 'dataframes_record/old_carpetas_procesadas_2.txt'\n",
    "\n",
    "# Función para corregir los nombres de los meses\n",
    "def corregir_nombre_mes(mes):\n",
    "    meses_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Agrega más mapeos si es necesario para otros nombres de meses\n",
    "    }\n",
    "    return meses_mapping.get(mes, mes)\n",
    "\n",
    "def registrar_carpeta_procesada(carpeta, num_archivos_procesados):\n",
    "    with open(registro_path, 'a') as file:\n",
    "        file.write(f\"{carpeta}:{num_archivos_procesados}\\n\")\n",
    "\n",
    "def carpeta_procesada(carpeta):\n",
    "    if not os.path.exists(registro_path):\n",
    "        return False\n",
    "    with open(registro_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(carpeta):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def obtener_fecha(df, engine):\n",
    "    id_ns = df['id_ns'].iloc[0]\n",
    "    year = df['year'].iloc[0]\n",
    "    query = f\"SELECT date FROM dates_growth_rates WHERE id_ns = '{id_ns}' AND year = '{year}';\"\n",
    "    fecha = pd.read_sql(query, engine)\n",
    "    return fecha.iloc[0, 0] if not fecha.empty else None\n",
    "\n",
    "def procesar_archivo_csv(csv_path, engine):\n",
    "    old_tables_dict_2 = {}  # Diccionario local para cada archivo CSV\n",
    "    table_counter = 1\n",
    "\n",
    "    filename = os.path.basename(csv_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No se encontraron coincidencias para id_ns y year en el nombre del archivo:\", filename)\n",
    "        return None, None, None, None\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(csv_path))[0].replace('-', '_')\n",
    "\n",
    "    df = pd.read_csv(csv_path, delimiter=';')\n",
    "    \n",
    "    nombre_dataframe = f\"{new_filename}_{table_counter}\"\n",
    "    old_tables_dict_2[nombre_dataframe] = df.copy()\n",
    "\n",
    "    # Aplicar las funciones de limpieza a una copia del DataFrame\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Funciones de limpieza (ajustar según sea necesario)\n",
    "    df_clean = replace_total_with_year(df_clean)\n",
    "    df_clean = drop_nan_rows(df_clean)\n",
    "    year_columns = extract_years(df_clean)\n",
    "    df_clean = roman_arabic(df_clean)\n",
    "    df_clean = fix_duplicates(df_clean)\n",
    "    df_clean = relocate_last_column(df_clean)\n",
    "    df_clean = replace_first_row_nan(df_clean)\n",
    "    df_clean = clean_first_row(df_clean)\n",
    "    df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "    df_clean = reset_index(df_clean)\n",
    "    df_clean = first_row_columns(df_clean)\n",
    "    df_clean = clean_columns_values(df_clean)\n",
    "    df_clean = reset_index(df_clean)\n",
    "    df_clean = convert_float(df_clean)\n",
    "    df_clean = replace_set_sep(df_clean)\n",
    "    df_clean = spaces_se_es(df_clean)\n",
    "    df_clean = replace_services(df_clean)\n",
    "    df_clean = rounding_values(df_clean, decimales=1)\n",
    "\n",
    "    # Añadir la columna 'year' al DataFrame limpio\n",
    "    df_clean.insert(0, 'year', year)\n",
    "    \n",
    "    # Añadir la columna 'id_ns' al DataFrame limpio\n",
    "    df_clean.insert(1, 'id_ns', id_ns)\n",
    "    \n",
    "    # Obtener la fecha correspondiente de la base de datos\n",
    "    fecha = obtener_fecha(df_clean, engine)\n",
    "    if fecha:\n",
    "        # Añadir la columna 'date' al DataFrame limpio\n",
    "        df_clean.insert(2, 'date', fecha)\n",
    "    else:\n",
    "        print(\"No se encontró fecha en la base de datos para id_ns:\", id_ns, \"y year:\", year)\n",
    "    \n",
    "    # Almacenar DataFrame limpio en old_dataframes_dict_2\n",
    "    old_dataframes_dict_2[nombre_dataframe] = df_clean\n",
    "\n",
    "    return id_ns, year, old_tables_dict_2\n",
    "\n",
    "def procesar_carpeta(carpeta, engine):\n",
    "    print(f\"Procesando la carpeta {os.path.basename(carpeta)}\")\n",
    "    csv_files = [os.path.join(carpeta, f) for f in os.listdir(carpeta) if f.endswith('.csv')]\n",
    "\n",
    "    num_csv_procesados = 0\n",
    "    num_dataframes_generados = 0\n",
    "\n",
    "    table_counter = 1  # Inicializar el contador de tabla aquí\n",
    "    old_tables_dict_2 = {}  # Declarar old_tables_dict_2 fuera del bucle principal\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        id_ns, year, tables_dict_temp = procesar_archivo_csv(csv_file, engine)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for nombre_df, df in tables_dict_temp.items():\n",
    "                nombre_archivo = os.path.splitext(os.path.basename(csv_file))[0].replace('-', '_')\n",
    "                nombre_df = f\"{nombre_archivo}_{table_counter}\"\n",
    "                \n",
    "                # Almacenar DataFrame sin procesar en old_tables_dict_2\n",
    "                old_tables_dict_2[nombre_df] = df.copy()\n",
    "                \n",
    "                # Procesar y limpiar el DataFrame\n",
    "                df_clean = df.copy()\n",
    "                \n",
    "                # Aplicar las funciones de limpieza a una copia del DataFrame\n",
    "                df_clean = replace_total_with_year(df_clean)\n",
    "                df_clean = drop_nan_rows(df_clean)\n",
    "                year_columns = extract_years(df_clean)\n",
    "                df_clean = roman_arabic(df_clean)\n",
    "                df_clean = fix_duplicates(df_clean)\n",
    "                df_clean = relocate_last_column(df_clean)\n",
    "                df_clean = replace_first_row_nan(df_clean)\n",
    "                df_clean = clean_first_row(df_clean)\n",
    "                df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "                df_clean = reset_index(df_clean)\n",
    "                df_clean = first_row_columns(df_clean)\n",
    "                df_clean = clean_columns_values(df_clean)\n",
    "                df_clean = reset_index(df_clean)\n",
    "                df_clean = convert_float(df_clean)\n",
    "                df_clean = replace_set_sep(df_clean)\n",
    "                df_clean = spaces_se_es(df_clean)\n",
    "                df_clean = replace_services(df_clean)\n",
    "                df_clean = rounding_values(df_clean, decimales=1)\n",
    "                \n",
    "                # Añadir la columna 'year' al DataFrame limpio\n",
    "                df_clean.insert(0, 'year', year)\n",
    "                \n",
    "                # Añadir la columna 'id_ns' al DataFrame limpio\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                \n",
    "                # Obtener la fecha correspondiente de la base de datos\n",
    "                fecha = obtener_fecha(df_clean, engine)\n",
    "                if fecha:\n",
    "                    # Añadir la columna 'date' al DataFrame limpio\n",
    "                    df_clean.insert(2, 'date', fecha)\n",
    "                else:\n",
    "                    print(\"No se encontró fecha en la base de datos para id_ns:\", id_ns, \"y year:\", year)\n",
    "                \n",
    "                # Almacenar DataFrame limpio en old_dataframes_dict_2\n",
    "                old_dataframes_dict_2[nombre_df] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. El dataframe generado para el archivo {csv_file} es: {nombre_df}')\n",
    "                num_dataframes_generados += 1\n",
    "                table_counter += 1  # Incrementar el contador de tabla aquí\n",
    "        \n",
    "        num_csv_procesados += 1  # Incrementar el número de CSVs procesados por cada archivo en la carpeta\n",
    "\n",
    "    return num_csv_procesados, num_dataframes_generados, old_tables_dict_2\n",
    "\n",
    "def procesar_carpetas():\n",
    "    base_folder = r'C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\coding_training\\old_dataset\\raw_data\\tabla 2'\n",
    "    carpetas = [os.path.join(base_folder, d) for d in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, d)) and re.match(r'\\d{4}', d)]\n",
    "    \n",
    "    old_tables_dict_2 = {}  # Inicializar old_tables_dict_2 aquí\n",
    "    \n",
    "    for carpeta in carpetas:\n",
    "        if carpeta_procesada(carpeta):\n",
    "            print(f\"La carpeta {carpeta} ya ha sido procesada.\")\n",
    "            continue\n",
    "        \n",
    "        num_csv_procesados, num_dataframes_generados, tables_dict_temp = procesar_carpeta(carpeta, engine)\n",
    "        \n",
    "        # Actualizar old_tables_dict_2 con los valores devueltos de procesar_carpeta()\n",
    "        old_tables_dict_2.update(tables_dict_temp)\n",
    "        \n",
    "        registrar_carpeta_procesada(carpeta, num_csv_procesados)\n",
    "\n",
    "        # Preguntar al usuario si desea continuar con la siguiente carpeta\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Para asegurar que la ventana esté en primer plano\n",
    "        \n",
    "        mensaje = f\"Se han generado {num_dataframes_generados} dataframes en la carpeta {carpeta}. ¿Deseas continuar con la siguiente carpeta?\"\n",
    "        continuar = messagebox.askyesno(\"Continuar\", mensaje)\n",
    "        root.destroy()\n",
    "\n",
    "        if not continuar:\n",
    "            print(\"Procesamiento detenido por el usuario.\")\n",
    "            break  # Romper el bucle for si el usuario decide no continuar\n",
    "\n",
    "    print(\"Procesamiento completado para todas las carpetas.\")  # Add a message to indicate completion\n",
    "\n",
    "    return old_tables_dict_2  # Devolver old_tables_dict_2 al final de la función\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Get environment variables\n",
    "    user = os.environ.get('CIUP_SQL_USER')\n",
    "    password = os.environ.get('CIUP_SQL_PASS')\n",
    "    host = os.environ.get('CIUP_SQL_HOST')\n",
    "    port = 5432\n",
    "    database = 'gdp_revisions_datasets'\n",
    "\n",
    "    # Check if all environment variables are defined\n",
    "    if not all([host, user, password]):\n",
    "        raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "    # Create connection string\n",
    "    connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "    old_tables_dict_2 = procesar_carpetas()  # Capturar el valor devuelto de procesar_carpetas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6ccdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb3390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tables_dict_2['ns_04_2010_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b05cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dataframes_dict_2['ns_32_2010_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f46c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de81e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f889c679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da498df2",
   "metadata": {},
   "source": [
    "# TO SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Substitua 'arquivo.csv' pelo caminho do seu arquivo CSV\n",
    "path = r'C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\coding_training\\to_sql\\old_raw_data_delivered.csv'\n",
    "\n",
    "# Carrega o arquivo CSV para um DataFrame\n",
    "old_raw_data_delivered = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "# Exibe as primeiras linhas do DataFrame para verificar se os dados foram carregados corretamente\n",
    "old_raw_data_delivered.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746f82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8b508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef0eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f615e2a",
   "metadata": {},
   "source": [
    "# Duplicate table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a3a37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Obtener variables de entorno\n",
    "user = os.environ.get('CIUP_SQL_USER')\n",
    "password = os.environ.get('CIUP_SQL_PASS')\n",
    "host = os.environ.get('CIUP_SQL_HOST')\n",
    "port = 5432\n",
    "database = 'gdp_revisions_datasets'\n",
    "\n",
    "# Verificar si todas las variables de entorno están definidas\n",
    "if not all([host, user, password]):\n",
    "    raise ValueError(\"Faltan algunas variables de entorno (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "# Crear la cadena de conexión\n",
    "connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "# Crear el motor de SQLAlchemy\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Definir la consulta SQL para importar datos\n",
    "query = f\"SELECT * FROM old_raw_data_delivered\"\n",
    "\n",
    "# Importar los datos a un DataFrame de pandas\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Ruta base donde están las carpetas de años\n",
    "base_path = r\"C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\coding_training\\old_dataset\\raw_data\\tabla 1\"\n",
    "\n",
    "# Función para duplicar archivos\n",
    "def duplicate_files(year, df_year, base_path):\n",
    "    year_path = os.path.join(base_path, str(year))\n",
    "    files = sorted([f for f in os.listdir(year_path) if f.endswith('.csv')])\n",
    "\n",
    "    # Obtener los archivos existentes\n",
    "    existing_files = {int(f.split('-')[1]): f for f in files}\n",
    "    \n",
    "    # Obtener el último archivo del año anterior\n",
    "    if year > 1994:\n",
    "        prev_year_path = os.path.join(base_path, str(year - 1))\n",
    "        prev_files = sorted([f for f in os.listdir(prev_year_path) if f.endswith('.csv')])\n",
    "        if prev_files:\n",
    "            last_prev_file = prev_files[-1]\n",
    "            last_prev_id_ns = int(last_prev_file.split('-')[1])\n",
    "        else:\n",
    "            last_prev_file = None\n",
    "            last_prev_id_ns = None\n",
    "    else:\n",
    "        last_prev_file = None\n",
    "        last_prev_id_ns = None\n",
    "\n",
    "    # Inicializar variables para duplicación\n",
    "    last_existing_file = last_prev_file\n",
    "    last_existing_id_ns = last_prev_id_ns\n",
    "\n",
    "    for index, row in df_year.iterrows():\n",
    "        id_ns = row['id_ns']\n",
    "        if row['delivered_1'] == 1:\n",
    "            last_existing_file = existing_files[id_ns]\n",
    "            last_existing_id_ns = id_ns\n",
    "        else:\n",
    "            # Crear nombre del nuevo archivo duplicado\n",
    "            new_file_name = f\"ns-{id_ns:02d}-{year}.csv\"\n",
    "            new_file_path = os.path.join(year_path, new_file_name)\n",
    "\n",
    "            # Duplicar archivo\n",
    "            if last_existing_file:\n",
    "                if last_existing_file in existing_files.values():\n",
    "                    src_file_path = os.path.join(year_path, last_existing_file)\n",
    "                else:\n",
    "                    src_file_path = os.path.join(prev_year_path, last_existing_file)\n",
    "                shutil.copy(src_file_path, new_file_path)\n",
    "                print(f\"Duplicated {last_existing_file} to {new_file_name}\")\n",
    "            else:\n",
    "                print(f\"No existing file to duplicate for {new_file_name}\")\n",
    "\n",
    "# Procesar cada año\n",
    "for year in range(2010, 2013):\n",
    "    df_year = df[df['year'] == year]\n",
    "    duplicate_files(year, df_year, base_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6cbc23",
   "metadata": {},
   "source": [
    "# Duplicate table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840ed1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Obtener variables de entorno\n",
    "user = os.environ.get('CIUP_SQL_USER')\n",
    "password = os.environ.get('CIUP_SQL_PASS')\n",
    "host = os.environ.get('CIUP_SQL_HOST')\n",
    "port = 5432\n",
    "database = 'gdp_revisions_datasets'\n",
    "\n",
    "# Verificar si todas las variables de entorno están definidas\n",
    "if not all([host, user, password]):\n",
    "    raise ValueError(\"Faltan algunas variables de entorno (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "# Crear la cadena de conexión\n",
    "connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "# Crear el motor de SQLAlchemy\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Definir la consulta SQL para importar datos\n",
    "query = f\"SELECT * FROM old_raw_data_delivered\"\n",
    "\n",
    "# Importar los datos a un DataFrame de pandas\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Ruta base donde están las carpetas de años\n",
    "base_path = r\"C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\coding_training\\old_dataset\\raw_data\\tabla 2\"\n",
    "\n",
    "# Función para duplicar archivos\n",
    "def duplicate_files(year, df_year, base_path):\n",
    "    year_path = os.path.join(base_path, str(year))\n",
    "    files = sorted([f for f in os.listdir(year_path) if f.endswith('.csv')])\n",
    "\n",
    "    # Obtener los archivos existentes\n",
    "    existing_files = {int(f.split('-')[1]): f for f in files}\n",
    "    \n",
    "    # Obtener el último archivo del año anterior\n",
    "    if year > 1994:\n",
    "        prev_year_path = os.path.join(base_path, str(year - 1))\n",
    "        prev_files = sorted([f for f in os.listdir(prev_year_path) if f.endswith('.csv')])\n",
    "        if prev_files:\n",
    "            last_prev_file = prev_files[-1]\n",
    "            last_prev_id_ns = int(last_prev_file.split('-')[1])\n",
    "        else:\n",
    "            last_prev_file = None\n",
    "            last_prev_id_ns = None\n",
    "    else:\n",
    "        last_prev_file = None\n",
    "        last_prev_id_ns = None\n",
    "\n",
    "    # Inicializar variables para duplicación\n",
    "    last_existing_file = last_prev_file\n",
    "    last_existing_id_ns = last_prev_id_ns\n",
    "\n",
    "    for index, row in df_year.iterrows():\n",
    "        id_ns = row['id_ns']\n",
    "        if row['delivered_2'] == 1:\n",
    "            last_existing_file = existing_files[id_ns]\n",
    "            last_existing_id_ns = id_ns\n",
    "        else:\n",
    "            # Crear nombre del nuevo archivo duplicado\n",
    "            new_file_name = f\"ns-{id_ns:02d}-{year}.csv\"\n",
    "            new_file_path = os.path.join(year_path, new_file_name)\n",
    "\n",
    "            # Duplicar archivo\n",
    "            if last_existing_file:\n",
    "                if last_existing_file in existing_files.values():\n",
    "                    src_file_path = os.path.join(year_path, last_existing_file)\n",
    "                else:\n",
    "                    src_file_path = os.path.join(prev_year_path, last_existing_file)\n",
    "                shutil.copy(src_file_path, new_file_path)\n",
    "                print(f\"Duplicated {last_existing_file} to {new_file_name}\")\n",
    "            else:\n",
    "                print(f\"No existing file to duplicate for {new_file_name}\")\n",
    "\n",
    "# Procesar cada año\n",
    "for year in range(2010, 2013):\n",
    "    df_year = df[df['year'] == year]\n",
    "    duplicate_files(year, df_year, base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512e504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7156504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdp_revisions",
   "language": "python",
   "name": "gdp_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
