{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5def3233-fcf3-47c6-aff8-3e5ddd77cc6c",
   "metadata": {},
   "source": [
    "<div style=\"background:#3366FF; color:white; padding:12px; box-sizing:border-box; border-radius:4px;\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127505dd-d5cc-4829-9190-6314ae54b3ca",
   "metadata": {},
   "source": [
    "# New GDP Real-Time Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d479e1-2781-4db5-acd4-a7eef933cb8a",
   "metadata": {},
   "source": [
    "> **Author:** Jason Cruz  \n",
    "  **Last updated:** 11/13/2025  \n",
    "  **Python version:** 3.12  \n",
    "  **Project:** Rationality and Nowcasting on Peruvian GDP Revisions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96357866-58f8-4d8b-b25a-b9b145465322",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Œ Summary\n",
    "Welcome to the **Peruvian GDP Real-Time Dataset (RTD)** construction notebook! This notebook will guide you through the **step-by-step process** of creating your own RTD using GDP revisions from the **Central Reserve Bank of Peru** (BCRP). Whether you are a researcher, policymaker, or analyst, this notebook helps you construct real-time data of monthly GDP growth for Peru, starting from scratch.\n",
    "\n",
    "### What will this notebook help you achieve?\n",
    "1. **Downloading PDFs** from the BCRP Weekly Reports (WR).\n",
    "2. **Generating PDF inputs** by shortening them to focus on key pages containing GDP growth rate tables.\n",
    "3. **Cleaning-up extracted data** to ensure it's usable and building RTD.\n",
    "4. **Concatenating RTD** from different years and frequencies (monthly, quarterly, annual).\n",
    "5. **Updating metadata** for storing base years changes and other revisions-based information.\n",
    "6. **Converting RTD** to releases dataset for econometric analysis.\n",
    "\n",
    "ðŸŒ **Main Data Source:** [BCRP Weekly Report](https://www.bcrp.gob.pe/publicaciones/nota-semanal.html) (ðŸ“° WR, from here on)  \n",
    "For any questions or issues, feel free to reach out via email: [Jason ðŸ“¨](mailto:jj.cruza@up.edu.pe)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552ce7a-bbdb-4ff2-b798-3bb3b1fcc33e",
   "metadata": {},
   "source": [
    "### âš™ï¸ Initial Set-up\n",
    "\n",
    "Before preprocessing the new GDP releases data, we need to perform some initial set-up steps:\n",
    "\n",
    "1. ðŸ§° **Import helper functions** from `gdp_rtd_pipeline.py` that are required for this notebook.\n",
    "2. ðŸ›¢ï¸ **Connect to the PostgreSQL database** that will contain GDP revisions datasets. _(This step is pending: direct access will be provided via ODBC or other methods, allowing users to connect from any software or programming language.)_\n",
    "3. ðŸ“‚ **Create necessary folders** to store inputs, outputs, logs, and screenshots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d9656-700e-461b-99c5-5ad2557ac8e1",
   "metadata": {},
   "source": [
    "> ðŸš§ Although the second step (database connection) is pending, the notebook currently works using **flat files (CSV)**. These CSV files will **not be saved in GitHub** as they are included in the `.gitignore` to ensure no data is stored publicly. Users can be confident that no data will be stored on GitHub. The notebook **automatically generates the CSV files**, giving users direct access to the dataset on their own systems. The data is created on the fly and can be saved locally for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e7f538-dada-47a6-977d-d420aaf3cb22",
   "metadata": {},
   "source": [
    "### ðŸ§° Import helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c34e4-662e-43f6-890e-cc307da4da36",
   "metadata": {},
   "source": [
    "This notebook relies on a set of helper functions found in the script `gdp_rtd_pipeline.py`. These functions will be used throughout the notebook, so please ensure you have them ready by running the line of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b7cb8e-8405-457d-a329-2a6cefa17844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gdp_rtd_pipeline import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907046a",
   "metadata": {},
   "source": [
    "> ðŸ› ï¸ **Libraries:** Before you begin, please ensure that you have the required libraries installed and imported. See all the libraries you need section by section in `gdp_rtd_pipeline.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c73193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d47b8-5873-426b-b739-e1fc05dcf8e5",
   "metadata": {},
   "source": [
    "**Check out Python information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55588d8e-8df5-406a-8644-e67ff1dcbc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ Python Information\n",
      "  Version  : 3.12.1\n",
      "  Compiler : MSC v.1916 64 bit (AMD64)\n",
      "  Build    : ('main', 'Jan 19 2024 15:44:08')\n",
      "  OS       : Windows 10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"ðŸ Python Information\")\n",
    "print(f\"  Version  : {sys.version.split()[0]}\")\n",
    "print(f\"  Compiler : {platform.python_compiler()}\")\n",
    "print(f\"  Build    : {platform.python_build()}\")\n",
    "print(f\"  OS       : {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b9cd4-2433-4fcd-9da2-05d533b33fd5",
   "metadata": {},
   "source": [
    "### ðŸ“‚ Create necessary folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ef8f8",
   "metadata": {},
   "source": [
    "We will start by creating the necessary folders to store the data at various stages of processing. The following code ensures all required directories exist, and if not, it creates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51762441-7a80-4c30-ac06-0be260372738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter relative path (default='.'):  .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using path: C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\RA\\CIUP\\GDP Revisions\\GitHub\\peru_gdp_revisions\\gdp_revisions_datasets\n",
      "ðŸ“‚ new_weekly_reports created\n",
      "ðŸ“‚ new_weekly_reports\\raw created\n",
      "ðŸ“‚ new_weekly_reports\\input created\n",
      "ðŸ“‚ data created\n",
      "ðŸ“‚ data\\input created\n",
      "ðŸ“‚ data\\output created\n",
      "ðŸ“‚ metadata created\n",
      "ðŸ“‚ new_weekly_reports created\n",
      "ðŸ“‚ new_weekly_reports\\input created\n",
      "ðŸ“‚ record created\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path  # Importing Path module from pathlib to handle file and directory paths in a cross-platform way.\n",
    "\n",
    "# Get current working directory\n",
    "PROJECT_ROOT = Path.cwd()  # Get the current working directory where the notebook is being executed.\n",
    "\n",
    "# User input for folder location\n",
    "user_input = input(\"Enter relative path (default='.'): \").strip() or \".\"  # Prompt user to input the folder path or use the default value \".\"\n",
    "target_path = (PROJECT_ROOT / user_input).resolve()  # Combine the project root directory with user input to get the full target path.\n",
    "\n",
    "# Create the necessary directories if they don't already exist\n",
    "target_path.mkdir(parents=True, exist_ok=True)  # Creates the target folder and any necessary parent directories.\n",
    "print(f\"Using path: {target_path}\")  # Print out the path being used for confirmation.\n",
    "\n",
    "# Define paths for saving data and PDFs\n",
    "pdf_folder = 'new_weekly_reports'  # This folder will store the new Weekly Reports (post-2013), which are in PDF format.\n",
    "raw_pdf_subfolder = os.path.join(pdf_folder, 'raw')  # Subfolder for saving the raw PDFs exactly as downloaded from the BCRP website.\n",
    "input_pdf_subfolder = os.path.join(pdf_folder, 'input')  # Subfolder for saving reduced PDFs that contain only the selected pages with GDP growth tables.\n",
    "\n",
    "data_folder = 'data'  # Main folder for storing all data files.\n",
    "input_data_subfolder = os.path.join(data_folder, 'input')  # Folder for storing preprocessed data throughout all periods (NEW+OLD data).\n",
    "output_data_subfolder = os.path.join(data_folder, 'output')  # Folder for storing final RTD datasets and releases after processing.\n",
    "\n",
    "# Create all folders if they don't exist yet\n",
    "for folder in [pdf_folder, raw_pdf_subfolder, input_pdf_subfolder, data_folder, input_data_subfolder, output_data_subfolder]:\n",
    "    os.makedirs(folder, exist_ok=True)  # Create each folder in the list if it doesn't already exist.\n",
    "    print(f\"ðŸ“‚ {folder} created\")  # Print confirmation for each folder created.\n",
    "\n",
    "# Additional folders for metadata, records, and alert tracking\n",
    "metadata_folder = 'metadata'  # Folder for storing metadata files like wr_metadata.csv.\n",
    "record_folder = 'record'  # Folder for storing .txt files that track the files already processed to avoid reprocessing them.\n",
    "alert_track_folder = 'alert_track'  # Folder for saving download notifications and alerts.\n",
    "\n",
    "# Create additional required folders\n",
    "for folder in [metadata_folder, pdf_folder, input_pdf_subfolder, record_folder]:\n",
    "    os.makedirs(folder, exist_ok=True)  # Create the additional folders if they don't exist.\n",
    "    print(f\"ðŸ“‚ {folder} created\")  # Print confirmation for each of these additional folders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41289f95-c398-4c99-843f-e7ba43229f7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f3192",
   "metadata": {},
   "source": [
    "## 1. Downloading PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a32d3c8-b2d6-4494-9c5f-6e764ab2bbd8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98250a64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The **BCRP Weekly Report** is our primary source of data collection for constructing the Peruvian GDP Real-Time Dataset (RTD). This report, published weekly by the **Central Reserve Bank of Peru (BCRP)**, is an official document that contains critical macroeconomic statistics, including GDP growth rates.\n",
    "\n",
    "The two main tables we focus on in this project are:\n",
    "- **Table 1:** Monthly GDP growth rates (real GDP, 12-month percentage changes)\n",
    "- **Table 2:** Quarterly/Annual GDP growth rates (real GDP, 12-month percentage changes)\n",
    "\n",
    "This section automates the process of downloading the **BCRP Weekly Report PDFs** directly from the official BCRP website, ensuring that we can collect the most up-to-date data for our analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ› ï¸ What the Scraper Bot Does:\n",
    "\n",
    "1. **Opens the official BCRP Weekly Report page** at [this link](https://www.bcrp.gob.pe/publicaciones/nota-semanal.html).\n",
    "2. **Finds and collects all PDF links** for the reports.\n",
    "3. **Downloads the PDFs** in chronological order (from newest to oldest).\n",
    "4. Optionally, plays a **notification sound** after every batch of downloads.\n",
    "5. **Organizes** the downloaded PDFs into year-based folders.\n",
    "\n",
    "---\n",
    "\n",
    "#### âš ï¸ Important Notes:\n",
    "\n",
    "- **CAPTCHA Handling**: If a CAPTCHA appears during the download process, you'll need to manually solve it in the browser window and then **re-run the Scraper Bot**. The Scraper Bot cannot bypass CAPTCHA verification.\n",
    "  \n",
    "- **Automatic WebDriver Management**: This script uses `webdriver-manager` to automatically handle browser drivers (by default, it uses Chrome). **No need to manually download ChromeDriver or GeckoDriver**. If you wish to use a different browser, you can modify the `browser` parameter in the `init_driver()` function.\n",
    "  \n",
    "- **Custom Notification Sound**: If you'd like to receive notifications when each batch of downloads finishes, you can place your own MP3 file in the `alert_track` folder. We provide a warning track (in .mp3 format on GitHub). However, here are some free sources of .mp3 files so you can choose the ones you prefer:\n",
    "  - [Pixabay Audio](https://pixabay.com/music/) ðŸŽµ\n",
    "  - [FreeSound](https://freesound.org/) ðŸŽ¶\n",
    "  - [FreePD](https://freepd.com/) ðŸŽ¼\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d95c04",
   "metadata": {},
   "source": [
    "### ðŸ“¥ Scraper Bot for BCRP Weekly Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee0199bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¥ Starting PDF downloader for BCRP WR...\n",
      "\n",
      "ðŸŒ BCRP site opened successfully.\n",
      "ðŸ”Ž Found 155 WR blocks on page (one per month).\n",
      "\n",
      "1. âœ”ï¸ Downloaded: ns-27-2024.pdf\n",
      "â³ Waiting 8.63 seconds...\n",
      "2. âœ”ï¸ Downloaded: ns-31-2024.pdf\n",
      "â³ Waiting 8.66 seconds...\n",
      "3. âœ”ï¸ Downloaded: ns-35-2024.pdf\n",
      "â³ Waiting 9.36 seconds...\n",
      "4. âœ”ï¸ Downloaded: ns-39-2024.pdf\n",
      "â³ Waiting 8.15 seconds...\n",
      "5. âœ”ï¸ Downloaded: ns-43-2024.pdf\n",
      "â³ Waiting 6.07 seconds...\n",
      "6. âœ”ï¸ Downloaded: ns-47-2024.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "â¸ï¸ Continue? (y = yes, any other key = stop):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Waiting 7.44 seconds...\n",
      "7. âœ”ï¸ Downloaded: ns-04-2025.pdf\n",
      "â³ Waiting 8.28 seconds...\n",
      "8. âœ”ï¸ Downloaded: ns-08-2025.pdf\n",
      "â³ Waiting 9.49 seconds...\n",
      "9. âœ”ï¸ Downloaded: ns-11-2025.pdf\n",
      "â³ Waiting 5.56 seconds...\n",
      "10. âœ”ï¸ Downloaded: ns-14-2025.pdf\n",
      "â³ Waiting 6.19 seconds...\n",
      "11. âœ”ï¸ Downloaded: ns-18-2025.pdf\n",
      "â³ Waiting 9.30 seconds...\n",
      "12. âœ”ï¸ Downloaded: ns-22-2025.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "â¸ï¸ Continue? (y = yes, any other key = stop):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Waiting 7.39 seconds...\n",
      "13. âœ”ï¸ Downloaded: ns-26-2025.pdf\n",
      "â³ Waiting 6.32 seconds...\n",
      "14. âœ”ï¸ Downloaded: ns-30-2025.pdf\n",
      "â³ Waiting 8.11 seconds...\n",
      "15. âœ”ï¸ Downloaded: ns-34-2025.pdf\n",
      "â³ Waiting 7.25 seconds...\n",
      "16. âœ”ï¸ Downloaded: ns-40-2025.pdf\n",
      "â³ Waiting 9.33 seconds...\n",
      "17. âœ”ï¸ Downloaded: viewform\n",
      "â³ Waiting 8.52 seconds...\n",
      "\n",
      "ðŸ‘‹ Browser closed.\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "\n",
      "ðŸ”— Total monthly links kept: 155\n",
      "ðŸ—‚ï¸ 138 already downloaded PDFs were skipped.\n",
      "âž• Newly downloaded: 17\n",
      "â±ï¸ 247 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run the function to start the scraper bot\n",
    "pdf_downloader(\n",
    "    bcrp_url = \"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\",  # URL of the BCRP Weekly Report\n",
    "    raw_pdf_folder = raw_pdf_subfolder,  # Folder to save the raw downloaded PDFs\n",
    "    download_record_folder = record_folder,  # Folder to store download logs\n",
    "    download_record_txt = '1_downloaded_pdfs.txt',  # Record of downloaded PDFs\n",
    "    alert_track_folder = alert_track_folder,  # Folder for MP3 alert sound\n",
    "    max_downloads = 60,  # Maximum number of PDFs to download\n",
    "    downloads_per_batch = 6,  # Number of PDFs to download per batch\n",
    "    headless = False  # Run in browser window (set to True for headless mode)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa2d8bb-3b90-489b-9008-c5a8d9a21935",
   "metadata": {},
   "source": [
    "### ðŸ—‚ï¸ Organize Downloaded PDFs\n",
    "\n",
    "After downloading the PDFs, it is essential to organize them into year-based folders to keep everything structured. This will help in later stages of data extraction and cleaning.\n",
    "\n",
    "Run the following code to organize the downloaded PDFs. It'll happen in the blink of an eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee016a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the directory\n",
    "files = os.listdir(raw_pdf_subfolder)\n",
    "\n",
    "# Call the function to organize files by year\n",
    "organize_files_by_year(raw_pdf_subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada7da1-58d3-47f0-a2b0-a069d14e810a",
   "metadata": {},
   "source": [
    "### ðŸ”§ Handling Defective PDFs\n",
    "\n",
    "Occasionally, you may encounter defective PDFs (e.g., corrupted files, incomplete downloads, etc.). In such cases, you can replace the defective PDFs with new, valid ones. The following function allows you to replace defective PDFs.\n",
    "\n",
    "ðŸ”„ Replace Defective PDFs:\n",
    "\n",
    "Use this function to replace any defective PDFs that were downloaded. Just specify the year, the defective PDF name, and the new PDF that you want to use as a replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e956de-ead1-4439-a3c5-bab8aeb75a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace specific defective PDFs (friendly outputs with icons)\n",
    "replace_defective_pdfs(\n",
    "    items=[\n",
    "        (\"2017\", \"ns-08-2017.pdf\", \"ns-07-2017\"), # Replace a defective PDF in 2017 folder\n",
    "        (\"2019\", \"ns-23-2019.pdf\", \"ns-22-2019\"), # Replace a defective PDF in 2019 folder\n",
    "    ],\n",
    "    root_folder=input_pdf_subfolder,  # Base folder containing year-based folders\n",
    "    record_folder=record_folder,  # Folder where downloaded PDF logs are stored\n",
    "    download_record_txt = '1_downloaded_pdfs.txt',  # Log of downloaded PDFs\n",
    "    quarantine=os.path.join(input_pdf_subfolder, \"_quarantine\")  # Folder to store defective PDFs (set to None to delete them)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4002384-8551-444e-ad82-e320b5d26314",
   "metadata": {},
   "source": [
    "> âš¡ **Troubleshooting Tip:** If you encounter any issues during the data cleansing step (section 3), and suspect that the problem lies with defective PDFs, you can replace those PDFs using the above function. This will help avoid errors in the following sections. In case you encounter a problem with any particular defective PDF, you can also download alternative versions of the Weekly Reports for the same month, and replace the faulty ones as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2cc4d-3828-45e9-ab64-60add90eec1b",
   "metadata": {},
   "source": [
    "#### ðŸ§© Key Takeaways\n",
    "- Downloading PDFs: The scraper bot automates the process of collecting the latest BCRP Weekly Reports.\n",
    "- Organizing PDFs: After downloading, the PDFs are organized by year to make further processing easier.\n",
    "- Replacing Defective PDFs: If any PDFs are corrupted or incomplete, you can replace them with valid ones to ensure clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da2a6b-01f1-419c-88ce-301220bc68bc",
   "metadata": {},
   "source": [
    "> ðŸš€ **Next Steps**: With the PDFs downloaded, organized, and ready for use, we can move on to the data cleaning and extraction steps. This will be covered in the next section of the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a25c8e-551c-41ba-bad5-de951b8f86ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74136880-c233-4ae5-a08d-1744165ff207",
   "metadata": {},
   "source": [
    "## 2. Generating input PDFs with key tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209b6fd-6095-45e3-94fd-f35f45d27089",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d24d2",
   "metadata": {},
   "source": [
    "Now that we have successfully downloaded the **BCRP Weekly Reports (WR)**, it is important to note that each PDF file contains over 100 pages. However, not all pages are relevant to this project.\n",
    "\n",
    "For this analysis, we only need a **few key pages** from each WR:\n",
    "- **Table 1**: Monthly real GDP growth (12-month percentage changes)\n",
    "- **Table 2**: Annual and quarterly real GDP growth\n",
    "\n",
    "The goal of this section is to **trim the PDFs**, retaining just the necessary pages for analysis: the key tables and the cover page that provides the publication date and serial number for identification.\n",
    "\n",
    "The following steps will guide you through the process of generating these trimmed PDF files.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ› ï¸ What This Step Does:\n",
    "\n",
    "1. **Extracts key pages** from each WR, focusing on the pages that contain **Table 1** and **Table 2**.\n",
    "2. **Retains the cover page** that provides metadata, such as publication date and serial number.\n",
    "3. **Creates new PDFs** containing only the relevant pages, ensuring efficiency by reducing file sizes.\n",
    "4. Organizes these **trimmed PDFs** into year-based subfolders for easy access.\n",
    "\n",
    "---\n",
    "\n",
    "#### âš™ï¸ How the Code Works\n",
    "\n",
    "In this section, we use a combination of `PyMuPDF` and `PyPDF2` to handle PDF file manipulation. Here's a breakdown of the core steps:\n",
    "\n",
    "1. **Keyword Search:** The function search_keywords() scans each PDF to find pages containing the specified keywords (in this case, \"ECONOMIC SECTORS\"), which helps us locate the relevant tables.\n",
    "2. **PDF Trimming:** The function `shortened_pdf()` creates a new PDF containing only the selected pages. If the trimmed PDF contains 4 pages, we retain only the 1st and 3rd pages, which typically hold the key GDP tables.\n",
    "3. **Tracking Processed PDFs:** The function `read_input_pdf_files()` reads the record of previously processed PDFs, ensuring that we do not reprocess the same file. The function `write_input_pdf_files()` updates the record with new files, ensuring that the workflow is deterministic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da51439-3497-4266-b5a0-8c6f8ab79275",
   "metadata": {},
   "source": [
    "### âœ‚ï¸ Run the Code to Generate Trimmed PDFs\n",
    "\n",
    "The following function extracts the relevant pages from each raw WR PDF, creating a shortened version that contains only the key tables and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to generate trimmed PDFs for input\n",
    "pdf_input_generator(\n",
    "    raw_pdf_folder = raw_pdf_subfolder,  # Folder containing raw WR PDFs\n",
    "    input_pdf_folder = input_pdf_subfolder,  # Folder to store the shortened PDFs\n",
    "    input_pdf_record_folder = record_folder,  # Folder to store the record of generated PDFs\n",
    "    input_pdf_record_txt = '2_generated_input_pdfs.txt',  # Record file name\n",
    "    keywords = [\"ECONOMIC SECTORS\"]  # Keywords to help find relevant pages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643079a",
   "metadata": {},
   "source": [
    "This code processes the raw WR PDFs, extracts the pages containing the key tables, and stores them in the designated input PDF folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9811a9-9060-474e-badd-e3aa1330bc1b",
   "metadata": {},
   "source": [
    "### ðŸ“‚ Organizing Trimmed PDFs\n",
    "\n",
    "After generating the trimmed PDFs, itâ€™s essential to organize them into subfolders based on the year of publication. This makes it easier to locate and manage the files in future steps.\n",
    "\n",
    "The following code sorts the trimmed PDFs into year-based subfolders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the directory\n",
    "files = os.listdir(input_pdf_subfolder)\n",
    "\n",
    "# Call the function to organize files by year\n",
    "organize_files_by_year(input_pdf_subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d2b6c7-b387-4c9b-ba9c-84c659980455",
   "metadata": {},
   "source": [
    "This will ensure that each trimmed WR PDF is placed into its respective year folder, making it simple to access data from specific years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e0929-9f2b-409b-94af-8a4b989f3404",
   "metadata": {},
   "source": [
    "#### ðŸš€ Moving Forward\n",
    "\n",
    "With the PDFs now trimmed and organized, we can proceed to the next steps in the data extraction and cleaning process.\n",
    "\n",
    "This section will significantly improve the efficiency of handling large numbers of PDFs, as weâ€™ve reduced the file size by focusing only on the pages that contain the data we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201c129-364b-4304-b77a-62c0f4000d77",
   "metadata": {},
   "source": [
    "#### ðŸ§© Key Takeaways\n",
    "\n",
    "- The trimmed PDFs will now contain only the relevant pages, making them easier to handle and faster to process.\n",
    "- The PDFs are organized by year for easy access and management.\n",
    "- Efficient processing: The record of processed files ensures that no data is reprocessed, saving time and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9103029-6452-4b45-acc0-2fa7f9a17ae8",
   "metadata": {},
   "source": [
    "> ðŸš€ **Next Steps:** Now that we have our trimmed PDFs, we are ready to move on to the data extraction and cleaning steps, where we will begin working with the key data from these PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5275a1c-68ea-42f2-b698-f522d221dc42",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e1e92",
   "metadata": {},
   "source": [
    "## 3. Cleaning tables and building RTD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07c0d9-f752-4123-a224-1e893a25a3f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0081858-3511-4d42-8d9b-9d4866ecfd20",
   "metadata": {},
   "source": [
    "In this section, we will tackle the core task of **extracting and cleaning** the tables required for constructing the **Real-Time Dataset (RTD)**. The input data consists of PDFs containing only the 2 key tables, and our goal is to **extract GDP growth rates data in the most faithful way** from these tables and clean the data for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§¹ Extracting Tables and Data Cleanup\n",
    "\n",
    "We will use the **`tabula`** library for extracting tables from the PDFs. This library efficiently converts PDF tables into **pandas DataFrames**, which are easier to manipulate and analyze.\n",
    "\n",
    "For more information on how **`tabula`** works, feel free to check out its [official documentation](https://tabula-py.readthedocs.io/en/latest/).\n",
    "\n",
    "In this section, we apply several cleaning functions, defined in **`gdp_rtd_pipeline.py`**, to address the challenges of cleaning the extracted tables.\n",
    "\n",
    "The cleaning process involves **3** main dictionaries:\n",
    "1. **The raw dictionary**: stores the original tables extracted directly from the PDFs.\n",
    "2. **The clean dictionary**: contains the fully cleaned tables, ready for analysis.\n",
    "3. **The vintage dictionary**: contains the fully converted tables into vintages. Every table from every PDF has been converted to vintage format.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¢ Step-by-Step Breakdown of the Cleaning Process\n",
    "\n",
    "#### 1. Extraction\n",
    "We begin by using **`tabula`** to extract the raw tables from the PDF files. These raw tables are then stored in dictionaries for easy access during the cleaning phase.\n",
    "\n",
    "#### 2. Cleaning\n",
    "A series of cleaning functions are applied to each table to ensure the data is in a usable format. See the `ðŸš¨ Main Issues in Weekly Reports and How We Cleaned Them` subsection below.\n",
    "\n",
    "#### 3. Data Inspection\n",
    "After cleaning, we provide a way for users to **inspect the data**. This will allow you to compare the **raw**, **cleaned**, and **vintage** tables by reviewing the output of the raw, clean, and vintage dictionaries. The goal is to visually ensure the quality of the data and confirm that all cleaning steps were applied correctly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c4d86c-d6e2-4d8d-873d-d88c5a4b00b7",
   "metadata": {},
   "source": [
    "### ðŸš¨ Main Issues in Weekly Reports and How We Cleaned Them\n",
    "\n",
    "The BCRP Weekly Reports (WRs) often present structural inconsistencies that can complicate data extraction. In this section, we focus on resolving key issues that commonly arise, ensuring the data is consistent, usable, and ready for analysis.\n",
    "\n",
    "Below are specific problems encountered in the reports, along with the corresponding cleaning steps we implemented:\n",
    "\n",
    "**1. Misaligned Headers**\n",
    "\n",
    "* **Problem:** The header row, which typically contains sector names or year labels, was sometimes misaligned, especially when certain headers like \"SECTORES ECONÃ“MICOS\" were incorrectly placed.\n",
    "* **Solution:** We used the `swap_nan_se()` function to correct misalignments, ensuring the header \"SECTORES ECONÃ“MICOS\" is placed in the correct column.\n",
    "\n",
    "> `d = swap_nan_se(d)                       # Align 'SECTORES ECONÃ“MICOS' header properly`\n",
    "\n",
    "**2. Mixed Header Patterns**\n",
    "\n",
    "* **Problem:** Some columns had combined headers, such as \"Sector. Subsector,\" leading to confusion when analyzing data.\n",
    "* **Solution:** The `split_column_by_pattern()` function was used to split these combined headers into separate, meaningful columns for easier analysis.\n",
    "\n",
    "> `d = split_column_by_pattern(d)           # Separate combined header values like \"Sector. Subsector\"`\n",
    "\n",
    "**3. Missing or Irregular Year Labels**\n",
    "\n",
    "* **Problem:** Year labels (e.g., \"2019\", \"2020\") were either missing or misaligned across columns.\n",
    "* **Solution:** We implemented the `find_year_column()` function to automatically detect and correct year columns, ensuring consistency across the data.\n",
    "\n",
    "> `d = find_year_column(d)                  # Detect and align year columns automatically`\n",
    "\n",
    "**4. Extra or Irrelevant Rows and Columns**\n",
    "\n",
    "* **Problem:** Some tables contained rows or columns with redundant or irrelevant data (such as placeholders or completely missing values).\n",
    "* **Solution:** We used functions like `drop_nan_rows()`, `drop_nan_columns()`, and `drop_rare_caracter_row()` to remove these unwanted entries.\n",
    "\n",
    "> `d = drop_nan_rows(d)                    # Remove rows where all values are NaN`\n",
    ">\n",
    "> `d = drop_nan_columns(d)                 # Drop columns with all NaN values`\n",
    ">\n",
    "> `d = drop_rare_caracter_row(d)           # Remove rows with rare characters like '}'`\n",
    "\n",
    "**5. Mixed Numeric and Text Values**\n",
    "\n",
    "* **Problem:** Some columns contained mixed content, with text and numeric values in the same column (e.g., \"Var. %\").\n",
    "* **Solution:** The `separate_text_digits()` function was used to split the mixed content into separate numeric and text values, making the data easier to analyze.\n",
    "\n",
    "> `d = separate_text_digits(d)            # Split mixed content (text + numeric) into separate columns`\n",
    "\n",
    "**6. Formatting and Naming Inconsistencies**\n",
    "\n",
    "* **Problem:** Sector names and labels had inconsistencies, especially between Spanish and English versions of terms like \"services\" and \"mining.\"\n",
    "* **Solution:** We standardized these terms using functions like `replace_services()` and `replace_mineria()` to harmonize the labels across all reports.\n",
    "\n",
    "> `d = replace_services(d)               # Standardize 'services' naming across sectors`\n",
    ">\n",
    "> `d = replace_mineria(d)                # Standardize 'mineria' naming in Spanish sectors`\n",
    ">\n",
    "> `d = replace_mining(d)                 # Standardize 'mining' naming in English sectors`\n",
    "\n",
    "**ðŸ§¹ Final DataFrame Cleaning**\n",
    "\n",
    "* After applying the aforementioned cleaning functions, the final DataFrame is fully normalized and ready for further analysis. We perform additional final cleaning steps to ensure the data is consistent across all columns:\n",
    "\n",
    "> `d = clean_columns_values(d)`         # Normalize column names and values\n",
    "> \n",
    "> `d = convert_float(d)`                # Convert non-label columns to numeric\n",
    ">\n",
    "> `d = rounding_values(d, decimals=1)`  # Round float columns to one decimal place\n",
    "\n",
    "These steps ensure that the final dataset is in a format suitable for analysis, with properly cleaned and formatted columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b32a78-b9bc-4a81-b22c-850744fe091f",
   "metadata": {},
   "source": [
    "### ðŸ§¼ Cleaning Process: Code Walkthrough\n",
    "\n",
    "The following steps are implemented using the functions defined in **`gdp_rtd_pipeline.py`**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021e356-54c3-4b4e-8e2e-666578a86b54",
   "metadata": {},
   "source": [
    "#### Table â¶: Monthly Data into Row-Based Vintage Format\n",
    "The first table we clean is **Table 1**, which contains monthly growth data. The goal here is to transform the table into a **row-based vintage format**, ensuring that each record corresponds to a specific observation (row) with relevant vintage and period information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2388f-faa2-4eb7-abce-8b61e8d7b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Table 1 (monthly growth data) into row-based vintage format\n",
    "raw_1, clean_1, vintages_1 = new_table_1_cleaner(\n",
    "    input_pdf_folder=input_pdf_subfolder,\n",
    "    record_folder=record_folder,\n",
    "    record_txt='3_created_new_rtd_tab_1.txt',\n",
    "    persist=True,\n",
    "    persist_folder=input_data_subfolder,\n",
    "    pipeline_version=\"s3.0.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of raw, clean, and vintage data\n",
    "raw_1.keys()\n",
    "clean_1.keys()\n",
    "vintages_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d0c72-b21f-45da-af5d-2897693959d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the cleaned table for a specific vintage\n",
    "clean_1['ns_11_2024_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095db26-838d-4dc6-81a2-e86208212618",
   "metadata": {},
   "source": [
    "In this step, we use **`gdp_rtd_pipeline.py`** to clean Table 1 and transform it into a vintage format. We also check the structure of the data and inspect specific vintages (e.g., ns_11_2024_1) to ensure everything is cleaned correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a4fe9-54cc-48b6-901c-b0b5396fe68d",
   "metadata": {},
   "source": [
    "**Checking the cleaning version out**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88e1dd-8c85-481b-9a42-3d3b8174cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version = vintages_1[\"ns_04_2022_1\"]\n",
    "#print(version.attrs)\n",
    "# {'pipeline_version': 's3.0.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb48b6ed-b2ce-44cd-b19e-930ff305f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vintages_1[\"ns_04_2022_1\"].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef214918-2835-4738-8ec0-5ea98e3e2d8d",
   "metadata": {},
   "source": [
    "#### Table â·: Quarterly/Annual Data into Row-Based Vintage Format\n",
    "\n",
    "Similarly, we clean Table 2, which contains quarterly and annual growth data, and transform it into the same vintage format for consistency. Just like for Table 1, new_table_2_cleaner is used to clean Table 2, and we again inspect the data for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fcfe1b-9f63-4850-937f-ca582be3ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Table 2 (quarterly/annual growth data) into row-based vintage format\n",
    "raw_2, clean_2, vintages_2 = new_table_2_cleaner(\n",
    "    input_pdf_folder=input_pdf_subfolder,\n",
    "    record_folder=record_folder,\n",
    "    record_txt='3_created_new_rtd_tab_2.txt',\n",
    "    persist=True,\n",
    "    persist_folder=input_data_subfolder,\n",
    "    pipeline_version=\"s3.0.0\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of raw, clean, and vintage data\n",
    "raw_2['ns_04_2022_2']\n",
    "clean_2['ns_04_2022_2']\n",
    "vintages_2['ns_04_2022_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2c05f-0f27-4443-976f-95196267e8c6",
   "metadata": {},
   "source": [
    "**Checking the cleaning version out**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2386c0-bf54-4d0a-b67d-97352ad8203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version_2 = vintages_2[\"ns_04_2022_2\"]\n",
    "#print(version_2.attrs)\n",
    "# {'pipeline_version': 's3.0.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2c580-8454-45c8-902f-11f4bfda68a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vintages_2[\"ns_04_2022_1\"].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7287857-5ec6-4a6d-a8af-c5163791c94b",
   "metadata": {},
   "source": [
    "#### ðŸ§© Key Takeaways\n",
    "\n",
    "* Extracting Tables: We used pdfplumber to extract Table 1 (monthly GDP growth) and Table 2 (quarterly/annual GDP growth) from each WR.\n",
    "* Cleaning the Data: The cleaning pipeline addressed issues such as misaligned headers, missing year labels, mixed content, and more, using a series of functions tailored for these specific problems.\n",
    "* Standardizing the Data: The final cleaned tables were standardized and formatted for easy use in further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c12da-416e-4f41-8546-ee3c26989fb9",
   "metadata": {},
   "source": [
    "> ðŸš€ **Next Steps:** With the data now cleaned and formatted, we can proceed to the next steps of building the real-time GDP dataset (RTD) by reshaping the tables and creating vintages for analysis. This will be covered in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465ed63-9779-469e-bcf0-526486edc7c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8989d65",
   "metadata": {},
   "source": [
    "## 4. Concatenating RTD across years by frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699317e3-6e80-407a-a17c-781f522206fc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796111d4-fab7-4421-a79b-74d9d63c61fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "In this section, we focus on **concatenating** the **Real-Time Data (RTD)** for **Table 1** (monthly GDP growth) and **Table 2** (quarterly/annual GDP growth) across multiple years. The goal is to create a unified RTD that spans all available years, aligned by **frequency** (monthly, quarterly, and annual).\n",
    "\n",
    "This process ensures that the data is consistent and ready for further analysis. We also support **saving** the concatenated data into a **persistent format**, such as **CSV** or **Parquet**, for easy access.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ› ï¸ What This Step Does:\n",
    "\n",
    "1. **Concatenates** Table 1 (monthly data) and Table 2 (quarterly/annual data) across years into unified DataFrames.\n",
    "2. Ensures that all columns are aligned properly by frequency and year.\n",
    "3. Optionally, **persists** the concatenated data to disk in **CSV** format.\n",
    "4. Provides a **summary** of the concatenation process, including how many files were processed, skipped, and newly concatenated.\n",
    "\n",
    "---\n",
    "\n",
    "âš™ï¸ How the Code Works\n",
    "\n",
    "1. Reads CSVs by Year: The functions read all the CSV files from each year folder (monthly for Table 1, quarterly/annual for Table 2).\n",
    "2. Identifies Target Period Columns: The code checks for columns that represent different time periods (e.g., tp_YYYYmM for monthly, tp_YYYYqN for quarterly).\n",
    "3. Aligns Columns Chronologically: Target period columns are sorted by year and month (for Table 1) or year and quarter (for Table 2).\n",
    "4. Vertical Concatenation: The individual DataFrames are concatenated vertically, ensuring the data from all years is combined into one unified DataFrame.\n",
    "5. Enforces Data Types: Columns are reindexed to match the final column schema, and the data types are normalized (e.g., converting numeric columns to the correct type).\n",
    "6. Optional Persistence: If the persist flag is set to True, the concatenated DataFrame is saved to disk in the specified format (CSV or Parquet).\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ§¹ Cleaning and Alignment\n",
    "\n",
    "The process ensures that data from different years is aligned by the following steps:\n",
    "- Target period columns: All columns corresponding to time periods (e.g., months, quarters) are identified and sorted chronologically.\n",
    "- Reindexing: Each DataFrame is reindexed to match the full set of target period columns, ensuring that the columns are consistently aligned across years.\n",
    "- Handling missing data: If any data is missing for specific time periods, it is handled in the concatenation step, either by using NaN or applying a specific data imputation strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa4434-c293-4b0f-8a75-7d636262e719",
   "metadata": {},
   "source": [
    "### ðŸ”— Run the Code to Concatenate RTD\n",
    "\n",
    "We use two functions: **`concatenate_table_1`** for monthly data (Table 1) and **`concatenate_table_2`** for quarterly/annual data (Table 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d5825-2669-4e34-8301-bcb26b5c72ab",
   "metadata": {},
   "source": [
    "#### Table â¶: Concatenate Monthly Data (Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e4426-8320-4dfe-bf4e-fb3ca1d211c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Table 1 (monthly GDP growth data) across years\n",
    "concatenated_1 = concatenate_table_1(\n",
    "    input_data_subfolder=input_data_subfolder,  # Path to the input data\n",
    "    record_folder=record_folder,  # Path to store the processed record\n",
    "    record_txt=\"4_concatenated_rtd_tab_1.txt\",  # Name of the record file\n",
    "    persist=True,  # Flag to persist the concatenated output\n",
    "    persist_folder=output_data_subfolder,  # Folder to save the output\n",
    "    csv_file_label=\"monthly_gdp_rtd.csv\",  # Custom name for the output file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c00c29-942f-49a9-9c44-96a80b9f6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 10 rows of the concatenated data\n",
    "concatenated_1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da6749a-00b4-4ae5-b18e-d95daeb44da9",
   "metadata": {},
   "source": [
    "#### Table â·: Concatenate Quarterly/Annual Data (Table 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c411a38-f585-4c9d-83d6-19c5bcff89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Table 2 (quarterly and annual GDP growth data) across years\n",
    "concatenated_2 = concatenate_table_2(\n",
    "    input_data_subfolder=input_data_subfolder,  # Path to the input data\n",
    "    record_folder=record_folder,  # Path to store the processed record\n",
    "    record_txt=\"4_concatenated_rtd_tab_2.txt\",  # Name of the record file\n",
    "    persist=True,  # Flag to persist the concatenated output\n",
    "    persist_folder=output_data_subfolder,  # Folder to save the output\n",
    "    csv_file_label=\"quarterly_annual_gdp_rtd.csv\",  # Custom name for the output file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d6f6a-a5e7-431d-a512-4eb4fb7a1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 10 rows of the concatenated data\n",
    "concatenated_2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e7c84-45eb-4f53-969a-30adfb0f22bb",
   "metadata": {},
   "source": [
    "These functions will load the raw data from each year, concatenate it vertically, and return a unified DataFrame with the full RTD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17533de9-5ddb-440f-9e28-50919d5948c4",
   "metadata": {},
   "source": [
    "> â—â— **Disclaimer:** If compared with the tables displayed in the supplemental document, the concatenated tables above have been transposed to compactly save the dataset. This approach avoids creating excessively long datasets with too many columns, which could be cumbersome for most software when saved. Transposing the tables results in the same structure as the ones in the supplemental material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a967f21-5d7b-486d-b52d-2a1043e01fa2",
   "metadata": {},
   "source": [
    "#### ðŸ§© Key Takeaways\n",
    "\n",
    "* Concatenation: The code vertically concatenates the data for Table 1 (monthly) and Table 2 (quarterly/annual) from all years.\n",
    "* Column Alignment: Ensures that columns representing different time periods are aligned correctly (e.g., months, quarters).\n",
    "* Persistence: Saves the concatenated data to disk if the persist flag is set to True.\n",
    "* Data Inspection: You can inspect the first 10 rows of the concatenated data to verify its correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1baab5-27d8-4a39-bd8a-0469d2d4d9e3",
   "metadata": {},
   "source": [
    "> ðŸš€ **Next Steps:** Once the data has been concatenated, we can proceed to the next steps in building the real-time GDP dataset (RTD). This involves reshaping the data and creating vintages for further analysis, which will be covered in the upcoming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef309d72-8c89-4676-a7d7-b9b1d02160ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8745bc5-8c1d-492f-939b-573cdcd4714b",
   "metadata": {},
   "source": [
    "## 5. Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dee730-22d4-4e79-b645-dac34b71d637",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5155fa2-5dc2-41fc-afce-aaed1a104f76",
   "metadata": {},
   "source": [
    "In this section, we handle the metadata associated with GDP revisions, which is essential for tracking, understanding, and ensuring the accuracy of our Real-Time Dataset (RTD). Metadata plays a key role in managing and tracking changes in GDP growth estimates over time, providing transparency and consistency for replication and further analysis.\n",
    "\n",
    "The primary goals in this section are:\n",
    "1. **Reading and updating metadata**: Extracts revision info from the Weekly Reports (WRs) and update through the time.\n",
    "2. **Base-year adjustments**: Tracks when and how base years are updated, ensuring data integrity. This is useful to adjust RTD removing GDP growwth rate affected by base-year changes. \n",
    "3. **Generating benchmark datasets**: Creates datasets adjusted according to benchmark revision procedures, facilitating accurate comparisons and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“… Revision Calendar\n",
    "\n",
    "The Revision Calendar is essential for understanding the timing and sequence of GDP updates published by the Central Reserve Bank of Peru (BCRP). The calendar helps track the evolution of GDP estimates, given that initial releases often undergo revisions over time. While the timing of the initial releases is predictable, revisions happen without a formal public schedule, making it difficult to track revisions in real-time without a clear framework.\n",
    "\n",
    "To address this, we constructed an implicit revision calendar using the information provided by the BCRP's Weekly Reports (WR). We used two main criteria to harmonize and standardize the calendar:\n",
    "\n",
    "* Chief Resolution No. 316-2003-INEI (see [https://www.gob.pe/institucion/inei/normas-legales/2294897-316-2003-inei](here)) mandates that sectoral offices update their data at least quarterly (March, June, September, December), suggesting monthly revisions.\n",
    "* Our analysis confirms that revisions are updated at least monthly in the WRs.\n",
    "  \n",
    "This revision calendar is essential to construct the RTD, allowing us to define \"vintages\" (sets of GDP estimates available at a specific time) and track the evolution of these estimates consistently across time.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”„ Updating Metadata\n",
    "\n",
    "The `update_metadata` function is used to read, update, and store the metadata related to the revisions of GDP growth rates. It works by:\n",
    "1. Reading the existing metadata from a CSV file.\n",
    "2. Extracting revision data from the BCRP's WR PDFs.\n",
    "3. Applying base-year adjustments to the new rows based on the provided base-year list.\n",
    "4. Marking where base-year changes have occurred.\n",
    "5. Appending the new metadata to the existing records.\n",
    "\n",
    "> â— **Disclaimer:** This is the only file requested externally by users. Therefore, it is the only (plain) file available on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db59b0-2a1f-48a7-82c1-2a8401c519fc",
   "metadata": {},
   "source": [
    "**Example: Define a List of Base Years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35972c04-3b3d-434d-8ae2-718b460a5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base_year_list for mapping base years (modify or extend this list as needed)\n",
    "base_year_list = [\n",
    "    {\"year\": 1994, \"wr\": 1, \"base_year\": 1990},\n",
    "    {\"year\": 2000, \"wr\": 28, \"base_year\": 1994},\n",
    "    {\"year\": 2014, \"wr\": 11, \"base_year\": 2007},\n",
    "    {\"year\": 2022, \"wr\": 20, \"base_year\": 2019},\n",
    "    # Add more mappings if needed\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e95d1d-4593-4f7c-b751-9bc49b0e5a8e",
   "metadata": {},
   "source": [
    "The runner below updates metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dba404-f4e5-495c-adf1-7714fb3ab43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to update the metadata\n",
    "updated_df = update_metadata(\n",
    "    metadata_folder = metadata_folder,\n",
    "    input_pdf_folder = input_pdf_subfolder,\n",
    "    record_folder = record_folder,\n",
    "    record_txt = \"5_weekly_report_metadata.txt\",\n",
    "    wr_metadata_csv = \"wr_metadata.csv\",\n",
    "    base_year_list = base_year_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8f1f9-e9f2-43f8-a648-6811534a1b16",
   "metadata": {},
   "source": [
    "After updating the metadata, you can inspect the last few rows to verify the changes and ensure the revisions were applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730862b-944b-47fa-b4a3-9ef75c43607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.iloc[-10:]   # last 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23fd435-5ae3-4018-b80d-1ca3d32aa378",
   "metadata": {},
   "source": [
    "### ðŸ§½ðŸ“… Generating Adjusted RTDs by Removing Revisions Affected by Base Years\n",
    "\n",
    "In this step, we apply base-year adjustments to the RTD data, marking values that are affected by changes in the base year. This process helps ensure that the dataset reflects the most accurate and up-to-date growth rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56916e24-088e-4911-afaa-978f82a7a68f",
   "metadata": {},
   "source": [
    "**Example: Apply Base-Year Sentinel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc92104-cfd4-4466-9d9f-2b1886d014b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_year_list_2 = [\n",
    "    \"2000m7\",   # 1990 -> 1994\n",
    "    \"2014m3\",   # 1994 -> 2007\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffe11d-90be-49e4-a3f7-610ce940a3aa",
   "metadata": {},
   "source": [
    "The `apply_base_year_sentinel` function applies a sentinel value (e.g., `-999999.0`) to data that is affected by a base-year change. This ensures that the affected data is marked as invalid, making it clear when and where the base-year changes occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1cbc5-64b0-45e1-952e-37642dabd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process both monthly and quarterly GDP files and save them with new names\n",
    "adjusted_rtd = apply_base_year_sentinel(\n",
    "    base_year_list=base_year_list_2,\n",
    "    sentinel=-999999.0,\n",
    "    output_data_subfolder=output_data_subfolder,\n",
    "    csv_file_labels=[\"monthly_gdp_rtd.csv\", \"quarterly_annual_gdp_rtd.csv\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e3ccf-dd44-48f5-97e5-61d7b9605efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the processed data (adjusted CSV files)\n",
    "adjusted_monthly_rtd = adjusted_rtd[\"by_adjusted_monthly_gdp_rtd.csv\"]\n",
    "adjusted_quarterly_rtd = adjusted_rtd[\"by_adjusted_quarterly_annual_gdp_rtd.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779bf6b-ef41-43dd-aeb1-6775ed332462",
   "metadata": {},
   "source": [
    "### ðŸ“ðŸ“Š Generating Benchmark RTDs for Revisions Affected by Benchmarking Procedures\n",
    "\n",
    "The benchmark RTDs are generated by applying the benchmark revision mapping to the real-time GDP data. This process ensures that GDP growth rates are adjusted based on the benchmark revisions, creating datasets that are aligned with the most recent and consistent methods used by statistical agencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb433db-fa8a-46d3-b5ea-612c7fbd134c",
   "metadata": {},
   "source": [
    "**Example: Generate Benchmark RTDs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3784e9-9c97-4157-8d97-334c6cdb27f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_labels = [\n",
    "    \"monthly_gdp_rtd\",\n",
    "    \"quarterly_annual_gdp_rtd\",\n",
    "    \"by_adjusted_monthly_gdp_rtd\",\n",
    "    \"by_adjusted_quarterly_annual_gdp_rtd\"\n",
    "]\n",
    "benchmark_dataset_csv = [\n",
    "    \"monthly_gdp_benchmark\",\n",
    "    \"quarterly_annual_gdp_benchmark\",\n",
    "    \"by_adjusted_monthly_gdp_benchmark\",\n",
    "    \"by_adjusted_quarterly_annual_gdp_benchmark\"\n",
    "]\n",
    "record_txt = \"5_converted_to_benchmark.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177506c-ac9e-4ccb-967d-9bda6e786dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_metadata_csv = \"wr_metadata.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a889a-9826-4f5c-92f6-bf6c857e7cd2",
   "metadata": {},
   "source": [
    "The `convert_to_benchmark_dataset` function applies the benchmark revision procedure to the real-time GDP data, ensuring that revisions are consistent with the latest updates from the statistical agencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e6e45-5a0e-4c58-9fb8-0e3c9da96b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_datasets = convert_to_benchmark_dataset(\n",
    "    output_data_subfolder=output_data_subfolder,\n",
    "    csv_file_labels=csv_file_labels,\n",
    "    metadata_folder=metadata_folder,\n",
    "    wr_metadata_csv=wr_metadata_csv,\n",
    "    record_folder=record_folder,\n",
    "    record_txt=record_txt,\n",
    "    benchmark_dataset_csv=benchmark_dataset_csv\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed57304e-8925-4b4d-a32b-155d09c473da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a los resultados procesados\n",
    "processed_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6948e9eb-b23a-4d53-a775-ee29e31f1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_datasets['monthly_gdp_benchmark']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa30192-0178-48a0-a7cf-ca70b23f7cc6",
   "metadata": {},
   "source": [
    "### ðŸ§© Key Takeaways\n",
    "\n",
    "* Update Metadata: Extract and apply base-year changes, ensuring consistency across the dataset.\n",
    "* Adjust RTDs: Mark revisions affected by base-year changes using a sentinel value.\n",
    "* Generate Benchmark RTDs: Apply the benchmark revision mapping to ensure the data is aligned with official procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2ea91-a71e-478a-8490-a0218d8c2cec",
   "metadata": {},
   "source": [
    "> ðŸš€ **Next Steps:** With the updated metadata and adjusted RTDs, we can proceed to the next steps in building the real-time GDP dataset (RTD). These steps will include reshaping the tables and creating vintages for in-depth analysis of GDP growth and revisions over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c762fc-366a-4dd6-9a9f-1a8dd269c3e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb74e8-7f22-4756-96ea-eb86a2aa400e",
   "metadata": {},
   "source": [
    "## 6. Releases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed572be-1c29-43e3-ad25-c8558976654f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55dc40f-3316-4fbf-8bc3-c79d9042ae58",
   "metadata": {},
   "source": [
    "This section is responsible for converting Real-Time GDP (RTD) datasets into releases datasets. The releases dataset is crucial for tracking and analyzing the sequence of GDP revisions. By restructuring the data into a release-based format, we can better map the evolution of GDP estimates in terms of \"releases\", helping to capture changes and dependence patterns in the statistical analysis.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "* Convert raw RTD data into release datasets.\n",
    "* Align non-NaN values for each industry and vintage. The first release of all target periods aligns in the first row, and so on.\n",
    "* Organize the data by release sequence for each industry and target period.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ› ï¸ Converting RTD to Releases Dataset\n",
    "\n",
    "The `convert_to_releases_dataset` function is designed to transform the RTD data into a format that is structured by release sequence. This function processes each dataset, aligning the non-NaN values for every target period and each industry, while removing any invalid values due to base-year changes.\n",
    "\n",
    "Key Steps in Conversion:\n",
    "\n",
    "1. **File Validation:** Ensures that the input and output file lists match in length.\n",
    "2. **Sorting and Grouping:** The data is sorted by industry, year, and month, ensuring chronological order.\n",
    "3. **Aligning Non-NaN Values:** For each industry, the function aligns non-NaN values across the target periods (`tp_` columns), creating a sequence of releases.\n",
    "4. **Removing Invalid Rows:** It drops rows where all target period columns are NaN, ensuring that only valid data is retained.\n",
    "5. **Reorganization:** The dataset is pivoted, with each industry and release forming new columns.\n",
    "6. **Final Output:** The dataset is saved into a CSV file for each industry and release sequence.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ” Data Processing Workflow\n",
    "\n",
    "1. Input Data: We start with the RTD data, which is stored in CSV files. Each dataset corresponds to a specific frequency (monthly, quarterly, or annual) and includes data for different vintages.\n",
    "\n",
    "2. Processing:\n",
    "\n",
    "* Sorting: Data is sorted by industry, year, and month to ensure the chronological order of releases.\n",
    "* Aligning Releases: Non-NaN values for each industry and vintage are aligned, ensuring that all releases are consistent and in the correct sequence.\n",
    "* Pivoting: The data is then pivoted to arrange each industryâ€™s releases in separate columns.\n",
    "\n",
    "3. Output: The converted releases datasets are saved as new CSV files, each named according to its respective label (e.g., `monthly_gdp_releases.csv`, `quarterly_annual_gdp_releases.csv`).\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ”„ Key Concepts and Terminology\n",
    "\n",
    "* **Industry:** Represents the economic sector (e.g., manufacturing, agriculture) for which GDP growth rates are reported.\n",
    "* **Vintage:** Refers to the specific release of GDP data for a given period (e.g., the first release, second release, etc.).\n",
    "* **Release:** Each release refers to an updated estimate of GDP for a given target period. These releases are tracked sequentially (first, second, third, etc.) for each industry.\n",
    "* **Target Period (tp_):** These are the columns representing GDP growth rates for specific periods (e.g., \"tp_2021m01\" for January 2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b767328-fccd-4258-b04b-c77a40fb9158",
   "metadata": {},
   "source": [
    "**Example: Convert RTD to Releases Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b4c9c-f988-44f1-8c94-8ae09a791ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_labels = [\n",
    "    \"monthly_gdp_rtd\",\n",
    "    \"quarterly_annual_gdp_rtd\",\n",
    "    \"by_adjusted_monthly_gdp_rtd\",\n",
    "    \"by_adjusted_quarterly_annual_gdp_rtd\",\n",
    "    \"monthly_gdp_benchmark\",\n",
    "    \"quarterly_annual_gdp_benchmark\",\n",
    "    \"by_adjusted_monthly_gdp_benchmark\",\n",
    "    \"by_adjusted_quarterly_annual_gdp_benchmark\"\n",
    "]\n",
    "releases_dataset_csv = [\n",
    "    \"monthly_gdp_releases\",\n",
    "    \"quarterly_annual_gdp_releases\",\n",
    "    \"by_adjusted_monthly_gdp_releases\",\n",
    "    \"by_adjusted_quarterly_annual_gdp_releases\",\n",
    "    \"monthly_gdp_benchmark_releases\",\n",
    "    \"quarterly_annual_gdp_benchmark_releases\",\n",
    "    \"by_adjusted_monthly_gdp_benchmark_releases\",\n",
    "    \"by_adjusted_quarterly_annual_gdp_benchmark_releases\"\n",
    "]\n",
    "record_txt = \"6_converted_to_releases.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d86a426-4e03-48b3-98a4-26f8f7f16395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the conversion function\n",
    "releases_df = convert_to_releases_dataset(\n",
    "    output_data_subfolder=output_data_subfolder,\n",
    "    csv_file_labels=csv_file_labels,\n",
    "    record_folder=record_folder,\n",
    "    record_txt=record_txt,\n",
    "    releases_dataset_csv=releases_dataset_csv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e403f-ad37-4997-94e3-f01a00f03324",
   "metadata": {},
   "source": [
    "After running the conversion, you can check the `releases_df` for specific datasets like \"monthly_gdp_releases\" to verify that the releases data has been processed and organized correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c124a3-8746-4936-ad91-2484ecc8b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the converted releases dataset for \"monthly_gdp_releases\"\n",
    "releases_df[\"by_adjusted_monthly_gdp_releases\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641a00c-a19c-4416-8f51-d32c042c4e68",
   "metadata": {},
   "source": [
    "### ðŸ§© Key Takeaways\n",
    "\n",
    "* Input Validation: Ensures matching lengths for input and output file lists.\n",
    "* Sorting and Grouping: Data is sorted by industry, year, and month.\n",
    "* Release Alignment: Non-NaN values are aligned vertically to form a sequence of releases for each industry.\n",
    "* Cleaning: Rows with missing data are dropped, and only valid data is retained.\n",
    "* Pivoting and Reshaping: The data is restructured to group all releases by industry and release sequence.\n",
    "* Saving Results: The final releases dataset is saved as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f6a65-380f-49fe-9385-3038cb9f8c25",
   "metadata": {},
   "source": [
    "> ðŸš€ **Next Steps:** Now that the data has been converted into releases datasets, we can proceed with further analysis, including:\n",
    "> * Revision analysis: Understanding how GDP estimates evolve over time.\n",
    "> * Benchmark testing: Comparing the real-time dataset to benchmark revisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac5e3e-75e2-4b63-8a7c-82fceb9c4f58",
   "metadata": {},
   "source": [
    "<div style=\"background:#3366FF; color:white; padding:12px; box-sizing:border-box; border-radius:4px;\">\n",
    "<b>ðŸ The End</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd24d9-76a6-406b-9af6-20520ec181de",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7e76d-4327-48e8-84f5-f1cfd0c2a041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50641d19-67d4-4978-8eb0-f94e684f31a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdp_revisions",
   "language": "python",
   "name": "gdp_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
