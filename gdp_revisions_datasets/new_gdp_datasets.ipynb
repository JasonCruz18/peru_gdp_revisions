{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127505dd-d5cc-4829-9190-6314ae54b3ca",
   "metadata": {},
   "source": [
    "# New GDP Real-Time Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96357866-58f8-4d8b-b25a-b9b145465322",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Author:** Jason Cruz  \n",
    "**Last updated:** 08/13/2025  \n",
    "**Python version:** 3.12  \n",
    "**Project:** Rationality and Nowcasting on Peruvian GDP Revisions  \n",
    "\n",
    "---\n",
    "## üìå Summary\n",
    "This notebook documents the step-by-step **construction of datasets** for analyzing **Peruvian GDP revisions** from 2013‚Äì2024.  \n",
    "It covers:\n",
    "1. **Data acquisition** from the Central Reserve Bank of Peru's Weekly Reports (PDF).\n",
    "2. **Data cleaning** and extraction of GDP tables.\n",
    "3. **Creation of real-time GDP vintages**.\n",
    "4. **Preparation of the final revisions dataset**.\n",
    "5. **Export to SQL** for further analysis.\n",
    "\n",
    "üåê **Main Data Source:** [BCRP Weekly Report](https://www.bcrp.gob.pe/publicaciones/nota-semanal.html) (üì∞ WR, from here on)  \n",
    "Any questions or issues regarding the coding, please email [Jason üì®](mailto:jj.cruza@alum.up.edu.pe)  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907046a",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac40ed0",
   "metadata": {},
   "source": [
    "If you don't have the libraries below, please use the following code (as example) to install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c73193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d47b8-5873-426b-b739-e1fc05dcf8e5",
   "metadata": {},
   "source": [
    "Check out Python information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55588d8e-8df5-406a-8644-e67ff1dcbc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python Information\n",
      "  Version  : 3.12.1\n",
      "  Compiler : MSC v.1916 64 bit (AMD64)\n",
      "  Build    : ('main', 'Jan 19 2024 15:44:08')\n",
      "  OS       : Windows 10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"üêç Python Information\")\n",
    "print(f\"  Version  : {sys.version.split()[0]}\")\n",
    "print(f\"  Compiler : {platform.python_compiler()}\")\n",
    "print(f\"  Build    : {platform.python_build()}\")\n",
    "print(f\"  OS       : {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b819ed7-9661-4efd-972e-b38e78f48ae3",
   "metadata": {},
   "source": [
    "**Import helper functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a836f",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è Please, check the script `new_gdp_datasets_functions.py` which contains all the functions required by this _jupyter notebook_. The functions there are ordered according to the sections of this jupyter notebok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d1e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from new_gdp_datasets_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ef8f8",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5afa5-c77a-4bd4-a0f4-509e2095728d",
   "metadata": {},
   "source": [
    "Before preprocessing new GDP releases data, we will:\n",
    "\n",
    "* **Create necessary folders** for storing inputs, outputs, logs, and screenshots.\n",
    "* **Connect to the PostgreSQL database** containing GDP revisions datasets.\n",
    "* **Import helper functions** from `new_gdp_datasets_functions.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc1c80",
   "metadata": {},
   "source": [
    "**Create necessary folders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc6db37-7963-4c46-95f7-404ee54664c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51762441-7a80-4c30-ac06-0be260372738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter relative path (default='.'):  .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Using path: C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\RA\\CIUP\\GDP Revisions\\GitHub\\peru_gdp_revisions\\gdp_revisions_datasets\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "user_input = input(\"Enter relative path (default='.'): \").strip() or \".\"\n",
    "target_path = (PROJECT_ROOT / user_input).resolve()\n",
    "target_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÇ Using path: {target_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21c6c2-92fe-453f-9a19-71aee798b2b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e36f49d-15ba-481e-b16b-0ae56d5d0c12",
   "metadata": {},
   "source": [
    "**Connect to the PostgreSQL database**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdd5dc",
   "metadata": {},
   "source": [
    "The following function will establish a connection to the `gdp_revisions_datasets` database in `PostgreSQL`. The **input data** used in this jupyter notebook will be loaded from this `PostgreSQL` database, and similarly, all **output data** generated by this jupyter notebook will be stored in that database. Ensure that you set the necessary parameters to access the server once you have obtained the required permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e982f83",
   "metadata": {},
   "source": [
    "> üí° **Tip:** To request permissions, please email [Jason üì®](mailto:jj.cruza@alum.up.edu.pe)  \n",
    "> ‚ö†Ô∏è **Warning:** Make sure you have set your SQL credentials as environment variables before proceeding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da345bbe-0d12-4340-9275-938bfef26fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b60634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sqlalchemy_engine(database=\"gdp_revisions_datasets\", port=5432):\n",
    "    \"\"\"\n",
    "    Create an SQLAlchemy engine to connect to the PostgreSQL database.\n",
    "    \n",
    "    Environment Variables Required:\n",
    "        CIUP_SQL_USER: SQL username\n",
    "        CIUP_SQL_PASS: SQL password\n",
    "        CIUP_SQL_HOST: SQL host address\n",
    "\n",
    "    Args:\n",
    "        database (str): Name of the database. Default is 'gdp_revisions_datasets'.\n",
    "        port (int): Port number. Default is 5432.\n",
    "\n",
    "    Returns:\n",
    "        engine (sqlalchemy.engine.Engine): SQLAlchemy engine object.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If required environment variables are missing.\n",
    "\n",
    "    Example:\n",
    "        engine = create_sqlalchemy_engine()\n",
    "    \"\"\"\n",
    "    user = os.environ.get('CIUP_SQL_USER')\n",
    "    password = os.environ.get('CIUP_SQL_PASS')\n",
    "    host = os.environ.get('CIUP_SQL_HOST')\n",
    "\n",
    "    if not all([host, user, password]):\n",
    "        raise ValueError(\"‚ùå Missing environment variables: CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS\")\n",
    "\n",
    "    connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    print(f\"üîó Connected to PostgreSQL database: {database} at {host}:{port}\")\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76e853-8dc9-45e6-9f30-cde33dd3966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_sqlalchemy_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f3192",
   "metadata": {},
   "source": [
    "## 1. PDF Downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98250a64",
   "metadata": {},
   "source": [
    "Our main source for data collection is the [BCRP Weekly Report](https://www.bcrp.gob.pe/publicaciones/nota-semanal.html). The weekly report is a periodic (weekly) publication of the BCRP in compliance with article 84 of the Peruvian Constitution and articles 2 and 74 of the BCRP's organic law, which include, among its functions, the periodic publication of the main national macroeconomic statistics.\n",
    "    \n",
    "Our project requires the publication of **two tables**: the table of monthly growth rates of real GDP (12-month percentage changes), and the table of quarterly (annual) growth rates of real GDP. These tables are referred to as **Table 1** and **Table 2**, respectively, throughout this jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d95c04",
   "metadata": {},
   "source": [
    "### Scraper bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcc4e3",
   "metadata": {},
   "source": [
    "This section automates the download of the **BCRP Weekly Report PDFs** directly from the official BCRP website.\n",
    "\n",
    "**What it does:**\n",
    "1. Opens the official BCRP Weekly Report page.\n",
    "2. Finds and collects all PDF links.\n",
    "3. Downloads them in chronological order (oldest to newest).\n",
    "4. Optionally plays a notification sound every N downloads.\n",
    "5. Organizes downloaded PDFs into year-based folders.\n",
    "\n",
    "> üí° If a CAPTCHA appears, solve it manually in the browser window and re-run the cell.\n",
    "\n",
    "> üîÅ This script uses webdriver-manager to automatically handle browser drivers (default: Chrome), so you DO NOT need to manually download ChromeDriver, GeckoDriver, etc. If you want to change browser for your replication, modify the 'browser' parameter in init_driver().\n",
    "\n",
    "> üéµ Place your own MP3 file in `alert_track` folder for download notifications. Recommended free sources (CC0/public domain):\n",
    ">  - Pixabay Audio: https://pixabay.com/music/\n",
    ">  - FreeSound: https://freesound.org/\n",
    ">  - FreePD: https://freepd.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbb4b638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ pdf created\n",
      "üìÇ pdf\\raw created\n",
      "üìÇ pdf\\input created\n",
      "üìÇ record created\n",
      "üìÇ alert_track created\n"
     ]
    }
   ],
   "source": [
    "# Define base folder for saving all digital PDFs\n",
    "pdf_folder = 'pdf'\n",
    "\n",
    "# Define subfolder for saving the original PDFs as downloaded from the BCRP website\n",
    "raw_pdf_subfolder = os.path.join(pdf_folder, 'raw')\n",
    "\n",
    "# Define subfolder for saving reduced PDFs containing only selected pages with GDP growth tables (monthly, quarterly, and annual frequencies)\n",
    "input_pdf_subfolder = os.path.join(pdf_folder, 'input')\n",
    "\n",
    "# Define folder for saving .txt files with download and dataframe record\n",
    "record_folder = 'record'\n",
    "\n",
    "# Define folder for saving warning bells. This is for download notifications (see section 1).\n",
    "alert_track_folder = 'alert_track'\n",
    "\n",
    "# Create all required folders (if they do not already exist) and confirm creation\n",
    "for folder in [pdf_folder, raw_pdf_subfolder, input_pdf_subfolder, record_folder, alert_track_folder]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    print(f\"üìÇ {folder} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0199bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Starting PDF downloader for BCRP WR...\n",
      "\n",
      "üåê BCRP site opened successfully.\n",
      "üîé Found 154 WR blocks on page (one per month).\n",
      "\n",
      "1. ‚úÖ Downloaded: ns-04-2024.pdf\n",
      "‚è≥ Waiting 9.31 seconds...\n",
      "2. ‚úÖ Downloaded: ns-08-2024.pdf\n",
      "‚è≥ Waiting 8.99 seconds...\n",
      "3. ‚úÖ Downloaded: ns-11-2024.pdf\n",
      "‚è≥ Waiting 9.12 seconds...\n",
      "4. ‚úÖ Downloaded: ns-15-2024.pdf\n",
      "‚è≥ Waiting 7.25 seconds...\n",
      "5. ‚úÖ Downloaded: ns-19-2024.pdf\n",
      "‚è≥ Waiting 7.55 seconds...\n",
      "6. ‚úÖ Downloaded: ns-23-2024.pdf\n",
      "‚è≥ Waiting 9.18 seconds...\n",
      "7. ‚úÖ Downloaded: ns-27-2024.pdf\n",
      "‚è≥ Waiting 9.32 seconds...\n",
      "8. ‚úÖ Downloaded: ns-31-2024.pdf\n",
      "‚è≥ Waiting 5.40 seconds...\n",
      "9. ‚úÖ Downloaded: ns-35-2024.pdf\n",
      "‚è≥ Waiting 6.95 seconds...\n",
      "10. ‚úÖ Downloaded: ns-39-2024.pdf\n",
      "‚è≥ Waiting 5.56 seconds...\n",
      "11. ‚úÖ Downloaded: ns-43-2024.pdf\n",
      "‚è≥ Waiting 6.23 seconds...\n",
      "12. ‚úÖ Downloaded: ns-47-2024.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚è∏Ô∏è Continue? (y = yes, any other key = stop):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Waiting 6.50 seconds...\n",
      "13. ‚úÖ Downloaded: ns-04-2025.pdf\n",
      "‚è≥ Waiting 7.05 seconds...\n",
      "14. ‚úÖ Downloaded: ns-08-2025.pdf\n",
      "‚è≥ Waiting 6.53 seconds...\n",
      "15. ‚úÖ Downloaded: ns-11-2025.pdf\n",
      "‚è≥ Waiting 9.75 seconds...\n",
      "16. ‚úÖ Downloaded: ns-14-2025.pdf\n",
      "‚è≥ Waiting 5.34 seconds...\n",
      "17. ‚úÖ Downloaded: ns-18-2025.pdf\n",
      "‚è≥ Waiting 8.65 seconds...\n",
      "18. ‚úÖ Downloaded: ns-22-2025.pdf\n",
      "‚è≥ Waiting 9.55 seconds...\n",
      "19. ‚úÖ Downloaded: ns-26-2025.pdf\n",
      "‚è≥ Waiting 9.61 seconds...\n",
      "20. ‚úÖ Downloaded: ns-30-2025.pdf\n",
      "‚è≥ Waiting 9.12 seconds...\n",
      "21. ‚úÖ Downloaded: ns-34-2025.pdf\n",
      "‚è≥ Waiting 7.39 seconds...\n",
      "22. ‚úÖ Downloaded: ns-36-2025.pdf\n",
      "‚è≥ Waiting 5.08 seconds...\n",
      "\n",
      "üëã Browser closed.\n",
      "\n",
      "üìä Summary:\n",
      "\n",
      "üîó Total monthly links kept: 154\n",
      "üóÇÔ∏è 132 already downloaded PDFs were skipped.\n",
      "‚ûï Newly downloaded: 22\n",
      "‚è±Ô∏è 379 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run the function to start the scraper bot\n",
    "pdf_downloader(\n",
    "    bcrp_url = \"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\",\n",
    "    raw_pdf_folder = raw_pdf_subfolder,\n",
    "    download_record_folder = record_folder,\n",
    "    download_record_txt = '1_downloaded_pdf.txt',\n",
    "    alert_track_folder = alert_track_folder,\n",
    "    max_downloads = 60,\n",
    "    downloads_per_batch = 6, \n",
    "    headless = False \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773491f5-fd62-497b-9f94-59e5095c5659",
   "metadata": {},
   "source": [
    "Probably the üì∞ WR were downloaded in a single folder, but we would like the WR to be sorted by years. The following code sorts the PDFs into subfolders (years) for us by placing each WR according to the year of its publication. This happens in the **\"blink of an eye\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854039b-4e01-47ea-8a6e-eae871829d78",
   "metadata": {},
   "source": [
    "Check your raw_pdf_subfolder out, every PDF should be placed in a year folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee016a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No 4-digit year detected in filename: _quarantine\n"
     ]
    }
   ],
   "source": [
    "# Get the list of files in the directory\n",
    "files = os.listdir(raw_pdf_subfolder)\n",
    "\n",
    "# Call the function to organize files\n",
    "organize_files_by_year(raw_pdf_subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b482a-b33d-4059-b4a2-805ba4612fd8",
   "metadata": {},
   "source": [
    "# WR-08-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a626509-2142-4de1-ba7f-9e47b7a8c81a",
   "metadata": {},
   "source": [
    "This  is crucial for the upcoming steps, specially for the section 3, cleansing. If -in the future- you enconuter some issues by executing cleaing it is likely to atributte to the pdf nature. IN that case, you can return to this code to replace defectiv pdfs for those convinient ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d75a9-308b-4fa1-848a-9771fcffbb1b",
   "metadata": {},
   "source": [
    "Don't worry about it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad4344-deba-476d-91e7-6871c6ba4996",
   "metadata": {},
   "source": [
    "T√∫ puedes hacer lo mismo si te enfrentas a un inconveniente similar. Incluso puedes descargar los casos excepecionales de WR de un mismo mes y reemplazar los defectuosos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e956de-ead1-4439-a3c5-bab8aeb75a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace specific defective PDFs (friendly outputs with icons)\n",
    "replace_ns_pdfs(\n",
    "    items=[\n",
    "        (\"2017\", \"ns-08-2017.pdf\", \"ns-07-2017\"), # Enter the year (folder) that contains the defective PDF, the defective PDF, and the new chosen PDF \n",
    "        (\"2019\", \"ns-23-2019.pdf\", \"ns-22-2019\"), # The same one above\n",
    "    ],\n",
    "    root_folder=input_pdf_subfolder, # base folder with /2017, /2019, ...\n",
    "    record_folder=record_folder, # folder with new_downloaded_pdfs.txt\n",
    "    download_record_txt = 'new_downloaded_pdfs.txt',\n",
    "    quarantine=os.path.join(input_pdf_subfolder, \"_quarantine\")  # set to None to delete instead\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d24d2",
   "metadata": {},
   "source": [
    "## 2. Generate PDF input with key tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666f310",
   "metadata": {},
   "source": [
    "Now that we have downloaded the üì∞ WR from the Central Bank, we should know that each of these files has more than 100 pages, but not all of them contain the information required for this project.\n",
    "\n",
    "All we really want is a couple of pages from each üì∞ WR, one for **Table 1** (monthly real GDP growth) and one for **Table 2** (annual and quarterly real GDP growth). The code below is executed to maintain the **two key pages** with both tables of each PDF plus the cover page that contains the information that helps us identify one üì∞ WR from another such as its date of publication and serial number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6af50-3b5b-44f2-859a-3b37d4b57495",
   "metadata": {},
   "source": [
    "_quarentine will be discard of the input PDF generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to generate trimmed PDFs for input\n",
    "input_pdfs_generator(\n",
    "    raw_pdf_folder = raw_pdf_subfolder,\n",
    "    input_pdf_folder = input_pdf_subfolder,\n",
    "    input_pdf_record_folder = record_folder,\n",
    "    input_pdf_record_txt = 'new_generated_input_pdfs.txt',\n",
    "    keywords = [\"ECONOMIC SECTORS\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643079a",
   "metadata": {},
   "source": [
    "Again, probably the WR (PDF files, now of few pages) were stored in disorder in the `input_pdf_folder` folder. The following code sorts the PDFs into subfolders (years) by placing each WR (which now includes only the key tables) according to the year of its publication. This happens in the **\"blink of an eye\"**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the directory\n",
    "files = os.listdir(input_pdf_subfolder)\n",
    "\n",
    "# Call the function to organize files\n",
    "organize_files_by_year(input_pdf_subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e1e92",
   "metadata": {},
   "source": [
    "## 3. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100857d4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "Since we already have the PDFs <span style=\"font-size: 24px;\">&#128462;</span> with just the tables required for this project, we can start extracting them. Then we can proceed with data cleaning.\n",
    "</p>  \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357318e8",
   "metadata": {},
   "source": [
    "### 3.2 Extracting tables and data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea58b1c",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "The main library used for extracting tables from PDFs <span style=\"font-size: 24px;\">&#128462;</span> is <code>pdfplumber</code>. You can review the official documentation by clicking <a href=\"https://github.com/jsvine/pdfplumber\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">here</a>.\n",
    "</p>\n",
    "    \n",
    "<p>     \n",
    "    The functions in <b>Section 3</b> of the <code>\"new_gdp_datasets_functions.py\"</code> script were built to deal with each of these issues. An interesting exercise is to compare the original tables (the ones in the PDF <span style=\"font-size: 24px;\">&#128462;</span>) and the cleaned tables (by the cleanup codes below). Thus, the cleanup codes for <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 1</a> and <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 2</a> generates two dictionaries, the first one stores the raw tables; that is, the original tables from the PDF <span style=\"font-size: 24px;\">&#128462;</span> extracted by the <code>pdfplumber</code> library, while the second dictionary stores the fully cleaned tables.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070fb47",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    The code iterates through each PDF <span style=\"font-size: 24px;\">&#128462;</span> and extracts the two required tables from each. The extracted information is then transformed into dataframes and the columns and values are cleaned up to conform to Python conventions (pythonic).\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8436c2d1",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.2.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    <span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">Table 1.</span> Extraction and cleaning of data from tables on monthly real GDP growth rates.\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65139cc2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "The basic criterion to start extracting tables is to use keywords (sufficient condition). I mean, tables containing the following keywords meet the requirements to be extracted.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4ac96",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"font-size: 24px; color: rgb(255, 32, 78); font-weight: bold;\">&#9888;</span>\n",
    "    <span style=\"font-family: PT Serif Pro Book; color: black; font-size: 16px;\">\n",
    "        Please check that the flat file <b>\"ns_dates.csv\"</b> is updated with the dates, years and ids for the newly downloaded PDF <span style=\"font-size: 24px;\">&#128462;</span> (WR). That file is located in the <b>\"ns_dates\"</b> folder and is uploaded to SQL from the jupyeter notebook <code>aux_files_to_sql.ipynb</code>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffcd08-7763-4eed-ad99-d3bce3644f90",
   "metadata": {},
   "source": [
    "Si por alguna raz√≥n ejecutas el c√≥digo de la secci√≥n 3 y no continuas ejecutando la secci√≥n subsecuente, puedes estar tranquilo de que un registro los guard√≥. La pr√≥xima vez que visite este script basta con empezar desde esta secci√≥n 3 (eliminando el txt) para generar los dataframes que no se guardaron en ningun lado, estos son insumos esenciales para la secci√≥n 4. Alternativamente puede guardar todos los dataframes generados en una carpeta como respaldo y empezar desde la secci√≥n 4 carg√°ndolos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd60792-ec6d-441c-a61a-2ced241a4f2f",
   "metadata": {},
   "source": [
    "# Section 3 ‚Äî Cleaning pipelines (Table 1 & Table 2)\n",
    "\n",
    "### Functions\n",
    "- `table_1_cleaner(...)` ‚Üí cleans **Table 1 (monthly)** pages and returns two dicts:\n",
    "  - `raw_tables_dict_1`: raw tables exactly as extracted from PDFs, keyed as `ns_xx_yyyy_1`.\n",
    "  - `new_dataframes_dict_1`: cleaned tables ready for downstream steps, keyed as `ns_xx_yyyy_1`.\n",
    "\n",
    "- `table_2_cleaner(...)` ‚Üí cleans **Table 2 (quarterly/annual)** pages and returns:\n",
    "  - `raw_tables_dict_2`: raw tables, keyed as `ns_xx_yyyy_2`.\n",
    "  - `new_dataframes_dict_2`: cleaned tables, keyed as `ns_xx_yyyy_2`.\n",
    "\n",
    "Both functions:\n",
    "- **skip** year folder `_quarantine`\n",
    "- maintain a **record txt** (chronologically sorted: year ‚Üí issue)  \n",
    "- show **Jupyter progress bars** (magenta = active, blue = finished)\n",
    "- write a **log file**:\n",
    "  - Table 1 ‚Üí `logs/3_cleaner_1.log`\n",
    "  - Table 2 ‚Üí `logs/3_cleaner_2.log`\n",
    "\n",
    "### Arguments\n",
    "- `input_pdf_folder: str`  \n",
    "  Root containing year subfolders with input PDFs (e.g., `input_pdf_subfolder/2017/ns-07-2017.pdf`).\n",
    "\n",
    "- `record_folder: str`  \n",
    "  Folder where the record txt is stored (e.g., `record/`).\n",
    "\n",
    "- `record_txt: str` *(optional)*  \n",
    "  Record filename. Defaults:\n",
    "  - Table 1 ‚Üí `new_generated_dataframes_1.txt`\n",
    "  - Table 2 ‚Üí `new_generated_dataframes_2.txt`\n",
    "\n",
    "- `log_folder: str` *(optional, default `logs`)*  \n",
    "  Where the `.log` files are written.\n",
    "\n",
    "- `log_txt: str` *(optional)*  \n",
    "  Log filename. Defaults:\n",
    "  - Table 1 ‚Üí `3_cleaner_1.log`\n",
    "  - Table 2 ‚Üí `3_cleaner_2.log`\n",
    "\n",
    "- `persist: bool` *(optional, default `False`)*  \n",
    "  If `True`, save cleaned tables to disk and update a manifest.  \n",
    "  If `False`, nothing is saved (keeps repo light and re-runnable).\n",
    "\n",
    "- `persist_folder: str | None` *(optional)*  \n",
    "  Base folder for persisted outputs (default: `./data/clean`).  \n",
    "  Layout when `persist=True`:\n",
    "    data/clean/\n",
    "    table_1/\n",
    "    manifest.csv\n",
    "    2017/\n",
    "    ns-07-2017.parquet (or .csv if Parquet engine unavailable)\n",
    "    table_2/\n",
    "    manifest.csv\n",
    "    2017/\n",
    "    ns-07-2017.parquet\n",
    "\n",
    "- `pipeline_version: str` *(optional, default `\"s3.0.0\"`)*  \n",
    "Version tag recorded in `manifest.csv` for cache/audit. Bump it when the cleaning logic changes.\n",
    "\n",
    "### Typical calls\n",
    "\n",
    "```python\n",
    "# Table 1 (monthly)\n",
    "raw_1, clean_1 = table_1_cleaner(\n",
    "  input_pdf_folder=input_pdf_subfolder,\n",
    "  record_folder=record_folder,\n",
    "  persist=True,                           # turn on checkpointing (Parquet/CSV + manifest)\n",
    "  persist_folder=clean_data,              # e.g., os.path.join(project_root, \"data\", \"clean\")\n",
    "  pipeline_version=\"s3.0.0\"\n",
    ")\n",
    "\n",
    "# Table 2 (quarterly/annual)\n",
    "raw_2, clean_2 = table_2_cleaner(\n",
    "  input_pdf_folder=input_pdf_subfolder,\n",
    "  record_folder=record_folder,\n",
    "  persist=True,\n",
    "  persist_folder=clean_data,\n",
    "  pipeline_version=\"s3.0.0\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4aabed-81df-4d47-af89-ab1fc70f6f0c",
   "metadata": {},
   "source": [
    "\n",
    "If you want the runners to *also* write the cleaned dicts out to a single combined Parquet/CSV per table (alongside the per-WR files), I can add that as an optional flag (`persist_combined=True`) without changing the defaults.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7075989-80c9-42f6-8a9e-df0728e67ace",
   "metadata": {},
   "source": [
    "# If you will run until this section and you are planning to go back and retake from section 4, enter \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2388f-faa2-4eb7-abce-8b61e8d7b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data'\n",
    "\n",
    "clean_data = os.path.join(\"data\", \"clean\")  # choose your path\n",
    "\n",
    "raw_1, clean_1, vintages_1 = table_1_cleaner(\n",
    "    input_pdf_folder=input_pdf_subfolder,\n",
    "    record_folder=record_folder,\n",
    "    persist=True,\n",
    "    persist_folder=clean_data,\n",
    "    pipeline_version=\"s3.0.0\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3004d-e825-41a2-8247-81fa5cceec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "vintages_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6cc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_1['ns_04_2022_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ecc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_1['ns_04_2022_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1392888-be8b-4b95-98cf-ebd71d34dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vintages_1['ns_04_2022_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a4fe9-54cc-48b6-901c-b0b5396fe68d",
   "metadata": {},
   "source": [
    "# Checking the cleaning version out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88e1dd-8c85-481b-9a42-3d3b8174cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df100 = vintages_1[\"ns_04_2022_1\"]\n",
    "print(df100.attrs)\n",
    "# {'pipeline_version': 's3.0.0'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb48b6ed-b2ce-44cd-b19e-930ff305f8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb8229f-e230-494f-97b1-bcf4e6a9ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vintages_1[\"ns_04_2022_1\"].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3f814",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.2.2.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    <span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">Table 2.</span> Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419acde4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "The basic criterion to start extracting tables is to use keywords (sufficient condition). I mean, tables containing the following keywords meet the requirements to be extracted.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86af3dc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"font-size: 24px; color: rgb(255, 32, 78); font-weight: bold;\">&#9888;</span>\n",
    "    <span style=\"font-family: PT Serif Pro Book; color: black; font-size: 16px;\">\n",
    "        Please check that the flat file <b>\"ns_dates.csv\"</b> is updated with the dates, years and ids for the newly downloaded PDF <span style=\"font-size: 24px;\">&#128462;</span> (WR). That file is located in the <code>ns_dates</code> folder and is uploaded to SQL from the jupyeter notebook <code>aux_files_to_sql.ipynb</code>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fcfe1b-9f63-4850-937f-ca582be3ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2 (quarterly/annual)\n",
    "raw_2, clean_2 = table_2_cleaner(\n",
    "  input_pdf_folder=input_pdf_subfolder,\n",
    "  record_folder=record_folder,\n",
    "  persist=True,\n",
    "  persist_folder=clean_data,\n",
    "  pipeline_version=\"s3.0.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_2['ns_04_2022_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_2['ns_04_2022_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8989d65",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">GDP Real-Time dataset</span></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f919d9-03bc-44fb-ac9d-7e0cbc0305b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def build_month_order_map(year_folder: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a {filename:month_order} dictionary mapping based on the numeric value (dd)\n",
    "    of the ns-dd-yyyy pattern of each PDF file in the year_folder. This ensures a correct\n",
    "    chronological order, regardless of irregularities in the numbering.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(year_folder) if f.endswith(\".pdf\")]\n",
    "    # Extract the numeric middle part (dd)\n",
    "    file_nums = [(f, int(re.search(r'ns-(\\d{2})-\\d{4}', f).group(1))) for f in files]\n",
    "    # Sort by dd and assign month order 1..12\n",
    "    sorted_files = sorted(file_nums, key=lambda x: x[1])\n",
    "    return {fname: i + 1 for i, (fname, _) in enumerate(sorted_files)}\n",
    "\n",
    "\n",
    "def prepare_table_1(filepath: str, month_order_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    # 1. Assign month order (already extracted from all filenames)\n",
    "    df[\"month\"] = build_month_order_map.get(filename)\n",
    "\n",
    "    # 2. Drop unused columns\n",
    "    df = df.drop(columns=[\"wr\", \"sectores_economicos\"], errors=\"ignore\")\n",
    "\n",
    "    # 3. Create short economic sector labels\n",
    "    SECTOR_MAP = {\n",
    "        \"agriculture and livestock\": \"agriculture\",\n",
    "        \"fishing\": \"fishing\",\n",
    "        \"mining and fuel\": \"mining\",\n",
    "        \"manufacturing\": \"manufacturing\",\n",
    "        \"electricity and water\": \"electricity\",\n",
    "        \"construction\": \"construction\",\n",
    "        \"commerce\": \"commerce\",\n",
    "        \"other services\": \"services\",\n",
    "        \"gdp\": \"gdp\"\n",
    "    }\n",
    "    df[\"economic_sector\"] = df[\"economic_sectors\"].map(SECTOR_MAP)\n",
    "\n",
    "    # 4. Filter valid sectors\n",
    "    df = df[df[\"economic_sector\"].notna()].copy()\n",
    "\n",
    "    # 5. Create 'vintage_id'\n",
    "    df[\"vintage_id\"] = (\n",
    "        df[\"economic_sector\"] + \"_\" + df[\"year\"].astype(str) + \"_\" + df[\"month\"].astype(str)\n",
    "    )\n",
    "\n",
    "    # 6. Keep only yyyy_mmm columns\n",
    "    pattern = re.compile(r\"^\\d{4}_(ene|feb|mar|abr|may|jun|jul|ago|sep|oct|nov|dic)$\")\n",
    "    cols_to_keep = [\"vintage_id\"] + [c for c in df.columns if pattern.match(c)]\n",
    "    df = df[cols_to_keep]\n",
    "\n",
    "    # 7. Transpose and convert to tidy long format\n",
    "    df_t = df.set_index(\"vintage_id\").T.reset_index().rename(columns={\"index\": \"target_period\"})\n",
    "    \n",
    "    # Mapping from Spanish month abbreviations to month numbers\n",
    "    month_map = {\n",
    "        \"ene\": \"01\", \"feb\": \"02\", \"mar\": \"03\", \"abr\": \"04\", \"may\": \"05\", \"jun\": \"06\",\n",
    "        \"jul\": \"07\", \"ago\": \"08\", \"sep\": \"09\", \"oct\": \"10\", \"nov\": \"11\", \"dic\": \"12\"\n",
    "    }\n",
    "    \n",
    "    # Convert to 'yyyymx' format (string)\n",
    "    df_t[\"target_period\"] = (\n",
    "        df_t[\"target_period\"]\n",
    "        .str.replace(\n",
    "            r\"(\\d{4})_(\\w{3})\",\n",
    "            lambda m: f\"{m.group(1)}m{int(month_map.get(m.group(2).lower(), '1'))}\",\n",
    "            regex=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e81be28-da59-43d2-b8c3-4e9dc6b9ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_table1_year(folder_path: str, year: str) -> dict:\n",
    "    \"\"\"Clean all monthly files for a given year and return dict of cleaned DataFrames.\"\"\"\n",
    "    year_folder = os.path.join(folder_path, year)\n",
    "    month_order_map = build_month_order_map(year_folder)\n",
    "    cleaned = {}\n",
    "    for file in sorted(month_order_map.keys()):\n",
    "        cleaned[file] = clean_table_1_file(os.path.join(year_folder, file), month_order_map)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a751f6d-fa85-4fa8-aa9a-78481bfe9304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£  Define your base path (adjust to your project)\n",
    "base_path = \"data/clean\"\n",
    "table1_path = os.path.join(base_path, \"table_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e35540-f17a-40c3-b5d7-c4334a441626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5Ô∏è‚É£  (Optional) Save each cleaned dataframe as a new CSV in a folder ‚Äúclean_1‚Äù\n",
    "output_folder = os.path.join(base_path, \"clean_1\")\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d4155-4d25-4c77-81a6-e6a84b8ea3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in sorted(os.listdir(table1_path)):\n",
    "    year_folder = os.path.join(table1_path, year)\n",
    "    if os.path.isdir(year_folder):\n",
    "        print(f\"Cleaning year {year} ...\")\n",
    "        cleaned_dict = clean_table1_year(table1_path, year)\n",
    "\n",
    "        # Save all monthly cleaned tables for this year\n",
    "        for file, df in cleaned_dict.items():\n",
    "            name = f\"clean_1_{year}_{file.replace('.csv', '')}.csv\"\n",
    "            df.to_csv(os.path.join(output_folder, name), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da68b7-e1de-47d8-b91d-7e340d812a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e0af1-0bd8-41b3-bb16-99ddc20398dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dict[\"ns-04-2022.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f704988-9e39-4161-bd1b-802d672e8fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7e2b4-922e-4cf3-ab54-e82e11fb3018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee65ce9f-0587-4844-b95f-a107553b1c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1385bc-d648-4935-a685-31f8be1a60fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_table_2(filepath: str, month_order_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    # 2. Drop unused columns\n",
    "    df = df.drop(columns=[\"wr\", \"sectores_economicos\"], errors=\"ignore\")\n",
    "\n",
    "    # 3. Create short economic sector labels\n",
    "    SECTOR_MAP = {\n",
    "        \"agriculture and livestock\": \"agriculture\",\n",
    "        \"fishing\": \"fishing\",\n",
    "        \"mining and fuel\": \"mining\",\n",
    "        \"manufacturing\": \"manufacturing\",\n",
    "        \"electricity and water\": \"electricity\",\n",
    "        \"construction\": \"construction\",\n",
    "        \"commerce\": \"commerce\",\n",
    "        \"other services\": \"services\",\n",
    "        \"gdp\": \"gdp\"\n",
    "    }\n",
    "    df[\"economic_sector\"] = df[\"economic_sectors\"].map(SECTOR_MAP)\n",
    "\n",
    "    # 4. Filter valid sectors\n",
    "    df = df[df[\"economic_sector\"].notna()].copy()\n",
    "\n",
    "    # 5. Create 'vintage_id'\n",
    "    df[\"vintage_id\"] = (\n",
    "        df[\"economic_sector\"] + \"_\" + df[\"year\"].astype(str) + \"_\" + df[\"month\"].astype(str)\n",
    "    )\n",
    "\n",
    "    # 6. Keep only yyyy_x columns where x ‚àà {1, 2, 3, 4, year}\n",
    "    pattern = re.compile(r\"^\\d{4}_(1|2|3|4|year)$\")\n",
    "    cols_to_keep = [\"vintage_id\"] + [c for c in df.columns if pattern.match(c)]\n",
    "    df = df[cols_to_keep]\n",
    "\n",
    "    # 7. Transpose and convert to tidy long format\n",
    "    df_t = df.set_index(\"vintage_id\").T.reset_index().rename(columns={\"index\": \"target_period\"})\n",
    "    \n",
    "    # Convert 'target_period' values:\n",
    "    # - yyyy_(1|2|3|4) ‚Üí yyyyq(1|2|3|4)\n",
    "    # - yyyy_year ‚Üí yyyy\n",
    "    df_t[\"target_period\"] = (\n",
    "        df_t[\"target_period\"]\n",
    "        .str.replace(r\"^(\\d{4})_(\\d)$\", r\"\\1q\\2\", regex=True)  # quarterly pattern\n",
    "        .str.replace(r\"^(\\d{4})_year$\", r\"\\1\", regex=True)     # annual pattern\n",
    "    )\n",
    "\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acddd8-0762-4c14-a6e7-824e58dad946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ff022-5687-468f-9e9f-8e252b376f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_table2_year(folder_path: str, year: str) -> dict:\n",
    "    \"\"\"Clean all monthly files for a given year and return dict of cleaned DataFrames.\"\"\"\n",
    "    year_folder = os.path.join(folder_path, year)\n",
    "    month_order_map = build_month_order_map(year_folder)\n",
    "    cleaned = {}\n",
    "    for file in sorted(month_order_map.keys()):\n",
    "        cleaned[file] = clean_table_2_file(os.path.join(year_folder, file), month_order_map)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823dcf81-ac41-4ab4-be51-7489743b32ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£  Define your base path (adjust to your project)\n",
    "base_path = \"data/clean\"\n",
    "table2_path = os.path.join(base_path, \"table_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe1e0f-6bef-4dd8-942b-f1ad2ce4e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5Ô∏è‚É£  (Optional) Save each cleaned dataframe as a new CSV in a folder ‚Äúclean_1‚Äù\n",
    "output_folder = os.path.join(base_path, \"clean_2\")\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8a877-30fb-485f-aa39-e17c3ebaeb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in sorted(os.listdir(table2_path)):\n",
    "    year_folder = os.path.join(table2_path, year)\n",
    "    if os.path.isdir(year_folder):\n",
    "        print(f\"Cleaning year {year} ...\")\n",
    "        cleaned_dict_2 = clean_table2_year(table2_path, year)\n",
    "\n",
    "        # Save all monthly cleaned tables for this year\n",
    "        for file, df in cleaned_dict.items():\n",
    "            name = f\"clean_2_{year}_{file.replace('.csv', '')}.csv\"\n",
    "            df.to_csv(os.path.join(output_folder, name), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4f137-dfc0-4685-9b03-7215febdac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dict_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e26da-d6fe-4208-98ad-ba97726147eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dict_2[\"ns-12-2022.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfeae0-b5b0-4629-855a-a5556a43465a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdp_revisions",
   "language": "python",
   "name": "gdp_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
