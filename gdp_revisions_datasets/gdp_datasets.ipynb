{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd9a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a055b3b9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter bt pro roman'; color: rgb(0, 65, 75);\">\n",
    "    <h1>\n",
    "    GDP Revisions Datasets\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd88a4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter bt pro roman'; color: rgb(0, 65, 75);\">\n",
    "    <h3>\n",
    "        Documentation\n",
    "        <br>\n",
    "        ____________________\n",
    "            </br>\n",
    "    </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d22837",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'PT Serif Pro Book'; color: rgb(0, 65, 75); font-size: 16px;\">\n",
    "    Jason Cruz\n",
    "    <br>\n",
    "    <a href=\"mailto:jj.cruza@up.edu.pe\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">\n",
    "        jj.cruza@up.edu.pe\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72f519",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "This <span style=\"color: rgb(0, 65, 75);\">jupyter notebook</span> documents step-by-step the <b>construction of datasets</b> for the project <b>'Revisions and biases in preliminary GDP estimates in Peru'</b>.\n",
    "\n",
    "This jupyter notebook goes from downloading the Weekly Notes (NS) from the Central Reserve Bank of Peru (BCRP), stored on their website as PDF files, to generating datasets of growth rates and revisions to Peru's GDP, loaded as tables to SQL. The NS contain the information on annual, quarterly and monthly GDP growth rates by economic sectors of Peru, while the main datasets that will be used for the data analysis of this project are generated in this jupyter notebook using big data and machine learning techniques.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28540df2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Amaya; text-align: left; color: rgb(0, 65, 75); font-size:16px\">The following <b>outline is functional</b>. By utilising the provided buttons, users are able to enhance their experience by browsing this script.<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc04c9",
   "metadata": {},
   "source": [
    "<div id=\"outilne\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0fa13",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #292929; padding: 10px;\">\n",
    "<h2 style=\"text-align: left; font-family: 'charter'; color: #E0E0E0;\">\n",
    "    Outline\n",
    "    </h2>\n",
    "    <br>\n",
    "    <a href=\"#libraries\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        Libraries</a>\n",
    "    <br>\n",
    "    <a href=\"#setup\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        Initial set-up</a>\n",
    "    <br>\n",
    "    <a href=\"#1\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        1. PDF Downloader</a>\n",
    "    <br>\n",
    "    <a href=\"#2\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        2. Generate PDF input with key tables</a>\n",
    "    <br>\n",
    "    <a href=\"#3\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        3. Data cleaning</a>\n",
    "    <br>\n",
    "    <a href=\"#3-1\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.1. A brief documentation on issus in the table information of the PDFs.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-2\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.2. Extracting tables and data cleanup.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.2.1. Tabla 1. Extraction and cleaning of data from tables on monthly real GDP growth rates.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-2-2\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.2.2. Tabla 2. Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.</a>\n",
    "    <br>\n",
    "    <a href=\"#4\" style=\"color: #687EFF; font-size: 18px;\">4. SQL Tables</a>\n",
    "    <br>\n",
    "    <a href=\"#4-1\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        4.1. Annual Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#4-2\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        4.2. Quarterly Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#4-3\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        4.3. Monthly Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#4-4\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        4.4. Loading SQL.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90323106",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    Any questions or issues regarding the coding, please <a href=\"mailto:jj.cruza@alum.up.edu.pe\" style=\"color: rgb(0, 153, 123)\">email Jason Cruz\n",
    "    </a>.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac40ed0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\"\">\n",
    "    If you don't have the libraries below, please use the following code (as example) to install the required libraries.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c73193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87292fcf",
   "metadata": {},
   "source": [
    "<div id=\"libraries\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907046a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark;\">\n",
    "    <h2>\n",
    "    Libraries\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214f5982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# 1. PDF downloader\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os  # For file and directory manipulation, for interacting with the operating system\n",
    "import random  # To generate random numbers\n",
    "from selenium import webdriver  # For automating web browsers\n",
    "from selenium.webdriver.common.by import By  # To locate elements on a webpage\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # To wait until certain conditions are met on a webpage.\n",
    "from selenium.webdriver.support import expected_conditions as EC  # To define expected conditions\n",
    "from selenium.common.exceptions import StaleElementReferenceException  # To handle exceptions related to elements on the webpage that are no longer available.\n",
    "import pygame # Allows you to handle graphics, sounds and input events.\n",
    "\n",
    "import shutil # Used for high-level file operations, such as copying, moving, renaming, and deleting files and directories.\n",
    "\n",
    "\n",
    "# 2. Generate PDF input with key tables\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import fitz  # This library is used for working with PDF documents, including reading, writing, and modifying PDFs (PyMuPDF).\n",
    "import tkinter as tk  # This library is used for creating graphical user interfaces (GUIs) in Python.\n",
    "\n",
    "\n",
    "# 3. Data cleaning\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 3.1. A brief documentation on issus in the table information of the PDFs\n",
    "\n",
    "from PIL import Image  # Used for opening, manipulating, and saving image files.\n",
    "import matplotlib.pyplot as plt  # Used for creating static, animated, and interactive visualizations.\n",
    "\n",
    "# 3.2. Extracting tables and data cleanup\n",
    "\n",
    "import pdfplumber  # For extracting text and metadata from PDF files\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import unicodedata  # For manipulating Unicode data\n",
    "import re  # For regular expressions operations\n",
    "from datetime import datetime  # For working with dates and times\n",
    "import locale  # For locale-specific formatting of numbers, dates, and currencies\n",
    "\n",
    "# 3.2.1. Tabla 1. Extraction and cleaning of data from tables on monthly real GDP growth rates\n",
    "\n",
    "import tabula  # Used to extract tables from PDF files into pandas DataFrames\n",
    "from tkinter import Tk, messagebox, TOP, YES, NO  # Used for creating graphical user interfaces\n",
    "from sqlalchemy import create_engine  # Used for connecting to and interacting with SQL databases\n",
    "\n",
    "# 3.2.2. Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates\n",
    "\n",
    "import roman\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# 4. SQL tables\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import psycopg2  # For interacting with PostgreSQL databases\n",
    "from sqlalchemy import create_engine, text  # For creating and executing SQL queries using SQLAlchemy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e776e8d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db739b",
   "metadata": {},
   "source": [
    "<div id=\"setup\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ef8f8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark;\">\n",
    "    <h2>\n",
    "    Initial set-up\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc1c80",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\"> The following code lines will create folders in your current path, call them to import and export your outputs. <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8899a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save downloaded PDF files\n",
    "\n",
    "raw_pdf = 'raw_pdf' # to save raw data (.pdf).\n",
    "if not os.path.exists(raw_pdf):\n",
    "    os.mkdir(raw_pdf) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26b4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save text file with the names of already downloaded files\n",
    "\n",
    "download_record = 'download_record'\n",
    "if not os.path.exists(download_record):\n",
    "    os.mkdir(download_record) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147227ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to download the trimmed PDF files (these are PDF inputs for the extraction and cleanup code)\n",
    "\n",
    "input_pdf = 'input_pdf'\n",
    "if not os.path.exists(input_pdf):\n",
    "    os.makedirs(input_pdf) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6dc316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save PDF files containing only the pages of interest (where the GDP growth rate tables are located)\n",
    "\n",
    "input_pdf_record = 'input_pdf_record'\n",
    "if not os.path.exists(input_pdf_record):\n",
    "    os.makedirs(input_pdf_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d69eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save dataframes generated record by year\n",
    "\n",
    "dataframes_record = 'dataframes_record'\n",
    "if not os.path.exists(dataframes_record):\n",
    "    os.makedirs(dataframes_record) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21e9ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save sound files\n",
    "\n",
    "sound_folder = 'sound'\n",
    "if not os.path.exists(sound_folder):\n",
    "    os.makedirs(sound_folder) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3957a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save screenshots about issues in the table information\n",
    "\n",
    "ns_issues_folder = 'ns_issues_folder'\n",
    "if not os.path.exists(ns_issues_folder):\n",
    "    os.makedirs(ns_issues_folder) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e154c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"font-size: 24px; color: rgb(178, 6, 0); font-weight: bold;\">&#9888;</span>\n",
    "    <span style=\"font-family: PT Serif Pro Book; color: black; font-size: 16px;\">\n",
    "        Import all functions required by this jupyter notebook.\n",
    "    </span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc1bce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdp_revisions_datasets_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0038d2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e051c",
   "metadata": {},
   "source": [
    "<div id=\"1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f3192",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: 'PT Serif Pro Book'; color: dark;\">1.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">PDF Downloader</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98250a64",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Our main source for data collection is the <a href=\"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\" style=\"color: rgb(0, 153, 123)\">BCRP's web page</a>. The weekly note is a periodic (weekly) publication of the BCRP in compliance with article 84 of the Peruvian Constitution and articles 2 and 74 of the BCRP's organic law, which include, among its functions, the periodic publication of the main national macroeconomic statistics.\n",
    "    \n",
    "Our project requires the publication of two tables: the table of monthly growth rates of real GDP (12-month percentage changes), and the table of quarterly (annual) growth rates of real GDP. These tables are referred to as Table 1 and Table 2, respectively, throughout this jupyter notebook.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcc4e3",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    The following bot runs the following steps:\n",
    "    <ol>\n",
    "        <li>Download the PDF files (NS) from the BCRP web page, starting with the oldest and continuing to the most recent.</li>\n",
    "        <li>Notify you with a fabulous song each time a certain number of downloads is reached.</li>\n",
    "        <li>Display a window asking if you want to continue with the downloads. You can stop them at any time.</li>\n",
    "        <li>Report in detail about the downloaded files. If a file has already been downloaded, you will also be notified.</li>\n",
    "        <li>Save the raw PDFs to the paths set in the preamble of this Jupyter Notebook.</li>\n",
    "    </ol>\n",
    "    Try the bot, it's an adventure!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the BCRP URL\n",
    "bcrp_url = \"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\"  # Never replace this URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6be0c",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    The next stage in the process will be to execute the code which enables the bot to carry out the downloading tasks.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0199bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize pygame\n",
    "pygame.mixer.init()\n",
    "\n",
    "# List of available sound files\n",
    "available_sounds = os.listdir(sound_folder)\n",
    "\n",
    "# Select a random sound\n",
    "random_sound = random.choice(available_sounds)\n",
    "\n",
    "# Full path of the random sound\n",
    "sound_path = os.path.join(sound_folder, random_sound)\n",
    "\n",
    "# Load the selected sound\n",
    "pygame.mixer.music.load(sound_path)\n",
    "\n",
    "# List to keep track of successfully downloaded files\n",
    "downloaded_files = []\n",
    "\n",
    "# Load the list of previously downloaded files if it exists\n",
    "if os.path.exists(os.path.join(download_record, \"downloaded_files.txt\")):\n",
    "    with open(os.path.join(download_record, \"downloaded_files.txt\"), \"r\") as f:\n",
    "        downloaded_files = f.read().splitlines()\n",
    "\n",
    "# Web driver setup\n",
    "\n",
    "'''\n",
    "Nota: Download chrome.exe from 'https://googlechromelabs.github.io/chrome-for-testing/#stable'\n",
    "and call in (1) the folder where you saved this application.\n",
    "'''\n",
    "driver_path = os.environ.get('driver_path') # (1)\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "# Number of downloads per batch\n",
    "downloads_per_batch = 5\n",
    "# Total number of downloads\n",
    "total_downloads = 5\n",
    "\n",
    "try:\n",
    "    # Open the test page\n",
    "    driver.get(bcrp_url)\n",
    "    print(\"Site opened successfully\")\n",
    "\n",
    "    # Wait for the container area to be present\n",
    "    wait = WebDriverWait(driver, 60)\n",
    "    container_area = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"rightside\"]')))\n",
    "\n",
    "    # Get all the links within the container area\n",
    "    pdf_links = container_area.find_elements(By.XPATH, './/a')\n",
    "\n",
    "    # Reverse the order of links\n",
    "    pdf_links = list(reversed(pdf_links))\n",
    "\n",
    "    # Initialize download counter\n",
    "    download_counter = 0\n",
    "\n",
    "    # Iterate over reversed links and download PDFs in batches\n",
    "    for pdf_link in pdf_links:\n",
    "        download_counter += 1\n",
    "\n",
    "        # Get the file name from the URL\n",
    "        new_url = pdf_link.get_attribute(\"href\")\n",
    "        file_name = new_url.split(\"/\")[-1]\n",
    "\n",
    "        # Check if the file has already been downloaded\n",
    "        if file_name in downloaded_files:\n",
    "            print(f\"{download_counter}. The file {file_name} has already been downloaded previously. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Try to download the file\n",
    "        try:\n",
    "            download_pdf(driver, pdf_link, wait, download_counter, raw_pdf, download_record)\n",
    "\n",
    "            # Update the list of downloaded files\n",
    "            downloaded_files.append(file_name)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading the file {file_name}: {str(e)}\")\n",
    "\n",
    "        # If the download count reaches a multiple of batch size, notify\n",
    "        if download_counter % downloads_per_batch == 0:\n",
    "            print(f\"Batch {download_counter // downloads_per_batch} of {downloads_per_batch} completed\")\n",
    "\n",
    "        # If the download count reaches a multiple of 25, ask the user if they want to continue\n",
    "        if download_counter % 5 == 0: # after the fifth PDF downloaded, you'll listen a beautiful song\n",
    "            play_sound()\n",
    "            user_input = input(\"Do you want to continue downloading? (Enter 'y' to continue, any other key to stop): \")\n",
    "            pygame.mixer.music.stop()\n",
    "            if user_input.lower() != 'y':\n",
    "                break\n",
    "\n",
    "        # Random wait before the next iteration\n",
    "        random_wait(5, 10)\n",
    "\n",
    "        # If total downloads reached, break out of loop\n",
    "        if download_counter == total_downloads:\n",
    "            print(f\"All downloads completed ({total_downloads} in total)\")\n",
    "            break\n",
    "\n",
    "except StaleElementReferenceException:\n",
    "    print(\"StaleElementReferenceException occurred. Retrying...\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser when finished\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168995d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "Probably the NS (PDF files) were downloaded in a single folder (raw_pdf), but we would like the NS to be sorted by years. The following code sorts the PDFs into subfolders (years) for us by placing each NS according to the year of its publication. This happens in the \"blink of an eye\". \n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee016a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the directory\n",
    "files = os.listdir(raw_pdf)\n",
    "\n",
    "# Call the function to organize files\n",
    "organize_files_by_year(raw_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9113a",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28a023",
   "metadata": {},
   "source": [
    "<div id=\"2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d24d2",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;; color: dark;\">2.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Generate PDF input with key tables</span></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopupWindow(tk.Toplevel):\n",
    "    \"\"\"Creates a pop-up window for user interaction.\"\"\"\n",
    "\n",
    "    def __init__(self, root, message):\n",
    "        \"\"\"Initialize the pop-up window.\"\"\"\n",
    "        super().__init__(root)\n",
    "        self.root = root\n",
    "        self.title(\"Attention!\")\n",
    "        self.message = message\n",
    "        self.result = None\n",
    "        self.configure_window()\n",
    "        self.create_widgets()\n",
    "\n",
    "    def configure_window(self):\n",
    "        \"\"\"Configure the window to be non-resizable.\"\"\"\n",
    "        self.resizable(False, False)\n",
    "\n",
    "    def create_widgets(self):\n",
    "        \"\"\"Create widgets (labels and buttons) inside the pop-up window.\"\"\"\n",
    "        self.label = tk.Label(self, text=self.message, wraplength=250)  # Adjust text if too long\n",
    "        self.label.pack(pady=10, padx=10)\n",
    "        self.btn_frame = tk.Frame(self)\n",
    "        self.btn_frame.pack(pady=5)\n",
    "        self.btn_yes = tk.Button(self.btn_frame, text=\"Yes\", command=self.yes)\n",
    "        self.btn_yes.pack(side=tk.LEFT, padx=5)\n",
    "        self.btn_no = tk.Button(self.btn_frame, text=\"No\", command=self.no)\n",
    "        self.btn_no.pack(side=tk.RIGHT, padx=5)\n",
    "\n",
    "        # Calculate window size based on text size\n",
    "        width = self.label.winfo_reqwidth() + 20\n",
    "        height = self.label.winfo_reqheight() + 100\n",
    "        self.geometry(f\"{width}x{height}\")\n",
    "\n",
    "    def yes(self):\n",
    "        \"\"\"Set result to True and close the window.\"\"\"\n",
    "        self.result = True\n",
    "        self.destroy()\n",
    "\n",
    "    def no(self):\n",
    "        \"\"\"Set result to False and close the window.\"\"\"\n",
    "        self.result = False\n",
    "        self.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = [\"ECONOMIC SECTORS\"]\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main Tkinter window\n",
    "\n",
    "    input_pdf_files = read_input_pdf_files()\n",
    "    processing_counter = 1\n",
    "\n",
    "    for folder in os.listdir(raw_pdf):\n",
    "        folder_path = os.path.join(raw_pdf, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(\"Processing folder:\", folder)\n",
    "            num_pdfs_trimmed = 0\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".pdf\"):\n",
    "                    pdf_file = os.path.join(folder_path, filename)\n",
    "                    if filename in input_pdf_files:\n",
    "                        print(f\"{processing_counter}. The PDF '{filename}' has already been trimmed and saved in '{input_pdf}'...\")\n",
    "                        processing_counter += 1\n",
    "                        continue\n",
    "                    print(f\"{processing_counter}. Processing:\", pdf_file)\n",
    "                    \n",
    "                    pages_with_keywords = search_keywords(pdf_file, keywords)\n",
    "                    num_pages_new_pdf = trim_pdf(pdf_file, pages_with_keywords)\n",
    "                    if num_pages_new_pdf > 0:\n",
    "                        num_pdfs_trimmed += 1\n",
    "                        input_pdf_files.add(filename)\n",
    "                        processing_counter += 1\n",
    "            \n",
    "            write_input_pdf_files(input_pdf_files)\n",
    "\n",
    "            message = f\"{num_pdfs_trimmed} PDFs have been trimmed in folder {folder}. Do you want to continue?\"\n",
    "            popup = PopupWindow(root, message)\n",
    "            root.wait_window(popup)\n",
    "            if not popup.result:\n",
    "                break\n",
    "                \n",
    "    print(\"Process completed for all PDFs in directory:\", input_pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643079a",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "Again, probably the NS (PDF files, now of few pages) were stored in disorder in the input_pdf folder. The following code sorts the PDFs into subfolders (years) by placing each NS (which now includes only the key tables) according to the year of its publication. This happens in the blink of an eye.  \n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the directory\n",
    "files = os.listdir(input_pdf)\n",
    "\n",
    "# Call the function to organize files\n",
    "organize_files_by_year(input_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045beb7b",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a384d2",
   "metadata": {},
   "source": [
    "<div id=\"3\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e1e92",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;; color: dark;\">3.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Data cleaning</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100857d4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "Since we already have the PDFs with just the tables required for this project, we can start extracting them. Then we can proceed with data cleaning.\n",
    "</p>  \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659419e",
   "metadata": {},
   "source": [
    "<div id=\"3-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156aa76",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    A brief documentation on issus in the table information of the PDFs. \n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438666b",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "Note that, the table information within the PDFs are available as editable text (including numeric values), but sometimes they can have various encoded formats that can make them difficult to extract and clean up. Undoubtedly, this is the most challenging stage of this jupyter notebook because there is no single pattern in which the information in the PDFs is arranged, each PDF adds a difficulty to extract the information. To understand more about this last point, we will start this section by documenting the most common problems we may face when trying to extract and clean tables from PDFs.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee78a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee6733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cda8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7a5a767",
   "metadata": {},
   "source": [
    "<div id=\"3-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357318e8",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.2.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Extracting tables and data cleanup\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13958cff",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "The main library used for extracting tables from PDFs is <code>pdfplumber</code>. You can review the official documentation by clicking <a href=\"https://github.com/jsvine/pdfplumber\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">here</a>.\n",
    "</p>\n",
    "    \n",
    "<p>     \n",
    "    The functions in <b>Section 3</b> of the <code>\"gdp_revisions_datasets_functions.py\"</code> script were built to deal with each of these issues. An interesting exercise is to compare the original tables (the ones in the PDF) and the cleaned tables (by the cleanup codes below). Thus, the cleanup codes for <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 1</a> and <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 2</a> generates two dictionaries, the first one stores the raw tables; that is, the original tables from the PDF extracted by the <code>pdfplumber</code> library, while the second dictionary stores the fully cleaned tables.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c91f7",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "The basic criterion to start extracting tables is to use keywords (sufficient condition). I mean, tables containing the following keywords meet the requirements to be extracted.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77729e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to search in the page text\n",
    "keywords = [\"ECONOMIC SECTORS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070fb47",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    The code iterates through each PDF and extracts the two required tables from each. The extracted information is then transformed into dataframes and the columns and values are cleaned up to conform to Python conventions (pythonic).\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf1618",
   "metadata": {},
   "source": [
    "<div id=\"3-2-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103d7bb",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.2.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    <span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">Tabla 1.</span> Extraction and cleaning of data from tables on monthly real GDP growth rates.\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f028da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carpeta input_pdf\\2013 ya ha sido procesada.\n",
      "La carpeta input_pdf\\2014 ya ha sido procesada.\n",
      "La carpeta input_pdf\\2015 ya ha sido procesada.\n",
      "La carpeta input_pdf\\2016 ya ha sido procesada.\n",
      "La carpeta input_pdf\\2017 ya ha sido procesada.\n",
      "La carpeta input_pdf\\2018 ya ha sido procesada.\n",
      "La carpeta input_pdf\\2019 ya ha sido procesada.\n",
      "La carpeta input_pdf\\2020 ya ha sido procesada.\n",
      "La carpeta input_pdf\\2021 ya ha sido procesada.\n",
      "La carpeta input_pdf\\2022 ya ha sido procesada.\n",
      "La carpeta input_pdf\\2023 ya ha sido procesada.\n",
      "Procesando la carpeta 2024\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'number_moving_average' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 246\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Create SQLAlchemy engine\u001b[39;00m\n\u001b[0;32m    244\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(connection_string)\n\u001b[1;32m--> 246\u001b[0m tables_dict_1 \u001b[38;5;241m=\u001b[39m \u001b[43mprocesar_carpetas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Capturar el valor devuelto de procesar_carpetas()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 204\u001b[0m, in \u001b[0;36mprocesar_carpetas\u001b[1;34m()\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLa carpeta \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcarpeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ya ha sido procesada.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m num_pdfs_procesados, num_dataframes_generados, tables_dict_temp \u001b[38;5;241m=\u001b[39m \u001b[43mprocesar_carpeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarpeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Actualizar tables_dict_1 con los valores devueltos de procesar_carpeta()\u001b[39;00m\n\u001b[0;32m    207\u001b[0m tables_dict_1\u001b[38;5;241m.\u001b[39mupdate(tables_dict_temp)\n",
      "Cell \u001b[1;32mIn[10], line 111\u001b[0m, in \u001b[0;36mprocesar_carpeta\u001b[1;34m(carpeta, engine)\u001b[0m\n\u001b[0;32m    109\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m replace_var_perc_first_column(df_clean)\n\u001b[0;32m    110\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m replace_var_perc_last_columns(df_clean)\n\u001b[1;32m--> 111\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m \u001b[43mreplace_number_moving_average\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_clean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m separate_text_digits(df_clean)\n\u001b[0;32m    113\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m exchange_values(df_clean)\n",
      "File \u001b[1;32m~\\OneDrive\\Documentos\\Research Assistant\\CIUP\\GDP Revisions\\GitHub\\peru_gdp_revisions\\gdp_revisions_datasets\\gdp_revisions_datasets_functions.py:617\u001b[0m, in \u001b[0;36mreplace_number_moving_average\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotnull(row\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])):\n\u001b[1;32m--> 617\u001b[0m         df\u001b[38;5;241m.\u001b[39mat[index, df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnumber_moving_average\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'number_moving_average' is not defined"
     ]
    }
   ],
   "source": [
    "# Establecer la localización en español\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Palabras clave para buscar en el texto de la página\n",
    "keywords = [\"ECONOMIC SECTORS\"]\n",
    "\n",
    "# Diccionario para almacenar los DataFrames generados\n",
    "dataframes_dict_1 = {}\n",
    "\n",
    "# Ruta del archivo de registro de carpetas procesadas\n",
    "registro_path = 'dataframes_record/carpetas_procesadas_1.txt'\n",
    "\n",
    "# Función para corregir los nombres de los meses\n",
    "def corregir_nombre_mes(mes):\n",
    "    meses_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Agrega más mapeos si es necesario para otros nombres de meses\n",
    "    }\n",
    "    return meses_mapping.get(mes, mes)\n",
    "\n",
    "def registrar_carpeta_procesada(carpeta, num_archivos_procesados):\n",
    "    with open(registro_path, 'a') as file:\n",
    "        file.write(f\"{carpeta}:{num_archivos_procesados}\\n\")\n",
    "\n",
    "def carpeta_procesada(carpeta):\n",
    "    if not os.path.exists(registro_path):\n",
    "        return False\n",
    "    with open(registro_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(carpeta):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def obtener_fecha(df, engine):\n",
    "    id_ns = df['id_ns'].iloc[0]\n",
    "    year = df['year'].iloc[0]\n",
    "    query = f\"SELECT date FROM dates_growth_rates WHERE id_ns = '{id_ns}' AND year = '{year}';\"\n",
    "    fecha = pd.read_sql(query, engine)\n",
    "    return fecha.iloc[0, 0] if not fecha.empty else None\n",
    "\n",
    "def procesar_pdf(pdf_path):\n",
    "    tables_dict_1 = {}  # Diccionario local para cada PDF\n",
    "    table_counter = 1\n",
    "    keyword_count = 0 \n",
    "\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No se encontraron coincidencias para id_ns y year en el nombre del archivo:\", filename)\n",
    "        return None, None, None, None, None  # Return None for tables_dict_1 as well\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(pdf_path))[0].replace('-', '_')\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            text = page.extract_text()\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                keyword_count += 1\n",
    "                if keyword_count == 1:  # Solo procesar la primera ocurrencia\n",
    "                    tables = tabula.read_pdf(pdf_path, pages=i, multiple_tables=False, stream=True) # change stream to another option if desired\n",
    "                    for j, table_df in enumerate(tables, start=1):\n",
    "                        nombre_dataframe = f\"{new_filename}_{keyword_count}\"\n",
    "                        tables_dict_1[nombre_dataframe] = table_df\n",
    "                        table_counter += 1\n",
    "\n",
    "                    break  # Salir del bucle después de encontrar la primera ocurrencia\n",
    "\n",
    "    return id_ns, year, tables_dict_1, keyword_count  # No retornar date aquí\n",
    "\n",
    "def procesar_carpeta(carpeta, engine):\n",
    "    print(f\"Procesando la carpeta {os.path.basename(carpeta)}\")\n",
    "    pdf_files = [os.path.join(carpeta, f) for f in os.listdir(carpeta) if f.endswith('.pdf')]\n",
    "\n",
    "    num_pdfs_procesados = 0\n",
    "    num_dataframes_generados = 0\n",
    "\n",
    "    table_counter = 1  # Inicializar el contador de tabla aquí\n",
    "    tables_dict_1 = {}  # Declarar tables_dict_1 fuera del bucle principal\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        id_ns, year, tables_dict_temp, keyword_count = procesar_pdf(pdf_file)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for nombre_df, df in tables_dict_temp.items():\n",
    "                nombre_archivo = os.path.splitext(os.path.basename(pdf_file))[0].replace('-', '_')\n",
    "                nombre_df = f\"{nombre_archivo}_{keyword_count}\"\n",
    "                \n",
    "                # Almacenar DataFrame sin procesar en tables_dict_1\n",
    "                tables_dict_1[nombre_df] = df.copy()\n",
    "                \n",
    "                # Aplicar las 20 líneas de funciones de limpieza a una copia del DataFrame\n",
    "                df_clean = df.copy()\n",
    "\n",
    "                if any(col.isdigit() and len(col) == 4 for col in df_clean.columns):\n",
    "                    # Si hay al menos una columna que representa un año\n",
    "                    df_clean = swap_nan_se(df_clean)\n",
    "                    df_clean = split_column_by_pattern(df_clean)\n",
    "                    df_clean = drop_rare_caracter_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = relocate_last_columns(df_clean)\n",
    "                    df_clean = replace_first_dot(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = replace_var_perc_first_column(df_clean)\n",
    "                    df_clean = replace_var_perc_last_columns(df_clean)\n",
    "                    df_clean = replace_number_moving_average(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = exchange_values(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = find_year_column(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = convert_float(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = rounding_values(df_clean, decimales=1)\n",
    "                else: # 2014 ns 08\n",
    "                    # Si no hay columnas que representen años\n",
    "                    df_clean = check_first_row(df_clean)\n",
    "                    df_clean = check_first_row_1(df_clean)\n",
    "                    df_clean = replace_first_row_with_columns(df_clean)\n",
    "                    df_clean = swap_nan_se(df_clean)\n",
    "                    df_clean = split_column_by_pattern(df_clean)\n",
    "                    df_clean = drop_rare_caracter_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = relocate_last_columns(df_clean)\n",
    "                    #df_clean = replace_first_dot(df_clean) # comment for 2014 ns 08\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = replace_var_perc_first_column(df_clean)\n",
    "                    df_clean = replace_var_perc_last_columns(df_clean)\n",
    "                    df_clean = replace_number_moving_average(df_clean)\n",
    "                    df_clean = expand_column(df_clean) # 2014 ns 08\n",
    "                    df_clean = split_values_1(df_clean) # 2014 ns 08\n",
    "                    df_clean = split_values_2(df_clean) # 2016 ns 15\n",
    "                    df_clean = split_values_3(df_clean) # 2016 ns 19\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = exchange_values(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = find_year_column(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = convert_float(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column_1(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column_2(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column_3(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = rounding_values(df_clean, decimales=1)\n",
    "                \n",
    "                # Añadir la columna 'year' al DataFrame limpio\n",
    "                df_clean.insert(0, 'year', year)\n",
    "                \n",
    "                # Añadir la columna 'id_ns' al DataFrame limpio\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                \n",
    "                # Obtener la fecha correspondiente de la base de datos\n",
    "                fecha = obtener_fecha(df_clean, engine)\n",
    "                if fecha:\n",
    "                    # Añadir la columna 'date' al DataFrame limpio\n",
    "                    df_clean.insert(2, 'date', fecha)\n",
    "                else:\n",
    "                    print(\"No se encontró fecha en la base de datos para id_ns:\", id_ns, \"y year:\", year)\n",
    "                \n",
    "                # Almacenar DataFrame limpio en dataframes_dict_1\n",
    "                dataframes_dict_1[nombre_df] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. El dataframe generado para el archivo {pdf_file} es: {nombre_df}')\n",
    "                num_dataframes_generados += 1\n",
    "                table_counter += 1  # Incrementar el contador de tabla aquí\n",
    "        \n",
    "        num_pdfs_procesados += 1  # Incrementar el número de PDFs procesados por cada PDF en la carpeta\n",
    "\n",
    "    return num_pdfs_procesados, num_dataframes_generados, tables_dict_1\n",
    "\n",
    "\n",
    "def procesar_carpetas():\n",
    "    pdf_folder = 'input_pdf'\n",
    "    carpetas = [os.path.join(pdf_folder, d) for d in os.listdir(pdf_folder) if os.path.isdir(os.path.join(pdf_folder, d))]\n",
    "    \n",
    "    tables_dict_1 = {}  # Inicializar tables_dict_1 aquí\n",
    "    \n",
    "    for carpeta in carpetas:\n",
    "        if carpeta_procesada(carpeta):\n",
    "            print(f\"La carpeta {carpeta} ya ha sido procesada.\")\n",
    "            continue\n",
    "        \n",
    "        num_pdfs_procesados, num_dataframes_generados, tables_dict_temp = procesar_carpeta(carpeta, engine)\n",
    "        \n",
    "        # Actualizar tables_dict_1 con los valores devueltos de procesar_carpeta()\n",
    "        tables_dict_1.update(tables_dict_temp)\n",
    "        \n",
    "        registrar_carpeta_procesada(carpeta, num_pdfs_procesados)\n",
    "\n",
    "        # Preguntar al usuario si desea continuar con la siguiente carpeta\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Para asegurar que la ventana esté en primer plano\n",
    "        \n",
    "        mensaje = f\"Se han generado {num_dataframes_generados} dataframes en la carpeta {carpeta}. ¿Deseas continuar con la siguiente carpeta?\"\n",
    "        continuar = messagebox.askyesno(\"Continuar\", mensaje)\n",
    "        root.destroy()\n",
    "\n",
    "        if not continuar:\n",
    "            print(\"Procesamiento detenido por el usuario.\")\n",
    "            break  # Romper el bucle for si el usuario decide no continuar\n",
    "\n",
    "    print(\"Procesamiento completado para todas las carpetas.\")  # Add a message to indicate completion\n",
    "\n",
    "    return tables_dict_1  # Devolver tables_dict_1 al final de la función\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get environment variables\n",
    "    user = os.environ.get('CIUP_SQL_USER')\n",
    "    password = os.environ.get('CIUP_SQL_PASS')\n",
    "    host = os.environ.get('CIUP_SQL_HOST')\n",
    "    port = 5432\n",
    "    database = 'gdp_revisions_datasets'\n",
    "\n",
    "    # Check if all environment variables are defined\n",
    "    if not all([host, user, password]):\n",
    "        raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "    # Create connection string\n",
    "    connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    tables_dict_1 = procesar_carpetas()  # Capturar el valor devuelto de procesar_carpetas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82953423",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc21c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff21f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6cc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_1['ns_01_2024_1'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ecc28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_1 = dataframes_dict_1['ns_01_2024_1']\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee02360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[(df_1['sectores_economicos'] == 'agropecuario') | (df_1['economic_sectors'] == 'agriculture and livestock')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105bd81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edee4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5ed43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc3862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5a4a14f",
   "metadata": {},
   "source": [
    "<div id=\"3-2-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea2543",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.2.2.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    <span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">Tabla 2.</span> Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer la localización en español\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Palabras clave para buscar en el texto de la página\n",
    "keywords = [\"ECONOMIC SECTORS\"]\n",
    "\n",
    "# Diccionario para almacenar los DataFrames generados\n",
    "dataframes_dict_2 = {}\n",
    "\n",
    "# Ruta del archivo de registro de carpetas procesadas\n",
    "registro_path = 'dataframes_record/carpetas_procesadas_2.txt'\n",
    "\n",
    "# Función para corregir los nombres de los meses\n",
    "def corregir_nombre_mes(mes):\n",
    "    meses_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Agrega más mapeos si es necesario para otros nombres de meses\n",
    "    }\n",
    "    return meses_mapping.get(mes, mes)\n",
    "\n",
    "def registrar_carpeta_procesada(carpeta, num_archivos_procesados):\n",
    "    with open(registro_path, 'a') as file:\n",
    "        file.write(f\"{carpeta}:{num_archivos_procesados}\\n\")\n",
    "\n",
    "def carpeta_procesada(carpeta):\n",
    "    if not os.path.exists(registro_path):\n",
    "        return False\n",
    "    with open(registro_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(carpeta):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def obtener_fecha(df, engine):\n",
    "    id_ns = df['id_ns'].iloc[0]\n",
    "    year = df['year'].iloc[0]\n",
    "    query = f\"SELECT date FROM dates_growth_rates WHERE id_ns = '{id_ns}' AND year = '{year}';\"\n",
    "    fecha = pd.read_sql(query, engine)\n",
    "    return fecha.iloc[0, 0] if not fecha.empty else None\n",
    "\n",
    "def procesar_pdf(pdf_path):\n",
    "    tables_dict_2 = {}  # Diccionario local para cada PDF\n",
    "    table_counter = 1\n",
    "    keyword_count = 0 \n",
    "\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No se encontraron coincidencias para id_ns y year en el nombre del archivo:\", filename)\n",
    "        return None, None, None, None\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(pdf_path))[0].replace('-', '_')\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            text = page.extract_text()\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                keyword_count += 1\n",
    "                if keyword_count == 2:\n",
    "                    tables = tabula.read_pdf(pdf_path, pages=i, multiple_tables=False)\n",
    "                    for j, table_df in enumerate(tables, start=1):\n",
    "                        nombre_dataframe = f\"{new_filename}_{keyword_count}\"\n",
    "                        tables_dict_2[nombre_dataframe] = table_df\n",
    "                        table_counter += 1\n",
    "\n",
    "    return id_ns, year, tables_dict_2, keyword_count\n",
    "\n",
    "\n",
    "def procesar_carpeta(carpeta, engine):\n",
    "    print(f\"Procesando la carpeta {os.path.basename(carpeta)}\")\n",
    "    pdf_files = [os.path.join(carpeta, f) for f in os.listdir(carpeta) if f.endswith('.pdf')]\n",
    "\n",
    "    num_pdfs_procesados = 0\n",
    "    num_dataframes_generados = 0\n",
    "\n",
    "    table_counter = 1  # Inicializar el contador de tabla aquí\n",
    "    tables_dict_2 = {}  # Declarar tables_dict fuera del bucle principal\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        id_ns, year, tables_dict_temp, keyword_count = procesar_pdf(pdf_file)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for nombre_df, df in tables_dict_temp.items():\n",
    "                nombre_archivo = os.path.splitext(os.path.basename(pdf_file))[0].replace('-', '_')\n",
    "                nombre_df = f\"{nombre_archivo}_{keyword_count}\"\n",
    "\n",
    "                # Almacenar DataFrame sin procesar en tables_dict\n",
    "                tables_dict_2[nombre_df] = df.copy()\n",
    "\n",
    "                # Aplicar las 20 líneas de funciones de limpieza a una copia del DataFrame\n",
    "                df_clean = df.copy()\n",
    "                if df_clean.iloc[0, 0] is np.nan:\n",
    "                    # Aplicar las 20 líneas de limpieza\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = separate_years(df_clean)\n",
    "                    df_clean = relocate_roman_numerals(df_clean)\n",
    "                    df_clean = extract_mixed_values(df_clean)\n",
    "                    df_clean = replace_first_row_nan(df_clean)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = drop_nan_row(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = split_values(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = roman_arabic(df_clean)\n",
    "                    df_clean = fix_duplicates(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = convert_float(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = rounding_values(df_clean, decimales=1)\n",
    "                else:\n",
    "                    # Aplicar las 15 líneas de limpieza\n",
    "                    df_clean = exchange_roman_nan(df_clean)\n",
    "                    df_clean = exchange_columns(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = last_column_es(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = roman_arabic(df_clean)\n",
    "                    df_clean = fix_duplicates(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = convert_float(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = rounding_values(df_clean, decimales=1)\n",
    "\n",
    "                # Añadir la columna 'year' al DataFrame limpio\n",
    "                df_clean.insert(0, 'year', year)\n",
    "                \n",
    "                # Añadir la columna 'id_ns' al DataFrame limpio\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                \n",
    "                # Obtener la fecha correspondiente de la base de datos\n",
    "                fecha = obtener_fecha(df_clean, engine)\n",
    "                if fecha:\n",
    "                    # Añadir la columna 'date' al DataFrame limpio\n",
    "                    df_clean.insert(2, 'date', fecha)\n",
    "                else:\n",
    "                    print(\"No se encontró fecha en la base de datos para id_ns:\", id_ns, \"y year:\", year)\n",
    "\n",
    "                # Almacenar DataFrame limpio en dataframes_dict\n",
    "                dataframes_dict_2[nombre_df] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. El dataframe generado para el archivo {pdf_file} es: {nombre_df}')\n",
    "                num_dataframes_generados += 1\n",
    "                table_counter += 1  # Incrementar el contador de tabla aquí\n",
    "                    \n",
    "        num_pdfs_procesados += 1  # Incrementar el número de PDFs procesados por cada PDF en la carpeta\n",
    "\n",
    "    return num_pdfs_procesados, num_dataframes_generados, tables_dict_2\n",
    "        \n",
    "def procesar_carpetas():\n",
    "    pdf_folder = 'input_pdf'\n",
    "    carpetas = [os.path.join(pdf_folder, d) for d in os.listdir(pdf_folder) if os.path.isdir(os.path.join(pdf_folder, d))]\n",
    "\n",
    "    tables_dict_2 = {}  # Inicializar tables_dict aquí\n",
    "    \n",
    "    for carpeta in carpetas:\n",
    "        if carpeta_procesada(carpeta):\n",
    "            print(f\"La carpeta {carpeta} ya ha sido procesada.\")\n",
    "            continue\n",
    "        \n",
    "        num_pdfs_procesados, num_dataframes_generados, tables_dict_temp = procesar_carpeta(carpeta, engine)\n",
    "        \n",
    "        # Actualizar tables_dict con los valores devueltos de procesar_carpeta()\n",
    "        tables_dict_2.update(tables_dict_temp)\n",
    "        \n",
    "        registrar_carpeta_procesada(carpeta, num_pdfs_procesados)\n",
    "\n",
    "        # Preguntar al usuario si desea continuar con la siguiente carpeta\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Para asegurar que la ventana esté en primer plano\n",
    "        \n",
    "        mensaje = f\"Se han generado {num_dataframes_generados} dataframes en la carpeta {carpeta}. ¿Deseas continuar con la siguiente carpeta?\"\n",
    "        continuar = messagebox.askyesno(\"Continuar\", mensaje)\n",
    "        root.destroy()\n",
    "\n",
    "        if not continuar:\n",
    "            print(\"Procesamiento detenido por el usuario.\")\n",
    "            break  # Romper el bucle for si el usuario decide no continuar\n",
    "\n",
    "    print(\"Procesamiento completado para todas las carpetas.\")  # Add a message to indicate completion\n",
    "\n",
    "    return tables_dict_2  # Devolver tables_dict al final de la función\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Get environment variables\n",
    "    user = os.environ.get('CIUP_SQL_USER')\n",
    "    password = os.environ.get('CIUP_SQL_PASS')\n",
    "    host = os.environ.get('CIUP_SQL_HOST')\n",
    "    port = 5432\n",
    "    database = 'gdp_revisions_datasets'\n",
    "\n",
    "    # Check if all environment variables are defined\n",
    "    if not all([host, user, password]):\n",
    "        raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "    # Create connection string\n",
    "    connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "    tables_dict_2 = procesar_carpetas() # Capturar el valor devuelto de procesar_carpetas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a9f51",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a46bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_2['ns_01_2024_2'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict_2['ns_01_2024_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c8410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e0f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709d3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "667d97b8",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217183e7",
   "metadata": {},
   "source": [
    "<div id=\"4\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8989d65",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">SQL Tables</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a04abd",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Finally, after obtaining and cleaning all the necessary data, we can create the three most important datasets to store realeses, vintages, and revisions. These datasets will be stored as tables in SQL and can be loaded into any software or programming language.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0b233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b689b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a26633b0",
   "metadata": {},
   "source": [
    "<div id=\"sector\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0dda6f",
   "metadata": {},
   "source": [
    "# Chose sector_economico and economic_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "# Definir la lista de opciones\n",
    "opciones = [\n",
    "    \"pbi\",\n",
    "    \"agropecuario\",\n",
    "    \"pesca\",\n",
    "    \"mineria e hidrocarburos\",\n",
    "    \"manufactura\",\n",
    "    \"electricidad y agua\",\n",
    "    \"construccion\",\n",
    "    \"comercio\",\n",
    "    \"otros servicios\"\n",
    "]\n",
    "\n",
    "# Función para guardar la opción seleccionada y cerrar la ventana\n",
    "def guardar_opcion():\n",
    "    global sector_economico\n",
    "    sector_economico = opcion_seleccionada.get()\n",
    "    root.destroy()  # Cerrar la ventana después de seleccionar una opción\n",
    "\n",
    "# Crear la ventana emergente\n",
    "root = tk.Tk()\n",
    "root.title(\"Seleccionar opción\")\n",
    "\n",
    "# Variable para almacenar la opción seleccionada\n",
    "opcion_seleccionada = tk.StringVar(root)\n",
    "opcion_seleccionada.set(opciones[0])  # Opción predeterminada\n",
    "\n",
    "# Crear el menú de opciones\n",
    "menu = tk.OptionMenu(root, opcion_seleccionada, *opciones)\n",
    "menu.pack(pady=10)\n",
    "\n",
    "# Botón para confirmar la selección\n",
    "boton_confirmar = tk.Button(root, text=\"Confirmar\", command=guardar_opcion)\n",
    "boton_confirmar.pack()\n",
    "\n",
    "# Mostrar la ventana\n",
    "root.update_idletasks()\n",
    "root.wait_window()\n",
    "\n",
    "# Mostrar el valor seleccionado\n",
    "print(\"Sector económico seleccionado:\", opcion_seleccionada.get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "# Definir la lista de opciones\n",
    "opciones = [\n",
    "    \"gdp\",\n",
    "    \"agriculture and livestock\",\n",
    "    \"fishing\",\n",
    "    \"mining and fuel\",\n",
    "    \"manufacturing\",\n",
    "    \"electricity and water\",\n",
    "    \"construction\",\n",
    "    \"commerce\",\n",
    "    \"other services\"\n",
    "]\n",
    "\n",
    "# Función para guardar la opción seleccionada y cerrar la ventana\n",
    "def guardar_opcion():\n",
    "    global economic_sector\n",
    "    economic_sector = opcion_seleccionada.get()\n",
    "    root.destroy()  # Cerrar la ventana después de seleccionar una opción\n",
    "\n",
    "# Crear la ventana emergente\n",
    "root = tk.Tk()\n",
    "root.title(\"Seleccionar opción\")\n",
    "\n",
    "# Variable para almacenar la opción seleccionada\n",
    "opcion_seleccionada = tk.StringVar(root)\n",
    "opcion_seleccionada.set(opciones[0])  # Opción predeterminada\n",
    "\n",
    "# Crear el menú de opciones\n",
    "menu = tk.OptionMenu(root, opcion_seleccionada, *opciones)\n",
    "menu.pack(pady=10)\n",
    "\n",
    "# Botón para confirmar la selección\n",
    "boton_confirmar = tk.Button(root, text=\"Confirmar\", command=guardar_opcion)\n",
    "boton_confirmar.pack()\n",
    "\n",
    "# Mostrar la ventana\n",
    "root.update_idletasks()\n",
    "root.wait_window()\n",
    "\n",
    "# Mostrar el valor seleccionado\n",
    "print(\"Sector económico seleccionado:\", opcion_seleccionada.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb61ed",
   "metadata": {},
   "source": [
    "# Chose the year and label datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f31ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "# Crear una ventana principal\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Ocultar la ventana principal\n",
    "\n",
    "# Pedir al usuario que introduzca el valor de sector_economico\n",
    "sector = simpledialog.askstring(\"Sector Económico\", \"Introduce el valor del sector:\")\n",
    "\n",
    "# Pedir al usuario que introduzca el valor de economic_sector\n",
    "#year = simpledialog.askstring(\"Year\", \"Introduce el valor de year:\")\n",
    "\n",
    "# Mostrar los valores introducidos por el usuario\n",
    "print(\"Valor del sector:\", sector)\n",
    "#print(\"Valor de economic_sector:\", year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6952a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84e6ca98",
   "metadata": {},
   "source": [
    "<div id=\"4-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b142e",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Annual Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_annual_df(dataframes_dict):\n",
    "    # List to store the names of dataframes that meet the criterion of ending in '_2'\n",
    "    dataframes_ending_with_2 = []\n",
    "\n",
    "    # List to store the names of dataframes to be concatenated\n",
    "    dataframes_to_concatenate = []\n",
    "\n",
    "    # Iterate over the dataframe names in the all_dataframes dictionary\n",
    "    for df_name in dataframes_dict.keys():\n",
    "        # Check if the dataframe name ends with '_2' and add it to the corresponding list\n",
    "        if df_name.endswith('_2'):\n",
    "            dataframes_ending_with_2.append(df_name)\n",
    "            dataframes_to_concatenate.append(dataframes_dict[df_name])\n",
    "\n",
    "    # Print the names of dataframes that meet the criterion of ending in '_2'\n",
    "    print(\"DataFrames ending with '_2' that will be concatenated:\")\n",
    "    for df_name in dataframes_ending_with_2:\n",
    "        print(df_name)\n",
    "\n",
    "    # Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "    if dataframes_to_concatenate:\n",
    "        # Concatenate only rows that meet the specified conditions\n",
    "        annual_growth_rates = pd.concat([df[(df['sectores_economicos'] == sector_economico) | (df['economic_sectors'] == economic_sector)] \n",
    "                                    for df in dataframes_to_concatenate \n",
    "                                    if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Keep only columns that start with 'year' and the 'id_ns', 'year', and 'date' columns\n",
    "        columns_to_keep = ['year', 'id_ns', 'date'] + [col for col in annual_growth_rates.columns if col.endswith('_year')]\n",
    "\n",
    "        # Drop unwanted columns\n",
    "        annual_growth_rates = annual_growth_rates[columns_to_keep]\n",
    "        \n",
    "        # Remove duplicate columns if any\n",
    "        annual_growth_rates = annual_growth_rates.loc[:,~annual_growth_rates.columns.duplicated()]\n",
    "    \n",
    "        # Cambia el nombre de las columnas a partir de la cuarta columna\n",
    "        annual_growth_rates.columns = [col.split('_')[1] + '_' + col.split('_')[0] if '_' in col and idx >= 3 else col for idx, col in enumerate(annual_growth_rates.columns)]\n",
    "\n",
    "        # Print the number of rows in the concatenated dataframe\n",
    "        print(\"Number of rows in the concatenated dataframe:\", len(annual_growth_rates))\n",
    "        \n",
    "        return annual_growth_rates\n",
    "    else:\n",
    "        print(\"No dataframes were found to concatenate.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()[f\"{sector}_annual_growth_rates\"] = concatenate_annual_df(dataframes_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "globals()[f\"{sector}_annual_growth_rates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f8d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61bbaac2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d9bac",
   "metadata": {},
   "source": [
    "<div id=\"4-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50087e95",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.2.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Quarterly Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6467ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concatenate_quarterly_df(dataframes_dict):\n",
    "    # List to store the names of dataframes that meet the criterion of ending in '_2'\n",
    "    dataframes_ending_with_2 = []\n",
    "\n",
    "    # List to store the names of dataframes to be concatenated\n",
    "    dataframes_to_concatenate = []\n",
    "\n",
    "    # Iterate over the dataframe names in the all_dataframes dictionary\n",
    "    for df_name in dataframes_dict.keys():\n",
    "        # Check if the dataframe name ends with '_2' and add it to the corresponding list\n",
    "        if df_name.endswith('_2'):\n",
    "            dataframes_ending_with_2.append(df_name)\n",
    "            dataframes_to_concatenate.append(dataframes_dict[df_name])\n",
    "\n",
    "    # Print the names of dataframes that meet the criterion of ending in '_2'\n",
    "    print(\"DataFrames ending with '_2' that will be concatenated:\")\n",
    "    for df_name in dataframes_ending_with_2:\n",
    "        print(df_name)\n",
    "\n",
    "    # Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "    if dataframes_to_concatenate:\n",
    "        # Concatenate only rows that meet the specified conditions\n",
    "        quarterly_growth_rates = pd.concat([df[(df['sectores_economicos'] == sector_economico) | (df['economic_sectors'] == economic_sector)] \n",
    "                                    for df in dataframes_to_concatenate \n",
    "                                    if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Keep all columns except those starting with 'year_', in addition to the 'id_ns', 'year', and 'date' columns\n",
    "        columns_to_keep = ['year', 'id_ns', 'date'] + [col for col in quarterly_growth_rates.columns if not col.endswith('_year')]\n",
    "\n",
    "        # Select unwanted columns\n",
    "        quarterly_growth_rates = quarterly_growth_rates[columns_to_keep]\n",
    "\n",
    "        # Drop the 'sectores_economicos' and 'economic_sectors' columns\n",
    "        quarterly_growth_rates.drop(columns=['sectores_economicos', 'economic_sectors'], inplace=True)\n",
    "\n",
    "        # Remove duplicate columns if any\n",
    "        quarterly_growth_rates = quarterly_growth_rates.loc[:,~quarterly_growth_rates.columns.duplicated()]\n",
    "\n",
    "        # Cambia el nombre de las columnas a partir de la cuarta columna\n",
    "        #quarterly_growth_rates.columns = [col.split('_')[0] + '_q' + col.split('_')[1] if '_' in col and idx >= 3 else col\n",
    "        #for idx, col in enumerate(quarterly_growth_rates.columns)]\n",
    "        \n",
    "        # Print the number of rows in the concatenated dataframe\n",
    "        print(\"Number of rows in the concatenated dataframe:\", len(quarterly_growth_rates))\n",
    "        \n",
    "        return quarterly_growth_rates\n",
    "    else:\n",
    "        print(\"No dataframes were found to concatenate.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b797aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()[f\"{sector}_quarterly_growth_rates\"] = concatenate_quarterly_df(dataframes_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "globals()[f\"{sector}_quarterly_growth_rates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d99fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24dd0fc0",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca271f",
   "metadata": {},
   "source": [
    "<div id=\"4-3\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453bb965",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.3.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Monthly Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a86c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concatenate_monthly_df(dataframes_dict):\n",
    "    # List to store the names of dataframes that meet the criterion of ending in '_1'\n",
    "    dataframes_ending_with_1 = []\n",
    "\n",
    "    # List to store the names of dataframes to be concatenated\n",
    "    dataframes_to_concatenate = []\n",
    "\n",
    "    # Iterate over the dataframe names in the all_dataframes dictionary\n",
    "    for df_name in dataframes_dict.keys():\n",
    "        # Check if the dataframe name ends with '_1' and add it to the corresponding list\n",
    "        if df_name.endswith('_1'):\n",
    "            dataframes_ending_with_1.append(df_name)\n",
    "            dataframes_to_concatenate.append(dataframes_dict[df_name])\n",
    "\n",
    "    # Print the names of dataframes that meet the criterion of ending with '_1'\n",
    "    print(\"DataFrames ending with '_1' that will be concatenated:\")\n",
    "    for df_name in dataframes_ending_with_1:\n",
    "        print(df_name)\n",
    "\n",
    "    # Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "    if dataframes_to_concatenate:\n",
    "        # Concatenate only rows that meet the specified conditions\n",
    "        monthly_growth_rates = pd.concat([df[(df['sectores_economicos'] == sector_economico) | (df['economic_sectors'] == economic_sector)] \n",
    "                                    for df in dataframes_to_concatenate \n",
    "                                    if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Keep all columns except those starting with 'year_', in addition to the 'id_ns', 'year', and 'date' columns\n",
    "        columns_to_keep = ['year', 'id_ns', 'date'] + [col for col in monthly_growth_rates.columns if not col.endswith('_year')]\n",
    "\n",
    "        # Select unwanted columns\n",
    "        monthly_growth_rates = monthly_growth_rates[columns_to_keep]\n",
    "\n",
    "        # Drop the 'sectores_economicos' and 'economic_sectors' columns\n",
    "        monthly_growth_rates.drop(columns=['sectores_economicos', 'economic_sectors'], inplace=True)\n",
    "\n",
    "        # Remove duplicate columns if any\n",
    "        monthly_growth_rates = monthly_growth_rates.loc[:,~monthly_growth_rates.columns.duplicated()]\n",
    "        \n",
    "        # Drop columns with at least two underscores in their names\n",
    "        columns_to_drop = [col for col in monthly_growth_rates.columns if col.count('_') >= 2]\n",
    "        monthly_growth_rates.drop(columns=columns_to_drop, inplace=True)\n",
    "        \n",
    "        # Cambia el nombre de las columnas a partir de la cuarta columna\n",
    "        monthly_growth_rates.columns = [col.split('_')[1] + '_' + col.split('_')[0] if '_' in col and idx >= 3 else col for idx, col in enumerate(monthly_growth_rates.columns)]\n",
    "        \n",
    "        # Diccionario de mapeo de nombres de meses\n",
    "        #meses = {\n",
    "        #    'ene': 'm1', 'feb': 'm2', 'mar': 'm3', 'abr': 'm4',\n",
    "        #    'may': 'm5', 'jun': 'm6', 'jul': 'm7', 'ago': 'm8',\n",
    "        #    'sep': 'm9', 'oct': 'm10', 'nov': 'm11', 'dic': 'm12'\n",
    "        #}\n",
    "        \n",
    "        # Función para reemplazar las claves por los valores del diccionario en el nombre de las columnas\n",
    "        #def replace_months(column_name, meses):\n",
    "        #    for key, value in meses.items():\n",
    "        #        if key in column_name:\n",
    "        #            return column_name.replace(key, value)\n",
    "        #    return column_name\n",
    "\n",
    "        # Aplicar la función a todas las columnas del DataFrame\n",
    "        #monthly_growth_rates.columns = monthly_growth_rates.columns.map(lambda x: replace_months(x, meses))\n",
    "\n",
    "        # Print the number of rows in the concatenated dataframe\n",
    "        print(\"Number of rows in the concatenated dataframe:\", len(monthly_growth_rates))\n",
    "\n",
    "        return monthly_growth_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671223c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "globals()[f\"{sector}_monthly_growth_rates\"] = concatenate_monthly_df(dataframes_dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "globals()[f\"{sector}_monthly_growth_rates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01064582",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #1E3B58; color: white; padding: 10px;\">\n",
    "    <h2>Create revision dataset (annual)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e71f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calcular la diferencia entre el último y el primer valor no NaN para cada columna, excepto 'year', 'ns_id' y 'date'\n",
    "revision = globals()[f\"{sector}_annual_growth_rates\"].drop(columns=['year', 'id_ns', 'date']).apply(lambda x: x.loc[x.last_valid_index()] - x.loc[x.first_valid_index()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4ec44",
   "metadata": {},
   "source": [
    "#  Nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Crear un nuevo DataFrame con los resultados\n",
    "globals()[f\"{sector}_annual_revisions\"] = pd.DataFrame({'revision_date': revision.index, f'{sector}_revision': revision.values})\n",
    "# Suponiendo que 'gdp_monthly_growth_rates' es tu DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "globals()[f\"{sector}_annual_revisions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b7e2d",
   "metadata": {},
   "source": [
    "# Para graficar en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extraer el año de la cadena y convertirlo a un tipo entero\n",
    "globals()[f\"{sector}_annual_revisions\"]['year'] = globals()[f\"{sector}_annual_revisions\"]['revision_date'].str.extract(r'(\\d+)')\n",
    "globals()[f\"{sector}_annual_revisions\"]['year'] = globals()[f\"{sector}_annual_revisions\"]['year'].astype(int)\n",
    "\n",
    "# Crear una nueva columna de tipo fecha\n",
    "globals()[f\"{sector}_annual_revisions\"]['revision_date'] = pd.to_datetime(globals()[f\"{sector}_annual_revisions\"]['year'], format='%Y')\n",
    "\n",
    "# Eliminar la columna 'year' si ya no es necesaria\n",
    "globals()[f\"{sector}_annual_revisions\"].drop(columns=['year'], inplace=True)\n",
    "\n",
    "# Mostrar el resultado\n",
    "globals()[f\"{sector}_annual_revisions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ccfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c21868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed00dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f59fc6d7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #1E3B58; color: white; padding: 10px;\">\n",
    "    <h2>Create revision dataset (quarterly)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac55f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calcular la diferencia entre el último y el primer valor no NaN para cada columna, excepto 'year', 'ns_id' y 'date'\n",
    "revision = globals()[f\"{sector}_quarterly_growth_rates\"].drop(columns=['year', 'id_ns', 'date']).apply(lambda x: x.loc[x.last_valid_index()] - x.loc[x.first_valid_index()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc5ee4",
   "metadata": {},
   "source": [
    "#  Nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782ba95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Crear un nuevo DataFrame con los resultados\n",
    "globals()[f\"{sector}_quarterly_revisions\"] = pd.DataFrame({'revision_date': revision.index, f'{sector}_revision': revision.values})\n",
    "# Suponiendo que 'gdp_monthly_growth_rates' es tu DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "globals()[f\"{sector}_quarterly_revisions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009913b",
   "metadata": {},
   "source": [
    "# Para graficar en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99e594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convertir la columna 'revision_date' a tipo de datos de fecha\n",
    "globals()[f\"{sector}_quarterly_revisions\"]['revision_date'] = pd.to_datetime(globals()[f\"{sector}_quarterly_revisions\"]['revision_date'], format='%Y_%m')\n",
    "\n",
    "# Mostrar el resultado\n",
    "globals()[f\"{sector}_quarterly_revisions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484e7df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #1E3B58; color: white; padding: 10px;\">\n",
    "    <h2>Create revision dataset (monthly)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a1a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302703a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Calcular la diferencia entre el último y el primer valor no NaN para cada columna, excepto 'year', 'ns_id' y 'date'\n",
    "revision = globals()[f\"{sector}_monthly_growth_rates\"].drop(columns=['year', 'id_ns', 'date']).apply(lambda x: x.loc[x.last_valid_index()] - x.loc[x.first_valid_index()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e6a3d6",
   "metadata": {},
   "source": [
    "#  Nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd75ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Crear un nuevo DataFrame con los resultados\n",
    "globals()[f\"{sector}_monthly_revisions\"] = pd.DataFrame({'revision_date': revision.index, f'{sector}_revision': revision.values})\n",
    "# Suponiendo que 'gdp_monthly_growth_rates' es tu DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "globals()[f\"{sector}_monthly_revisions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6897cc9",
   "metadata": {},
   "source": [
    "# Para graficar en python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea547e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extraer el mes y el año de la columna 'revision_date'\n",
    "globals()[f\"{sector}_monthly_revisions\"]['month'] = globals()[f\"{sector}_monthly_revisions\"]['revision_date'].str.split('_').str[0]\n",
    "globals()[f\"{sector}_monthly_revisions\"]['year'] = globals()[f\"{sector}_monthly_revisions\"]['revision_date'].str.split('_').str[1]\n",
    "\n",
    "# Mapear los nombres de los meses a sus respectivos números\n",
    "month_mapping = {\n",
    "    'ene': '01', 'feb': '02', 'mar': '03', 'abr': '04',\n",
    "    'may': '05', 'jun': '06', 'jul': '07', 'ago': '08',\n",
    "    'sep': '09', 'oct': '10', 'nov': '11', 'dic': '12'\n",
    "}\n",
    "\n",
    "globals()[f\"{sector}_monthly_revisions\"]['month'] = globals()[f\"{sector}_monthly_revisions\"]['month'].map(month_mapping)\n",
    "\n",
    "# Crear una nueva columna con la fecha en formato YYYY-MM-DD\n",
    "globals()[f\"{sector}_monthly_revisions\"]['revision_date'] = globals()[f\"{sector}_monthly_revisions\"]['year'] + '-' + globals()[f\"{sector}_monthly_revisions\"]['month']\n",
    "\n",
    "# Convertir la columna 'revision_date' a tipo de datos de fecha\n",
    "globals()[f\"{sector}_monthly_revisions\"]['revision_date'] = pd.to_datetime(globals()[f\"{sector}_monthly_revisions\"]['revision_date'], format='%Y-%m')\n",
    "\n",
    "# Eliminar columnas temporales 'month' y 'year'\n",
    "globals()[f\"{sector}_monthly_revisions\"].drop(['month', 'year'], axis=1, inplace=True)\n",
    "\n",
    "# Mostrar el resultado\n",
    "globals()[f\"{sector}_monthly_revisions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d089739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79df4c62",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda85bba",
   "metadata": {},
   "source": [
    "<div id=\"4-4\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d3554",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.4.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Loading SQL\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e2d194",
   "metadata": {},
   "source": [
    "# Growth Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Get environment variables\n",
    "user = os.environ.get('CIUP_SQL_USER')\n",
    "password = os.environ.get('CIUP_SQL_PASS')\n",
    "host = os.environ.get('CIUP_SQL_HOST')\n",
    "port = 5432\n",
    "database = 'gdp_revisions_datasets'\n",
    "\n",
    "# Check if all environment variables are defined\n",
    "if not all([host, user, password]):\n",
    "    raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "# Create connection string\n",
    "connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# gdp_monthly_growth_rates is the DataFrame you want to save to the database\n",
    "#gdp_annual_growth_rates.to_sql('gdp_annual_growth_rates_2013', engine, index=False, if_exists='replace')\n",
    "#gdp_quarterly_growth_rates.to_sql('gdp_quarterly_growth_rates', engine, index=False, if_exists='replace')\n",
    "#gdp_monthly_growth_rates.to_sql(f'{sector}_monthly_growth_rates', engine, index=False, if_exists='replace')\n",
    "\n",
    "# REVISIONES\n",
    "\n",
    "globals()[f\"{sector}_monthly_growth_rates\"].to_sql(f'{sector}_monthly_growth_rates', engine, index=False, if_exists='replace')\n",
    "#globals()[f\"{sector}_quarterly_growth_rates\"].to_sql(f'{sector}_quarterly_growth_rates', engine, index=False, if_exists='replace')\n",
    "#globals()[f\"{sector}_annual_growth_rates\"].to_sql(f'{sector}_annual_growth_rates', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb622ad1",
   "metadata": {},
   "source": [
    "# Revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132cb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Get environment variables\n",
    "user = os.environ.get('CIUP_SQL_USER')\n",
    "password = os.environ.get('CIUP_SQL_PASS')\n",
    "host = os.environ.get('CIUP_SQL_HOST')\n",
    "port = 5432\n",
    "database = 'gdp_revisions_datasets'\n",
    "\n",
    "# Check if all environment variables are defined\n",
    "if not all([host, user, password]):\n",
    "    raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "# Create connection string\n",
    "connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# gdp_monthly_growth_rates is the DataFrame you want to save to the database\n",
    "#gdp_annual_growth_rates.to_sql('gdp_annual_growth_rates_2013', engine, index=False, if_exists='replace')\n",
    "#gdp_quarterly_growth_rates.to_sql('gdp_quarterly_growth_rates', engine, index=False, if_exists='replace')\n",
    "#gdp_monthly_growth_rates.to_sql(f'{sector}_monthly_growth_rates', engine, index=False, if_exists='replace')\n",
    "\n",
    "# REVISIONES\n",
    "\n",
    "globals()[f\"{sector}_monthly_revisions\"].to_sql(f'{sector}_monthly_revisions', engine, index=False, if_exists='replace')\n",
    "#globals()[f\"{sector}_quarterly_revisions\"].to_sql(f'{sector}_quarterly_revisions', engine, index=False, if_exists='replace')\n",
    "#globals()[f\"{sector}_annual_revisions\"].to_sql(f'{sector}_annual_revisions', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443ebfd",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af48bc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15cd18b4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc920fd",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eae11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf105ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdp_revisions",
   "language": "python",
   "name": "gdp_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
