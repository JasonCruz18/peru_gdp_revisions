{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd9a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a055b3b9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter bt pro roman'; color: rgb(0, 65, 75);\">\n",
    "    <h1>\n",
    "    GDP Revisions Datasets\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd88a4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter bt pro roman'; color: rgb(0, 65, 75);\">\n",
    "    <h3>\n",
    "        Documentation\n",
    "        <br>\n",
    "        ____________________\n",
    "            </br>\n",
    "    </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d22837",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'PT Serif Pro Book'; color: rgb(0, 65, 75); font-size: 16px;\">\n",
    "    Jason Cruz\n",
    "    <br>\n",
    "    <a href=\"mailto:jj.cruza@up.edu.pe\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">\n",
    "        jj.cruza@up.edu.pe\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72f519",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "This <span style=\"color: rgb(0, 65, 75);\">jupyter notebook</span> documents step-by-step the <b>construction of datasets</b> for the project <b>'Revisions and biases in preliminary GDP estimates in Peru'</b>.\n",
    "\n",
    "This jupyter notebook goes from downloading the Weekly Notes (NS) from the Central Reserve Bank of Peru (BCRP), stored on their website as PDF files, to generating datasets of growth rates and revisions to Peru's GDP, loaded as tables to SQL. The NS contain the information on annual, quarterly and monthly GDP growth rates by economic sectors of Peru, while the main datasets that will be used for the data analysis of this project are generated in this jupyter notebook using big data and machine learning techniques.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28540df2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Amaya; text-align: left; color: rgb(0, 65, 75); font-size:16px\">The following <b>outline is functional</b>. By utilising the provided buttons, users are able to enhance their experience by browsing this script.<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc04c9",
   "metadata": {},
   "source": [
    "<div id=\"outilne\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0fa13",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #272727; padding: 10px;\">\n",
    "<h2 style=\"text-align: left; font-family: 'charter'; color: #E0E0E0;\">\n",
    "    Outline\n",
    "    </h2>\n",
    "    <br>\n",
    "    <a href=\"#libraries\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        Libraries</a>\n",
    "    <br>\n",
    "    <a href=\"#setup\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        Initial set-up</a>\n",
    "    <br>\n",
    "    <a href=\"#1\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        1. PDF Downloader</a>\n",
    "    <br>\n",
    "    <a href=\"#2\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        2. Data cleaning</a>\n",
    "    <br>\n",
    "    <a href=\"#2-1\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        2.1. 'pdfplumber' demo.</a>\n",
    "    <br>\n",
    "    <a href=\"#2-1-1\" style=\"color: #E0E0E0; font-size: 12px;\">\n",
    "        2.1.1. What data would we get if we used the default settings?.</a>   \n",
    "    <br>\n",
    "    <a href=\"#2-1-2\" style=\"color: #E0E0E0; font-size: 12px;\">\n",
    "        2.1.2. Using custom '.extract_table' settings.</a>\n",
    "    <br> \n",
    "    <a href=\"#2-2\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        2.2. Extracting tables and generating dataframes (includes data cleanup).</a>\n",
    "    <br>\n",
    "    <a href=\"#3\" style=\"color: #687EFF; font-size: 18px;\">3. SQL Tables</a>\n",
    "    <br>\n",
    "    <a href=\"#3-1\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.1. Annual Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-2\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.2. Quarterly Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-3\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.3. Monthly Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-4\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.4. Loading SQL.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90323106",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    Any questions or issues regarding the coding, please <a href=\"mailto:jj.cruza@alum.up.edu.pe\" style=\"color: rgb(0, 153, 123)\">email Jason Cruz\n",
    "    </a>.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac40ed0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\"\">\n",
    "    If you don't have the libraries below, please use the following code (as example) to install the required libraries.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c73193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce1e25d",
   "metadata": {},
   "source": [
    "<div id=\"libraries\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907046a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark;\">\n",
    "    <h2>\n",
    "    Libraries\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214f5982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# 1. PDF downloader\n",
    "\n",
    "import os  # for file and directory manipulation\n",
    "import random  # to generate random numbers\n",
    "from selenium import webdriver  # for automating web browsers\n",
    "from selenium.webdriver.common.by import By  # to locate elements on a webpage\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # to wait until certain conditions are met on a webpage.\n",
    "from selenium.webdriver.support import expected_conditions as EC  # to define expected conditions\n",
    "from selenium.common.exceptions import StaleElementReferenceException  # To handle exceptions related to elements on the webpage that are no longer available.\n",
    "import pygame # Allows you to handle graphics, sounds and input events.\n",
    "\n",
    "# 2. Extracting tables (and data cleaning)\n",
    "\n",
    "import pdfplumber  # for extracting text and metadata from PDF files\n",
    "import pandas as pd  # for data manipulation and analysis\n",
    "import os  # for interacting with the operating system\n",
    "import unicodedata  # for manipulating Unicode data\n",
    "import re  # for regular expressions operations\n",
    "from datetime import datetime  # for working with dates and times\n",
    "import locale  # for locale-specific formatting of numbers, dates, and currencies\n",
    "\n",
    "\n",
    "# 3. SQL tables\n",
    "\n",
    "import psycopg2  # for interacting with PostgreSQL databases\n",
    "from sqlalchemy import create_engine, text  # for creating and executing SQL queries using SQLAlchemy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ed0aab",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6daf1",
   "metadata": {},
   "source": [
    "<div id=\"setup\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ef8f8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark;\">\n",
    "    <h2>\n",
    "    Initial set-up\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc1c80",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\"> The following code lines will create folders in your current path, call them to import and export your outputs. <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8899a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save downloaded PDF files\n",
    "\n",
    "raw_pdf = 'raw_pdf' # to save raw data (.pdf).\n",
    "if not os.path.exists(raw_pdf):\n",
    "    os.mkdir(raw_pdf) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26b4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save text file with the names of already downloaded files\n",
    "\n",
    "download_record = 'download_record'\n",
    "if not os.path.exists(download_record):\n",
    "    os.mkdir(download_record) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147227ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to download the trimmed PDF files (these are PDF inputs for the extraction and cleanup code)\n",
    "\n",
    "input_pdf = 'input_pdf'\n",
    "if not os.path.exists(input_pdf):\n",
    "    os.makedirs(input_pdf) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6dc316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save PDF files containing only the pages of interest (where the GDP growth rate tables are located)\n",
    "\n",
    "input_pdf_record = 'input_pdf_record'\n",
    "if not os.path.exists(input_pdf_record):\n",
    "    os.makedirs(input_pdf_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d69eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save dataframes generated record by year\n",
    "\n",
    "dataframes_record = 'dataframes_record'\n",
    "if not os.path.exists(dataframes_record):\n",
    "    os.makedirs(dataframes_record) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb98b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save sound files\n",
    "\n",
    "sound_folder = 'sound'\n",
    "if not os.path.exists(sound_folder):\n",
    "    os.makedirs(sound_folder) # to create the folder (if it doesn't exist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0038d2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e051c",
   "metadata": {},
   "source": [
    "<div id=\"1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f3192",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: 'PT Serif Pro Book'; color: dark;\">1.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">PDF Downloader</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98250a64",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Our main source for data collection is the <a href=\"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\" style=\"color: rgb(0, 153, 123)\">BCRP's web page</a>. The weekly note is a periodic (weekly) publication of the BCRP in compliance with article 84 of the Peruvian Constitution and articles 2 and 74 of the BCRP's organic law, which include, among its functions, the periodic publication of the main national macroeconomic statistics. Our project requires the publication of two tables: the table of monthly growth rates of real GDP (12-month percentage changes), and the table of quarterly (annual) growth rates of real GDP. These tables are referred to as Table 1 and Table 2, respectively, throughout this jupyter notebook.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcc4e3",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    The bot runs the following steps:\n",
    "    <ol>\n",
    "        <li>Download the PDF files (NS) from the BCRP web page, starting with the oldest and continuing to the most recent.</li>\n",
    "        <li>Notify you with a fabulous song each time a certain number of downloads is reached.</li>\n",
    "        <li>Display a window asking if you want to continue with the downloads. You can stop them at any time.</li>\n",
    "        <li>Report in detail about the downloaded files. If a file has already been downloaded, you will also be notified.</li>\n",
    "        <li>Save the raw PDFs to the paths set in the preamble of this Jupyter Notebook.</li>\n",
    "    </ol>\n",
    "    Try the bot, it's an adventure!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e812cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the BCRP URL\n",
    "bcrp_url = \"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\"  # Never replace this URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269c03d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    We import the functions required by the bot to execute the above steps.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc7e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdp_revisions_datasets_functions import play_sound, random_wait, download_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e986d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Let's run the code for the bot to do its downloading tasks.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee0199bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site opened successfully\n",
      "1. The file ns-01-2013.pdf has already been downloaded previously. Skipping...\n",
      "2. The file ns-02-2013.pdf has already been downloaded previously. Skipping...\n",
      "3. The file ns-03-2013.pdf has already been downloaded previously. Skipping...\n",
      "4. The file ns-04-2013.pdf has already been downloaded previously. Skipping...\n",
      "5. The file ns-05-2013.pdf has already been downloaded previously. Skipping...\n",
      "6. The file ns-06-2013.pdf has already been downloaded previously. Skipping...\n",
      "7. The file ns-07-2013.pdf has already been downloaded previously. Skipping...\n",
      "8. The file ns-08-2013.pdf has already been downloaded previously. Skipping...\n",
      "9. The file ns-09-2013.pdf has already been downloaded previously. Skipping...\n",
      "10. The file ns-10-2013.pdf has already been downloaded previously. Skipping...\n",
      "11. The file ns-11-2013.pdf has already been downloaded previously. Skipping...\n",
      "12. The file ns-12-2013.pdf has already been downloaded previously. Skipping...\n",
      "13. The file ns-13-2013.pdf has already been downloaded previously. Skipping...\n",
      "14. The file ns-14-2013.pdf has already been downloaded previously. Skipping...\n",
      "15. The file ns-15-2013.pdf has already been downloaded previously. Skipping...\n",
      "16. The file ns-16-2013.pdf has already been downloaded previously. Skipping...\n",
      "17. The file ns-17-2013.pdf has already been downloaded previously. Skipping...\n",
      "18. The file ns-18-2013.pdf has already been downloaded previously. Skipping...\n",
      "19. The file ns-19-2013.pdf has already been downloaded previously. Skipping...\n",
      "20. The file ns-20-2013.pdf has already been downloaded previously. Skipping...\n",
      "21. The file ns-21-2013.pdf has already been downloaded previously. Skipping...\n",
      "22. The file ns-22-2013.pdf has already been downloaded previously. Skipping...\n",
      "23. The file ns-23-2013.pdf has already been downloaded previously. Skipping...\n",
      "24. The file ns-24-2013.pdf has already been downloaded previously. Skipping...\n",
      "25. The file ns-25-2013.pdf has already been downloaded previously. Skipping...\n",
      "26. The file ns-26-2013.pdf has already been downloaded previously. Skipping...\n",
      "27. The file ns-27-2013.pdf has already been downloaded previously. Skipping...\n",
      "28. The file ns-28-2013.pdf has already been downloaded previously. Skipping...\n",
      "29. The file ns-29-2013.pdf has already been downloaded previously. Skipping...\n",
      "30. The file ns-30-2013.pdf has already been downloaded previously. Skipping...\n",
      "31. The file ns-31-2013.pdf has already been downloaded previously. Skipping...\n",
      "32. The file ns-32-2013.pdf has already been downloaded previously. Skipping...\n",
      "33. The file ns-33-2013.pdf has already been downloaded previously. Skipping...\n",
      "34. The file ns-34-2013.pdf has already been downloaded previously. Skipping...\n",
      "35. The file ns-35-2013.pdf has already been downloaded previously. Skipping...\n",
      "36. The file ns-36-2013.pdf has already been downloaded previously. Skipping...\n",
      "37. The file ns-37-2013.pdf has already been downloaded previously. Skipping...\n",
      "38. The file ns-38-2013.pdf has already been downloaded previously. Skipping...\n",
      "39. The file ns-39-2013.pdf has already been downloaded previously. Skipping...\n",
      "40. The file ns-40-2013.pdf has already been downloaded previously. Skipping...\n",
      "41. The file ns-41-2013.pdf has already been downloaded previously. Skipping...\n",
      "42. The file ns-42-2013.pdf has already been downloaded previously. Skipping...\n",
      "43. The file ns-43-2013.pdf has already been downloaded previously. Skipping...\n",
      "44. The file ns-44-2013.pdf has already been downloaded previously. Skipping...\n",
      "45. The file ns-45-2013.pdf has already been downloaded previously. Skipping...\n",
      "46. The file ns-46-2013.pdf has already been downloaded previously. Skipping...\n",
      "47. The file ns-47-2013.pdf has already been downloaded previously. Skipping...\n",
      "48. The file ns-48-2013.pdf has already been downloaded previously. Skipping...\n",
      "49. The file ns-49-2013.pdf has already been downloaded previously. Skipping...\n",
      "50. The file ns-50-2013.pdf has already been downloaded previously. Skipping...\n",
      "51. The file ns-01-2014.pdf has already been downloaded previously. Skipping...\n",
      "52. The file ns-02-2014.pdf has already been downloaded previously. Skipping...\n",
      "53. The file ns-03-2014.pdf has already been downloaded previously. Skipping...\n",
      "54. The file ns-04-2014.pdf has already been downloaded previously. Skipping...\n",
      "55. The file ns-05-2014.pdf has already been downloaded previously. Skipping...\n",
      "56. The file ns-06-2014.pdf has already been downloaded previously. Skipping...\n",
      "57. The file ns-07-2014.pdf has already been downloaded previously. Skipping...\n",
      "58. The file ns-08-2014.pdf has already been downloaded previously. Skipping...\n",
      "59. The file ns-09-2014.pdf has already been downloaded previously. Skipping...\n",
      "60. The file ns-10-2014.pdf has already been downloaded previously. Skipping...\n",
      "61. The file ns-11-2014.pdf has already been downloaded previously. Skipping...\n",
      "62. The file ns-12-2014.pdf has already been downloaded previously. Skipping...\n",
      "63. The file ns-13-2014.pdf has already been downloaded previously. Skipping...\n",
      "64. The file ns-14-2014.pdf has already been downloaded previously. Skipping...\n",
      "65. The file ns-15-2014.pdf has already been downloaded previously. Skipping...\n",
      "66. The file ns-16-2014.pdf has already been downloaded previously. Skipping...\n",
      "67. The file ns-17-2014.pdf has already been downloaded previously. Skipping...\n",
      "68. The file ns-18-2014.pdf has already been downloaded previously. Skipping...\n",
      "69. The file ns-19-2014.pdf has already been downloaded previously. Skipping...\n",
      "70. The file ns-20-2014.pdf has already been downloaded previously. Skipping...\n",
      "71. The file ns-21-2014.pdf has already been downloaded previously. Skipping...\n",
      "72. The file ns-22-2014.pdf has already been downloaded previously. Skipping...\n",
      "73. The file ns-23-2014.pdf has already been downloaded previously. Skipping...\n",
      "74. The file ns-24-2014.pdf has already been downloaded previously. Skipping...\n",
      "75. The file ns-25-2014.pdf has already been downloaded previously. Skipping...\n",
      "76. The file ns-26-2014.pdf has already been downloaded previously. Skipping...\n",
      "77. The file ns-27-2014.pdf has already been downloaded previously. Skipping...\n",
      "78. The file ns-28-2014.pdf has already been downloaded previously. Skipping...\n",
      "79. The file ns-29-2014.pdf has already been downloaded previously. Skipping...\n",
      "80. The file ns-30-2014.pdf has already been downloaded previously. Skipping...\n",
      "81. The file ns-31-2014.pdf has already been downloaded previously. Skipping...\n",
      "82. The file ns-32-2014.pdf has already been downloaded previously. Skipping...\n",
      "83. The file ns-33-2014.pdf has already been downloaded previously. Skipping...\n",
      "84. The file ns-34-2014.pdf has already been downloaded previously. Skipping...\n",
      "85. The file ns-35-2014.pdf has already been downloaded previously. Skipping...\n",
      "86. The file ns-36-2014.pdf has already been downloaded previously. Skipping...\n",
      "87. The file ns-37-2014.pdf has already been downloaded previously. Skipping...\n",
      "88. The file ns-38-2014.pdf has already been downloaded previously. Skipping...\n",
      "89. The file ns-39-2014.pdf has already been downloaded previously. Skipping...\n",
      "90. The file ns-40-2014.pdf has already been downloaded previously. Skipping...\n",
      "91. The file ns-41-2014.pdf has already been downloaded previously. Skipping...\n",
      "92. The file ns-42-2014.pdf has already been downloaded previously. Skipping...\n",
      "93. The file ns-43-2014.pdf has already been downloaded previously. Skipping...\n",
      "94. The file ns-44-2014.pdf has already been downloaded previously. Skipping...\n",
      "95. The file ns-45-2014.pdf has already been downloaded previously. Skipping...\n",
      "96. The file ns-46-2014.pdf has already been downloaded previously. Skipping...\n",
      "97. The file ns-47-2014.pdf has already been downloaded previously. Skipping...\n",
      "98. The file ns-48-2014.pdf has already been downloaded previously. Skipping...\n",
      "99. The file ns-49-2014.pdf has already been downloaded previously. Skipping...\n",
      "100. The file ns-01-2015.pdf has already been downloaded previously. Skipping...\n",
      "101. The file ns-02-2015.pdf has already been downloaded previously. Skipping...\n",
      "102. The file ns-03-2015.pdf has already been downloaded previously. Skipping...\n",
      "103. The file ns-04-2015.pdf has already been downloaded previously. Skipping...\n",
      "104. The file ns-05-2015.pdf has already been downloaded previously. Skipping...\n",
      "105. The file ns-06-2015.pdf has already been downloaded previously. Skipping...\n",
      "106. The file ns-07-2015.pdf has already been downloaded previously. Skipping...\n",
      "107. The file ns-08-2015.pdf has already been downloaded previously. Skipping...\n",
      "108. The file ns-09-2015.pdf has already been downloaded previously. Skipping...\n",
      "109. The file ns-10-2015.pdf has already been downloaded previously. Skipping...\n",
      "110. The file ns-11-2015.pdf has already been downloaded previously. Skipping...\n",
      "111. The file ns-12-2015.pdf has already been downloaded previously. Skipping...\n",
      "112. The file ns-13-2015.pdf has already been downloaded previously. Skipping...\n",
      "113. The file ns-14-2015.pdf has already been downloaded previously. Skipping...\n",
      "114. The file ns-15-2015.pdf has already been downloaded previously. Skipping...\n",
      "115. The file ns-16-2015.pdf has already been downloaded previously. Skipping...\n",
      "116. The file ns-17-2015.pdf has already been downloaded previously. Skipping...\n",
      "117. The file ns-18-2015.pdf has already been downloaded previously. Skipping...\n",
      "118. The file ns-19-2015.pdf has already been downloaded previously. Skipping...\n",
      "119. The file ns-20-2015.pdf has already been downloaded previously. Skipping...\n",
      "120. The file ns-21-2015.pdf has already been downloaded previously. Skipping...\n",
      "121. The file ns-21-2015.pdf has already been downloaded previously. Skipping...\n",
      "122. The file ns-22-2015.pdf has already been downloaded previously. Skipping...\n",
      "123. The file ns-23-2015.pdf has already been downloaded previously. Skipping...\n",
      "124. The file ns-24-2015.pdf has already been downloaded previously. Skipping...\n",
      "125. The file ns-25-2015.pdf has already been downloaded previously. Skipping...\n",
      "126. The file ns-26-2015.pdf has already been downloaded previously. Skipping...\n",
      "127. The file ns-27-2015.pdf has already been downloaded previously. Skipping...\n",
      "128. The file ns-28-2015.pdf has already been downloaded previously. Skipping...\n",
      "129. The file ns-29-2015.pdf has already been downloaded previously. Skipping...\n",
      "130. The file ns-30-2015.pdf has already been downloaded previously. Skipping...\n",
      "131. The file ns-31-2015.pdf has already been downloaded previously. Skipping...\n",
      "132. The file ns-32-2015.pdf has already been downloaded previously. Skipping...\n",
      "133. The file ns-33-2015.pdf has already been downloaded previously. Skipping...\n",
      "134. The file ns-34-2015.pdf has already been downloaded previously. Skipping...\n",
      "135. The file ns-35-2015.pdf has already been downloaded previously. Skipping...\n",
      "136. The file ns-36-2015.pdf has already been downloaded previously. Skipping...\n",
      "137. The file ns-37-2015.pdf has already been downloaded previously. Skipping...\n",
      "138. The file ns-38-2015.pdf has already been downloaded previously. Skipping...\n",
      "139. The file ns-39-2015.pdf has already been downloaded previously. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140. The file ns-40-2015.pdf has already been downloaded previously. Skipping...\n",
      "141. The file ns-41-2015.pdf has already been downloaded previously. Skipping...\n",
      "142. The file ns-42-2015.pdf has already been downloaded previously. Skipping...\n",
      "143. The file ns-43-2015.pdf has already been downloaded previously. Skipping...\n",
      "144. The file ns-44-2015.pdf has already been downloaded previously. Skipping...\n",
      "145. The file ns-45-2015.pdf has already been downloaded previously. Skipping...\n",
      "146. The file ns-45-2015.pdf has already been downloaded previously. Skipping...\n",
      "147. The file ns-46-2015.pdf has already been downloaded previously. Skipping...\n",
      "148. The file ns-47-2015.pdf has already been downloaded previously. Skipping...\n",
      "149. The file ns-48-2015.pdf has already been downloaded previously. Skipping...\n",
      "150. The file ns-01-2016.pdf has already been downloaded previously. Skipping...\n",
      "151. The file ns-02-2016.pdf has already been downloaded previously. Skipping...\n",
      "152. The file ns-03-2016.pdf has already been downloaded previously. Skipping...\n",
      "153. The file ns-04-2016.pdf has already been downloaded previously. Skipping...\n",
      "154. The file ns-05-2016.pdf has already been downloaded previously. Skipping...\n",
      "155. The file ns-06-2016.pdf has already been downloaded previously. Skipping...\n",
      "156. The file ns-07-2016.pdf has already been downloaded previously. Skipping...\n",
      "157. The file ns-08-2016.pdf has already been downloaded previously. Skipping...\n",
      "158. The file ns-09-2016.pdf has already been downloaded previously. Skipping...\n",
      "159. The file ns-10-2016.pdf has already been downloaded previously. Skipping...\n",
      "160. The file ns-11-2016.pdf has already been downloaded previously. Skipping...\n",
      "161. The file ns-12-2016.pdf has already been downloaded previously. Skipping...\n",
      "162. The file ns-13-2016.pdf has already been downloaded previously. Skipping...\n",
      "163. The file ns-14-2016.pdf has already been downloaded previously. Skipping...\n",
      "164. The file ns-15-2016.pdf has already been downloaded previously. Skipping...\n",
      "165. The file ns-16-2016.pdf has already been downloaded previously. Skipping...\n",
      "166. The file ns-17-2016.pdf has already been downloaded previously. Skipping...\n",
      "167. The file ns-18-2016.pdf has already been downloaded previously. Skipping...\n",
      "168. The file ns-19-2016.pdf has already been downloaded previously. Skipping...\n",
      "169. The file ns-20-2016.pdf has already been downloaded previously. Skipping...\n",
      "170. The file ns-21-2016.pdf has already been downloaded previously. Skipping...\n",
      "171. The file ns-22-2016.pdf has already been downloaded previously. Skipping...\n",
      "172. The file ns-23-2016.pdf has already been downloaded previously. Skipping...\n",
      "173. The file ns-24-2016.pdf has already been downloaded previously. Skipping...\n",
      "174. The file ns-25-2016.pdf has already been downloaded previously. Skipping...\n",
      "175. The file ns-26-2016.pdf has already been downloaded previously. Skipping...\n",
      "176. The file ns-27-2016.pdf has already been downloaded previously. Skipping...\n",
      "177. The file ns-28-2016.pdf has already been downloaded previously. Skipping...\n",
      "178. The file ns-29-2016.pdf has already been downloaded previously. Skipping...\n",
      "179. The file ns-30-2016.pdf has already been downloaded previously. Skipping...\n",
      "180. The file ns-31-2016.pdf has already been downloaded previously. Skipping...\n",
      "181. The file ns-32-2016.pdf has already been downloaded previously. Skipping...\n",
      "182. The file ns-33-2016.pdf has already been downloaded previously. Skipping...\n",
      "183. The file ns-34-2016.pdf has already been downloaded previously. Skipping...\n",
      "184. The file ns-35-2016.pdf has already been downloaded previously. Skipping...\n",
      "185. The file ns-36-2016.pdf has already been downloaded previously. Skipping...\n",
      "186. The file ns-37-2016.pdf has already been downloaded previously. Skipping...\n",
      "187. The file ns-38-2016.pdf has already been downloaded previously. Skipping...\n",
      "188. The file ns-39-2016.pdf has already been downloaded previously. Skipping...\n",
      "189. The file ns-40-2016.pdf has already been downloaded previously. Skipping...\n",
      "190. The file ns-41-2016.pdf has already been downloaded previously. Skipping...\n",
      "191. The file ns-42-2016.pdf has already been downloaded previously. Skipping...\n",
      "192. The file ns-43-2016.pdf has already been downloaded previously. Skipping...\n",
      "193. The file ns-44-2016.pdf has already been downloaded previously. Skipping...\n",
      "194. The file ns-45-2016.pdf has already been downloaded previously. Skipping...\n",
      "195. The file ns-46-2016.pdf has already been downloaded previously. Skipping...\n",
      "196. The file ns-47-2016.pdf has already been downloaded previously. Skipping...\n",
      "197. The file ns-48-2016.pdf has already been downloaded previously. Skipping...\n",
      "198. The file ns-01-2017.pdf has already been downloaded previously. Skipping...\n",
      "199. The file ns-02-2017.pdf has already been downloaded previously. Skipping...\n",
      "200. The file ns-03-2017.pdf has already been downloaded previously. Skipping...\n",
      "201. The file ns-04-2017.pdf has already been downloaded previously. Skipping...\n",
      "202. The file ns-05-2017.pdf has already been downloaded previously. Skipping...\n",
      "203. The file ns-06-2017.pdf has already been downloaded previously. Skipping...\n",
      "204. The file ns-07-2017.pdf has already been downloaded previously. Skipping...\n",
      "205. The file ns-08-2017.pdf has already been downloaded previously. Skipping...\n",
      "206. The file ns-09-2017.pdf has already been downloaded previously. Skipping...\n",
      "207. The file ns-10-2017.pdf has already been downloaded previously. Skipping...\n",
      "208. The file ns-11-2017.pdf has already been downloaded previously. Skipping...\n",
      "209. The file ns-12-2017.pdf has already been downloaded previously. Skipping...\n",
      "210. The file ns-13-2017.pdf has already been downloaded previously. Skipping...\n",
      "211. The file ns-14-2017.pdf has already been downloaded previously. Skipping...\n",
      "212. The file ns-15-2017.pdf has already been downloaded previously. Skipping...\n",
      "213. The file ns-16-2017.pdf has already been downloaded previously. Skipping...\n",
      "214. The file ns-17-2017.pdf has already been downloaded previously. Skipping...\n",
      "215. The file ns-18-2017.pdf has already been downloaded previously. Skipping...\n",
      "216. The file ns-19-2017.pdf has already been downloaded previously. Skipping...\n",
      "217. The file ns-20-2017.pdf has already been downloaded previously. Skipping...\n",
      "218. The file ns-21-2017.pdf has already been downloaded previously. Skipping...\n",
      "219. The file ns-22-2017.pdf has already been downloaded previously. Skipping...\n",
      "220. The file ns-23-2017.pdf has already been downloaded previously. Skipping...\n",
      "221. The file ns-24-2017.pdf has already been downloaded previously. Skipping...\n",
      "222. The file ns-25-2017.pdf has already been downloaded previously. Skipping...\n",
      "223. The file ns-26-2017.pdf has already been downloaded previously. Skipping...\n",
      "224. The file ns-27-2017.pdf has already been downloaded previously. Skipping...\n",
      "225. The file ns-28-2017.pdf has already been downloaded previously. Skipping...\n",
      "226. The file ns-29-2017.pdf has already been downloaded previously. Skipping...\n",
      "227. The file ns-30-2017.pdf has already been downloaded previously. Skipping...\n",
      "228. The file ns-31-2017.pdf has already been downloaded previously. Skipping...\n",
      "229. The file ns-32-2017.pdf has already been downloaded previously. Skipping...\n",
      "230. The file ns-33-2017.pdf has already been downloaded previously. Skipping...\n",
      "231. The file ns-34-2017.pdf has already been downloaded previously. Skipping...\n",
      "232. The file ns-35-2017.pdf has already been downloaded previously. Skipping...\n",
      "233. The file ns-36-2017.pdf has already been downloaded previously. Skipping...\n",
      "234. The file ns-37-2017.pdf has already been downloaded previously. Skipping...\n",
      "235. The file ns-38-2017.pdf has already been downloaded previously. Skipping...\n",
      "236. The file ns-39-2017.pdf has already been downloaded previously. Skipping...\n",
      "237. The file ns-40-2017.pdf has already been downloaded previously. Skipping...\n",
      "238. The file ns-41-2017.pdf has already been downloaded previously. Skipping...\n",
      "239. The file ns-42-2017.pdf has already been downloaded previously. Skipping...\n",
      "240. The file ns-43-2017.pdf has already been downloaded previously. Skipping...\n",
      "241. The file ns-44-2017.pdf has already been downloaded previously. Skipping...\n",
      "242. The file ns-45-2017.pdf has already been downloaded previously. Skipping...\n",
      "243. The file ns-46-2017.pdf has already been downloaded previously. Skipping...\n",
      "244. The file ns-47-2017.pdf has already been downloaded previously. Skipping...\n",
      "245. The file ns-48-2017.pdf has already been downloaded previously. Skipping...\n",
      "246. The file ns-49-2017.pdf has already been downloaded previously. Skipping...\n",
      "247. The file ns-01-2018.pdf has already been downloaded previously. Skipping...\n",
      "248. The file ns-02-2018.pdf has already been downloaded previously. Skipping...\n",
      "249. The file ns-03-2018.pdf has already been downloaded previously. Skipping...\n",
      "250. The file ns-04-2018.pdf has already been downloaded previously. Skipping...\n",
      "251. The file ns-05-2018.pdf has already been downloaded previously. Skipping...\n",
      "252. The file ns-06-2018.pdf has already been downloaded previously. Skipping...\n",
      "253. The file ns-07-2018.pdf has already been downloaded previously. Skipping...\n",
      "254. The file ns-08-2018.pdf has already been downloaded previously. Skipping...\n",
      "255. The file ns-09-2018.pdf has already been downloaded previously. Skipping...\n",
      "256. The file ns-10-2018.pdf has already been downloaded previously. Skipping...\n",
      "257. The file ns-11-2018.pdf has already been downloaded previously. Skipping...\n",
      "258. The file ns-12-2018.pdf has already been downloaded previously. Skipping...\n",
      "259. The file ns-13-2018.pdf has already been downloaded previously. Skipping...\n",
      "260. The file ns-14-2018.pdf has already been downloaded previously. Skipping...\n",
      "261. The file ns-15-2018.pdf has already been downloaded previously. Skipping...\n",
      "262. The file ns-16-2018.pdf has already been downloaded previously. Skipping...\n",
      "263. The file ns-17-2018.pdf has already been downloaded previously. Skipping...\n",
      "264. The file ns-18-2018.pdf has already been downloaded previously. Skipping...\n",
      "265. The file ns-19-2018.pdf has already been downloaded previously. Skipping...\n",
      "266. The file ns-20-2018.pdf has already been downloaded previously. Skipping...\n",
      "267. The file ns-21-2018.pdf has already been downloaded previously. Skipping...\n",
      "268. The file ns-22-2018.pdf has already been downloaded previously. Skipping...\n",
      "269. The file ns-23-2018.pdf has already been downloaded previously. Skipping...\n",
      "270. The file ns-24-2018.pdf has already been downloaded previously. Skipping...\n",
      "271. The file ns-25-2018.pdf has already been downloaded previously. Skipping...\n",
      "272. The file ns-26-2018.pdf has already been downloaded previously. Skipping...\n",
      "273. The file ns-27-2018.pdf has already been downloaded previously. Skipping...\n",
      "274. The file ns-28-2018.pdf has already been downloaded previously. Skipping...\n",
      "275. The file ns-29-2018.pdf has already been downloaded previously. Skipping...\n",
      "276. The file ns-30-2018.pdf has already been downloaded previously. Skipping...\n",
      "277. The file ns-31-2018.pdf has already been downloaded previously. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278. The file ns-32-2018.pdf has already been downloaded previously. Skipping...\n",
      "279. The file ns-33-2018.pdf has already been downloaded previously. Skipping...\n",
      "280. The file ns-34-2018.pdf has already been downloaded previously. Skipping...\n",
      "281. The file ns-35-2018.pdf has already been downloaded previously. Skipping...\n",
      "282. The file ns-36-2018.pdf has already been downloaded previously. Skipping...\n",
      "283. The file ns-37-2018.pdf has already been downloaded previously. Skipping...\n",
      "284. The file ns-38-2018.pdf has already been downloaded previously. Skipping...\n",
      "285. The file ns-39-2018.pdf has already been downloaded previously. Skipping...\n",
      "286. The file ns-40-2018.pdf has already been downloaded previously. Skipping...\n",
      "287. The file ns-41-2018.pdf has already been downloaded previously. Skipping...\n",
      "288. The file ns-42-2018.pdf has already been downloaded previously. Skipping...\n",
      "289. The file ns-43-2018.pdf has already been downloaded previously. Skipping...\n",
      "290. The file ns-44-2018.pdf has already been downloaded previously. Skipping...\n",
      "291. The file ns-45-2018.pdf has already been downloaded previously. Skipping...\n",
      "292. The file ns-46-2018.pdf has already been downloaded previously. Skipping...\n",
      "293. The file ns-47-2018.pdf has already been downloaded previously. Skipping...\n",
      "294. The file ns-48-2018.pdf has already been downloaded previously. Skipping...\n",
      "295. The file ns-49-2018.pdf has already been downloaded previously. Skipping...\n",
      "296. The file ns-01-2019.pdf has already been downloaded previously. Skipping...\n",
      "297. The file ns-02-2019.pdf has already been downloaded previously. Skipping...\n",
      "298. The file ns-03-2019.pdf has already been downloaded previously. Skipping...\n",
      "299. The file ns-04-2019.pdf has already been downloaded previously. Skipping...\n",
      "300. The file ns-05-2019.pdf has already been downloaded previously. Skipping...\n",
      "301. The file ns-06-2019.pdf has already been downloaded previously. Skipping...\n",
      "302. The file ns-07-2019.pdf has already been downloaded previously. Skipping...\n",
      "303. The file ns-08-2019.pdf has already been downloaded previously. Skipping...\n",
      "304. The file ns-09-2019.pdf has already been downloaded previously. Skipping...\n",
      "305. The file ns-10-2019.pdf has already been downloaded previously. Skipping...\n",
      "306. The file ns-11-2019.pdf has already been downloaded previously. Skipping...\n",
      "307. The file ns-12-2019.pdf has already been downloaded previously. Skipping...\n",
      "308. The file ns-13-2019.pdf has already been downloaded previously. Skipping...\n",
      "309. The file ns-14-2019.pdf has already been downloaded previously. Skipping...\n",
      "310. The file ns-15-2019.pdf has already been downloaded previously. Skipping...\n",
      "311. The file ns-16-2019.pdf has already been downloaded previously. Skipping...\n",
      "312. The file ns-17-2019.pdf has already been downloaded previously. Skipping...\n",
      "313. The file ns-18-2019.pdf has already been downloaded previously. Skipping...\n",
      "314. The file ns-19-2019.pdf has already been downloaded previously. Skipping...\n",
      "315. The file ns-20-2019.pdf has already been downloaded previously. Skipping...\n",
      "316. The file ns-21-2019.pdf has already been downloaded previously. Skipping...\n",
      "317. The file ns-22-2019.pdf has already been downloaded previously. Skipping...\n",
      "318. The file ns-23-2019.pdf has already been downloaded previously. Skipping...\n",
      "319. The file ns-24-2019.pdf has already been downloaded previously. Skipping...\n",
      "320. The file ns-25-2019.pdf has already been downloaded previously. Skipping...\n",
      "321. The file ns-26-2019.pdf has already been downloaded previously. Skipping...\n",
      "322. The file ns-27-2019.pdf has already been downloaded previously. Skipping...\n",
      "323. The file ns-28-2019.pdf has already been downloaded previously. Skipping...\n",
      "324. The file ns-29-2019.pdf has already been downloaded previously. Skipping...\n",
      "325. The file ns-30-2019.pdf has already been downloaded previously. Skipping...\n",
      "326. The file ns-31-2019.pdf has already been downloaded previously. Skipping...\n",
      "327. The file ns-32-2019.pdf has already been downloaded previously. Skipping...\n",
      "328. The file ns-33-2019.pdf has already been downloaded previously. Skipping...\n",
      "329. The file ns-34-2019.pdf has already been downloaded previously. Skipping...\n",
      "330. The file ns-35-2019.pdf has already been downloaded previously. Skipping...\n",
      "331. The file ns-36-2019.pdf has already been downloaded previously. Skipping...\n",
      "332. The file ns-37-2019.pdf has already been downloaded previously. Skipping...\n",
      "333. The file ns-38-2019.pdf has already been downloaded previously. Skipping...\n",
      "334. The file ns-39-2019.pdf has already been downloaded previously. Skipping...\n",
      "335. The file ns-01-2020.pdf has already been downloaded previously. Skipping...\n",
      "336. The file ns-02-2020.pdf has already been downloaded previously. Skipping...\n",
      "337. The file ns-03-2020.pdf has already been downloaded previously. Skipping...\n",
      "338. The file ns-04-2020.pdf has already been downloaded previously. Skipping...\n",
      "339. The file ns-05-2020.pdf has already been downloaded previously. Skipping...\n",
      "340. The file ns-06-2020.pdf has already been downloaded previously. Skipping...\n",
      "341. The file ns-07-2020.pdf has already been downloaded previously. Skipping...\n",
      "342. The file ns-08-2020.pdf has already been downloaded previously. Skipping...\n",
      "343. The file ns-09-2020.pdf has already been downloaded previously. Skipping...\n",
      "344. The file ns-10-2020.pdf has already been downloaded previously. Skipping...\n",
      "345. The file ns-11-2020.pdf has already been downloaded previously. Skipping...\n",
      "346. The file ns-12-2020.pdf has already been downloaded previously. Skipping...\n",
      "347. The file ns-13-2020.pdf has already been downloaded previously. Skipping...\n",
      "348. The file ns-14-2020.pdf has already been downloaded previously. Skipping...\n",
      "349. The file ns-15-2020.pdf has already been downloaded previously. Skipping...\n",
      "350. The file ns-16-2020.pdf has already been downloaded previously. Skipping...\n",
      "351. The file ns-17-2020.pdf has already been downloaded previously. Skipping...\n",
      "352. The file ns-18-2020.pdf has already been downloaded previously. Skipping...\n",
      "353. The file ns-19-2020.pdf has already been downloaded previously. Skipping...\n",
      "354. The file ns-20-2020.pdf has already been downloaded previously. Skipping...\n",
      "355. The file ns-21-2020.pdf has already been downloaded previously. Skipping...\n",
      "356. The file ns-22-2020.pdf has already been downloaded previously. Skipping...\n",
      "357. The file ns-23-2020.pdf has already been downloaded previously. Skipping...\n",
      "358. The file ns-24-2020.pdf has already been downloaded previously. Skipping...\n",
      "359. The file ns-25-2020.pdf has already been downloaded previously. Skipping...\n",
      "360. The file ns-26-2020.pdf has already been downloaded previously. Skipping...\n",
      "361. The file ns-27-2020.pdf has already been downloaded previously. Skipping...\n",
      "362. The file ns-28-2020.pdf has already been downloaded previously. Skipping...\n",
      "363. The file ns-29-2020.pdf has already been downloaded previously. Skipping...\n",
      "364. The file ns-30-2020.pdf has already been downloaded previously. Skipping...\n",
      "365. The file ns-31-2020.pdf has already been downloaded previously. Skipping...\n",
      "366. The file ns-32-2020.pdf has already been downloaded previously. Skipping...\n",
      "367. The file ns-33-2020.pdf has already been downloaded previously. Skipping...\n",
      "368. The file ns-34-2020.pdf has already been downloaded previously. Skipping...\n",
      "369. The file ns-35-2020.pdf has already been downloaded previously. Skipping...\n",
      "370. The file ns-36-2020.pdf has already been downloaded previously. Skipping...\n",
      "371. The file ns-37-2020.pdf has already been downloaded previously. Skipping...\n",
      "372. The file ns-38-2020.pdf has already been downloaded previously. Skipping...\n",
      "373. The file ns-39-2020.pdf has already been downloaded previously. Skipping...\n",
      "374. The file ns-40-2020.pdf has already been downloaded previously. Skipping...\n",
      "375. The file ns-41-2020.pdf has already been downloaded previously. Skipping...\n",
      "376. The file ns-42-2020.pdf has already been downloaded previously. Skipping...\n",
      "377. The file ns-43-2020.pdf has already been downloaded previously. Skipping...\n",
      "378. The file ns-44-2020.pdf has already been downloaded previously. Skipping...\n",
      "379. The file ns-45-2020.pdf has already been downloaded previously. Skipping...\n",
      "380. The file ns-46-2020.pdf has already been downloaded previously. Skipping...\n",
      "381. The file ns-47-2020.pdf has already been downloaded previously. Skipping...\n",
      "382. The file ns-01-2021.pdf has already been downloaded previously. Skipping...\n",
      "383. The file ns-02-2021.pdf has already been downloaded previously. Skipping...\n",
      "384. The file ns-03-2021.pdf has already been downloaded previously. Skipping...\n",
      "385. The file ns-04-2021.pdf has already been downloaded previously. Skipping...\n",
      "386. The file ns-05-2021.pdf has already been downloaded previously. Skipping...\n",
      "387. The file ns-06-2021.pdf has already been downloaded previously. Skipping...\n",
      "388. The file ns-07-2021.pdf has already been downloaded previously. Skipping...\n",
      "389. The file ns-08-2021.pdf has already been downloaded previously. Skipping...\n",
      "390. The file ns-09-2021.pdf has already been downloaded previously. Skipping...\n",
      "391. The file ns-10-2021.pdf has already been downloaded previously. Skipping...\n",
      "392. The file ns-11-2021.pdf has already been downloaded previously. Skipping...\n",
      "393. The file ns-12-2021.pdf has already been downloaded previously. Skipping...\n",
      "394. The file ns-13-2021.pdf has already been downloaded previously. Skipping...\n",
      "395. The file ns-14-2021.pdf has already been downloaded previously. Skipping...\n",
      "396. The file ns-15-2021.pdf has already been downloaded previously. Skipping...\n",
      "397. The file ns-16-2021.pdf has already been downloaded previously. Skipping...\n",
      "398. The file ns-17-2021.pdf has already been downloaded previously. Skipping...\n",
      "399. The file ns-18-2021.pdf has already been downloaded previously. Skipping...\n",
      "400. The file ns-19-2021.pdf has already been downloaded previously. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401. The file ns-20-2021.pdf has already been downloaded previously. Skipping...\n",
      "402. The file ns-21-2021.pdf has already been downloaded previously. Skipping...\n",
      "403. The file ns-22-2021.pdf has already been downloaded previously. Skipping...\n",
      "404. The file ns-23-2021.pdf has already been downloaded previously. Skipping...\n",
      "405. The file ns-24-2021.pdf has already been downloaded previously. Skipping...\n",
      "406. The file ns-25-2021.pdf has already been downloaded previously. Skipping...\n",
      "407. The file ns-26-2021.pdf has already been downloaded previously. Skipping...\n",
      "408. The file ns-27-2021.pdf has already been downloaded previously. Skipping...\n",
      "409. The file ns-28-2021.pdf has already been downloaded previously. Skipping...\n",
      "410. The file ns-29-2021.pdf has already been downloaded previously. Skipping...\n",
      "411. The file ns-30-2021.pdf has already been downloaded previously. Skipping...\n",
      "412. The file ns-31-2021.pdf has already been downloaded previously. Skipping...\n",
      "413. The file ns-32-2021.pdf has already been downloaded previously. Skipping...\n",
      "414. The file ns-33-2021.pdf has already been downloaded previously. Skipping...\n",
      "415. The file ns-34-2021.pdf has already been downloaded previously. Skipping...\n",
      "416. The file ns-35-2021.pdf has already been downloaded previously. Skipping...\n",
      "417. The file ns-36-2021.pdf has already been downloaded previously. Skipping...\n",
      "418. The file ns-37-2021.pdf has already been downloaded previously. Skipping...\n",
      "419. The file ns-38-2021.pdf has already been downloaded previously. Skipping...\n",
      "420. The file ns-39-2021.pdf has already been downloaded previously. Skipping...\n",
      "421. The file ns-40-2021.pdf has already been downloaded previously. Skipping...\n",
      "422. The file ns-41-2021.pdf has already been downloaded previously. Skipping...\n",
      "423. The file ns-42-2021.pdf has already been downloaded previously. Skipping...\n",
      "424. The file ns-43-2021.pdf has already been downloaded previously. Skipping...\n",
      "425. The file ns-44-2021.pdf has already been downloaded previously. Skipping...\n",
      "426. The file ns-45-2021.pdf has already been downloaded previously. Skipping...\n",
      "427. The file ns-46-2021.pdf has already been downloaded previously. Skipping...\n",
      "428. The file ns-01-2022.pdf has already been downloaded previously. Skipping...\n",
      "429. The file ns-02-2022.pdf has already been downloaded previously. Skipping...\n",
      "430. The file ns-03-2022.pdf has already been downloaded previously. Skipping...\n",
      "431. The file ns-04-2022.pdf has already been downloaded previously. Skipping...\n",
      "432. The file ns-05-2022.pdf has already been downloaded previously. Skipping...\n",
      "433. The file ns-06-2022.pdf has already been downloaded previously. Skipping...\n",
      "434. The file ns-07-2022.pdf has already been downloaded previously. Skipping...\n",
      "435. The file ns-08-2022.pdf has already been downloaded previously. Skipping...\n",
      "436. The file ns-09-2022.pdf has already been downloaded previously. Skipping...\n",
      "437. The file ns-10-2022.pdf has already been downloaded previously. Skipping...\n",
      "438. The file ns-11-2022.pdf has already been downloaded previously. Skipping...\n",
      "439. The file ns-12-2022.pdf has already been downloaded previously. Skipping...\n",
      "440. The file ns-13-2022.pdf has already been downloaded previously. Skipping...\n",
      "441. The file ns-14-2022.pdf has already been downloaded previously. Skipping...\n",
      "442. The file ns-15-2022.pdf has already been downloaded previously. Skipping...\n",
      "443. The file ns-16-2022.pdf has already been downloaded previously. Skipping...\n",
      "444. The file ns-17-2022.pdf has already been downloaded previously. Skipping...\n",
      "445. The file ns-18-2022.pdf has already been downloaded previously. Skipping...\n",
      "446. The file ns-19-2022.pdf has already been downloaded previously. Skipping...\n",
      "447. The file ns-20-2022.pdf has already been downloaded previously. Skipping...\n",
      "448. The file ns-21-2022.pdf has already been downloaded previously. Skipping...\n",
      "449. The file ns-22-2022.pdf has already been downloaded previously. Skipping...\n",
      "450. The file ns-23-2022.pdf has already been downloaded previously. Skipping...\n",
      "451. The file ns-24-2022.pdf has already been downloaded previously. Skipping...\n",
      "452. The file ns-25-2022.pdf has already been downloaded previously. Skipping...\n",
      "453. The file ns-26-2022.pdf has already been downloaded previously. Skipping...\n",
      "454. The file ns-27-2022.pdf has already been downloaded previously. Skipping...\n",
      "455. The file ns-28-2022.pdf has already been downloaded previously. Skipping...\n",
      "456. The file ns-29-2022.pdf has already been downloaded previously. Skipping...\n",
      "457. The file ns-30-2022.pdf has already been downloaded previously. Skipping...\n",
      "458. The file ns-31-2022.pdf has already been downloaded previously. Skipping...\n",
      "459. The file ns-32-2022.pdf has already been downloaded previously. Skipping...\n",
      "460. The file ns-33-2022.pdf has already been downloaded previously. Skipping...\n",
      "461. The file ns-34-2022.pdf has already been downloaded previously. Skipping...\n",
      "462. The file ns-35-2022.pdf has already been downloaded previously. Skipping...\n",
      "463. The file ns-36-2022.pdf has already been downloaded previously. Skipping...\n",
      "464. The file ns-37-2022.pdf has already been downloaded previously. Skipping...\n",
      "465. The file ns-38-2022.pdf has already been downloaded previously. Skipping...\n",
      "466. The file ns-39-2022.pdf has already been downloaded previously. Skipping...\n",
      "467. The file ns-40-2022.pdf has already been downloaded previously. Skipping...\n",
      "468. The file ns-41-2022.pdf has already been downloaded previously. Skipping...\n",
      "469. The file ns-42-2022.pdf has already been downloaded previously. Skipping...\n",
      "470. The file ns-43-2022.pdf has already been downloaded previously. Skipping...\n",
      "471. The file ns-44-2022.pdf has already been downloaded previously. Skipping...\n",
      "472. The file ns-01-2023.pdf has already been downloaded previously. Skipping...\n",
      "473. The file ns-02-2023.pdf has already been downloaded previously. Skipping...\n",
      "474. The file ns-03-2023.pdf has already been downloaded previously. Skipping...\n",
      "475. The file ns-04-2023.pdf has already been downloaded previously. Skipping...\n",
      "476. The file ns-05-2023.pdf has already been downloaded previously. Skipping...\n",
      "477. The file ns-06-2023.pdf has already been downloaded previously. Skipping...\n",
      "478. The file ns-07-2023.pdf has already been downloaded previously. Skipping...\n",
      "479. The file ns-08-2023.pdf has already been downloaded previously. Skipping...\n",
      "480. The file ns-09-2023.pdf has already been downloaded previously. Skipping...\n",
      "481. The file ns-10-2023.pdf has already been downloaded previously. Skipping...\n",
      "482. The file ns-11-2023.pdf has already been downloaded previously. Skipping...\n",
      "483. The file ns-12-2023.pdf has already been downloaded previously. Skipping...\n",
      "484. The file ns-13-2023.pdf has already been downloaded previously. Skipping...\n",
      "485. The file ns-14-2023.pdf has already been downloaded previously. Skipping...\n",
      "486. The file ns-15-2023.pdf has already been downloaded previously. Skipping...\n",
      "487. The file ns-16-2023.pdf has already been downloaded previously. Skipping...\n",
      "488. The file ns-17-2023.pdf has already been downloaded previously. Skipping...\n",
      "489. The file ns-18-2023.pdf has already been downloaded previously. Skipping...\n",
      "490. The file ns-19-2023.pdf has already been downloaded previously. Skipping...\n",
      "491. The file ns-20-2023.pdf has already been downloaded previously. Skipping...\n",
      "492. The file ns-21-2023.pdf has already been downloaded previously. Skipping...\n",
      "493. The file ns-22-2023.pdf has already been downloaded previously. Skipping...\n",
      "494. The file ns-23-2023.pdf has already been downloaded previously. Skipping...\n",
      "495. The file ns-24-2023.pdf has already been downloaded previously. Skipping...\n",
      "496. The file ns-25-2023.pdf has already been downloaded previously. Skipping...\n",
      "497. The file ns-26-2023.pdf has already been downloaded previously. Skipping...\n",
      "498. The file ns-27-2023.pdf has already been downloaded previously. Skipping...\n",
      "499. The file ns-28-2023.pdf has already been downloaded previously. Skipping...\n",
      "500. The file ns-29-2023.pdf has already been downloaded previously. Skipping...\n",
      "501. The file ns-30-2023.pdf has already been downloaded previously. Skipping...\n",
      "502. The file ns-31-2023.pdf has already been downloaded previously. Skipping...\n",
      "503. The file ns-32-2023.pdf has already been downloaded previously. Skipping...\n",
      "504. The file ns-33-2023.pdf has already been downloaded previously. Skipping...\n",
      "505. The file ns-34-2023.pdf has already been downloaded previously. Skipping...\n",
      "506. The file ns-35-2023.pdf has already been downloaded previously. Skipping...\n",
      "507. The file ns-36-2023.pdf has already been downloaded previously. Skipping...\n",
      "508. The file ns-37-2023.pdf has already been downloaded previously. Skipping...\n",
      "509. The file ns-38-2023.pdf has already been downloaded previously. Skipping...\n",
      "510. The file ns-39-2023.pdf has already been downloaded previously. Skipping...\n",
      "511. The file ns-40-2023.pdf has already been downloaded previously. Skipping...\n",
      "512. The file ns-41-2023.pdf has already been downloaded previously. Skipping...\n",
      "513. The file ns-42-2023.pdf has already been downloaded previously. Skipping...\n",
      "514. The file ns-43-2023.pdf has already been downloaded previously. Skipping...\n",
      "515. The file ns-01-2024.pdf has already been downloaded previously. Skipping...\n",
      "516. The file ns-02-2024.pdf has already been downloaded previously. Skipping...\n",
      "517. The file ns-03-2024.pdf has already been downloaded previously. Skipping...\n",
      "518. The file ns-04-2024.pdf has already been downloaded previously. Skipping...\n",
      "519. The file ns-05-2024.pdf has already been downloaded previously. Skipping...\n",
      "520. The file ns-06-2024.pdf has already been downloaded previously. Skipping...\n",
      "521. The file ns-07-2024.pdf has already been downloaded previously. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522. The file ns-08-2024.pdf has already been downloaded previously. Skipping...\n",
      "523. The file ns-09-2024.pdf has already been downloaded previously. Skipping...\n",
      "524. The file ns-10-2024.pdf has already been downloaded previously. Skipping...\n",
      "525. The file ns-11-2024.pdf has already been downloaded previously. Skipping...\n",
      "526. The file ns-12-2024.pdf has already been downloaded previously. Skipping...\n",
      "527. The file ns-13-2024.pdf has already been downloaded previously. Skipping...\n",
      "528. The file ns-14-2024.pdf has already been downloaded previously. Skipping...\n",
      "529. The file ns-15-2024.pdf has already been downloaded previously. Skipping...\n",
      "530. The file ns-16-2024.pdf has already been downloaded previously. Skipping...\n",
      "531. The file ns-17-2024.pdf has already been downloaded previously. Skipping...\n",
      "532. The file ns-18-2024.pdf has already been downloaded previously. Skipping...\n",
      "533. The file ns-19-2024.pdf has already been downloaded previously. Skipping...\n",
      "534. The file ns-20-2024.pdf has already been downloaded previously. Skipping...\n",
      "535. The file ns-21-2024.pdf has already been downloaded previously. Skipping...\n",
      "536. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2024/ns-22-2024.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-22-2024.pdf\n",
      "Batch 536 of 1 completed\n",
      "Do you want to continue downloading? (Enter 'y' to continue, any other key to stop): y\n",
      "Waiting randomly for 5.26 seconds\n",
      "537. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2024/ns-23-2024.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-23-2024.pdf\n",
      "Batch 537 of 1 completed\n",
      "Waiting randomly for 6.51 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize pygame\n",
    "pygame.mixer.init()\n",
    "\n",
    "# List of available sound files\n",
    "available_sounds = os.listdir(sound_folder)\n",
    "\n",
    "# Select a random sound\n",
    "random_sound = random.choice(available_sounds)\n",
    "\n",
    "# Full path of the random sound\n",
    "sound_path = os.path.join(sound_folder, random_sound)\n",
    "\n",
    "# Load the selected sound\n",
    "pygame.mixer.music.load(sound_path)\n",
    "\n",
    "# List to keep track of successfully downloaded files\n",
    "downloaded_files = []\n",
    "\n",
    "# Load the list of previously downloaded files if it exists\n",
    "if os.path.exists(os.path.join(download_record, \"downloaded_files.txt\")):\n",
    "    with open(os.path.join(download_record, \"downloaded_files.txt\"), \"r\") as f:\n",
    "        downloaded_files = f.read().splitlines()\n",
    "\n",
    "# Web driver setup\n",
    "driver_path = os.environ.get('driver_path')\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "# Number of downloads per batch\n",
    "downloads_per_batch = 5\n",
    "# Total number of downloads\n",
    "total_downloads = 5\n",
    "\n",
    "try:\n",
    "    # Open the test page\n",
    "    driver.get(bcrp_url)\n",
    "    print(\"Site opened successfully\")\n",
    "\n",
    "    # Wait for the container area to be present\n",
    "    wait = WebDriverWait(driver, 60)\n",
    "    container_area = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"rightside\"]')))\n",
    "\n",
    "    # Get all the links within the container area\n",
    "    pdf_links = container_area.find_elements(By.XPATH, './/a')\n",
    "\n",
    "    # Reverse the order of links\n",
    "    pdf_links = list(reversed(pdf_links))\n",
    "\n",
    "    # Initialize download counter\n",
    "    download_counter = 0\n",
    "\n",
    "    # Iterate over reversed links and download PDFs in batches\n",
    "    for pdf_link in pdf_links:\n",
    "        download_counter += 1\n",
    "\n",
    "        # Get the file name from the URL\n",
    "        new_url = pdf_link.get_attribute(\"href\")\n",
    "        file_name = new_url.split(\"/\")[-1]\n",
    "\n",
    "        # Check if the file has already been downloaded\n",
    "        if file_name in downloaded_files:\n",
    "            print(f\"{download_counter}. The file {file_name} has already been downloaded previously. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Try to download the file\n",
    "        try:\n",
    "            download_pdf(driver, pdf_link, wait, download_counter, raw_pdf, download_record)\n",
    "\n",
    "            # Update the list of downloaded files\n",
    "            downloaded_files.append(file_name)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading the file {file_name}: {str(e)}\")\n",
    "\n",
    "        # If the download count reaches a multiple of batch size, notify\n",
    "        if download_counter % downloads_per_batch == 0:\n",
    "            print(f\"Batch {download_counter // downloads_per_batch} of {downloads_per_batch} completed\")\n",
    "\n",
    "        # If the download count reaches a multiple of 25, ask the user if they want to continue\n",
    "        if download_counter % 5 == 0: # after the fifth PDF downloaded, you'll listen a beautiful song\n",
    "            play_sound()\n",
    "            user_input = input(\"Do you want to continue downloading? (Enter 'y' to continue, any other key to stop): \")\n",
    "            pygame.mixer.music.stop()\n",
    "            if user_input.lower() != 'y':\n",
    "                break\n",
    "\n",
    "        # Random wait before the next iteration\n",
    "        random_wait(5, 10)\n",
    "\n",
    "        # If total downloads reached, break out of loop\n",
    "        if download_counter == total_downloads:\n",
    "            print(f\"All downloads completed ({total_downloads} in total)\")\n",
    "            break\n",
    "\n",
    "except StaleElementReferenceException:\n",
    "    print(\"StaleElementReferenceException occurred. Retrying...\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser when finished\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168995d",
   "metadata": {},
   "source": [
    "### Ordenando pdf por años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee016a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Obtener la lista de archivos en el directorio\n",
    "archivos = os.listdir(raw_pdf)\n",
    "\n",
    "# Iterar sobre cada archivo\n",
    "for archivo in archivos:\n",
    "    # Obtener el año del nombre del archivo\n",
    "    nombre, extension = os.path.splitext(archivo)\n",
    "    año = None\n",
    "    partes_nombre = nombre.split('-')\n",
    "    for parte in partes_nombre:\n",
    "        if parte.isdigit() and len(parte) == 4:\n",
    "            año = parte\n",
    "            break\n",
    "\n",
    "    # Si se encontró el año, mover el archivo a la carpeta correspondiente\n",
    "    if año:\n",
    "        carpeta_destino = os.path.join(raw_pdf, año)\n",
    "        # Crear la carpeta si no existe\n",
    "        if not os.path.exists(carpeta_destino):\n",
    "            os.makedirs(carpeta_destino)\n",
    "        # Mover el archivo a la carpeta destino\n",
    "        shutil.move(os.path.join(raw_pdf, archivo), carpeta_destino)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d24d2",
   "metadata": {},
   "source": [
    "# Recortando PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd2370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import tkinter as tk\n",
    "\n",
    "# Rutas de directorios\n",
    "\n",
    "input_pdf_record_txt = 'input_pdf_record.txt'\n",
    "\n",
    "class PopupWindow(tk.Toplevel):\n",
    "    def __init__(self, root, message):\n",
    "        super().__init__(root)\n",
    "        self.root = root\n",
    "        self.title(\"Atención!\")\n",
    "        self.message = message\n",
    "        self.result = None\n",
    "        self.configure_window()\n",
    "        self.create_widgets()\n",
    "\n",
    "    def configure_window(self):\n",
    "        self.resizable(False, False)  # Evita cambiar el tamaño de la ventana\n",
    "\n",
    "    def create_widgets(self):\n",
    "        self.label = tk.Label(self, text=self.message, wraplength=250)  # Ajusta el texto si es demasiado largo\n",
    "        self.label.pack(pady=10, padx=10)\n",
    "        self.btn_frame = tk.Frame(self)\n",
    "        self.btn_frame.pack(pady=5)\n",
    "        self.btn_yes = tk.Button(self.btn_frame, text=\"Sí\", command=self.yes)\n",
    "        self.btn_yes.pack(side=tk.LEFT, padx=5)\n",
    "        self.btn_no = tk.Button(self.btn_frame, text=\"No\", command=self.no)\n",
    "        self.btn_no.pack(side=tk.RIGHT, padx=5)\n",
    "\n",
    "        # Calcula el tamaño de la ventana en función del tamaño del texto\n",
    "        width = self.label.winfo_reqwidth() + 20\n",
    "        height = self.label.winfo_reqheight() + 100\n",
    "        self.geometry(f\"{width}x{height}\")\n",
    "\n",
    "    def yes(self):\n",
    "        self.result = True\n",
    "        self.destroy()\n",
    "\n",
    "    def no(self):\n",
    "        self.result = False\n",
    "        self.destroy()\n",
    "\n",
    "def search_keywords(pdf_file, keywords):\n",
    "    pages_with_keywords = []\n",
    "    with fitz.open(pdf_file) as doc:\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc.load_page(page_num)\n",
    "            text = page.get_text()\n",
    "            if any(keyword in text for keyword in keywords):\n",
    "                pages_with_keywords.append(page_num)\n",
    "    return pages_with_keywords\n",
    "\n",
    "def trim_pdf(pdf_file, pages):\n",
    "    if not pages:\n",
    "        print(f\"No se encontraron páginas con palabras clave en {pdf_file}\")\n",
    "        return 0\n",
    "    \n",
    "    new_pdf_file = os.path.join(input_pdf, os.path.basename(pdf_file))\n",
    "    \n",
    "    with fitz.open(pdf_file) as doc:\n",
    "        new_doc = fitz.open()\n",
    "        new_doc.insert_pdf(doc, from_page=0, to_page=0)\n",
    "        for page_num in pages:\n",
    "            new_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)\n",
    "        new_doc.save(new_pdf_file)\n",
    "    \n",
    "    num_pages_new_pdf = new_doc.page_count\n",
    "    print(f\"El PDF recortado '{new_pdf_file}' tiene {num_pages_new_pdf} páginas.\")\n",
    "\n",
    "    if num_pages_new_pdf == 5:\n",
    "        final_doc = fitz.open()\n",
    "        final_doc.insert_pdf(new_doc, from_page=0, to_page=0)\n",
    "        final_doc.insert_pdf(new_doc, from_page=1, to_page=1)\n",
    "        final_doc.insert_pdf(new_doc, from_page=3, to_page=3)\n",
    "        final_doc.save(new_pdf_file)\n",
    "\n",
    "        num_pages_new_pdf = final_doc.page_count\n",
    "        print(f\"Solo se conservaron la portada y las páginas con 2 tablas de interés en el PDF recortado '{new_pdf_file}'.\")\n",
    "    else:\n",
    "        print(f\"Se conservaron todas las páginas en el PDF recortado '{new_pdf_file}'.\")\n",
    "\n",
    "    return num_pages_new_pdf\n",
    "\n",
    "def read_input_pdf_files():\n",
    "    input_pdf_files_path = os.path.join(input_pdf_record, input_pdf_record_txt)\n",
    "    if not os.path.exists(input_pdf_files_path):\n",
    "        return set()\n",
    "    \n",
    "    with open(input_pdf_files_path, 'r') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "def write_input_pdf_files(input_pdf_files):\n",
    "    input_pdf_files_path = os.path.join(input_pdf_record, input_pdf_record_txt)\n",
    "    sorted_filenames = sorted(input_pdf_files)  # Sort the filenames\n",
    "    with open(input_pdf_files_path, 'w') as file:\n",
    "        for filename in sorted_filenames:\n",
    "            file.write(filename + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = [\"ECONOMIC SECTORS\"]\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Oculta la ventana principal de Tkinter\n",
    "\n",
    "    input_pdf_files = read_input_pdf_files()\n",
    "    processing_counter = 1\n",
    "\n",
    "    for folder in os.listdir(raw_pdf):\n",
    "        folder_path = os.path.join(raw_pdf, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(\"Procesando carpeta:\", folder)\n",
    "            num_pdfs_trimmed = 0\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".pdf\"):\n",
    "                    pdf_file = os.path.join(folder_path, filename)\n",
    "                    if filename in input_pdf_files:\n",
    "                        print(f\"{processing_counter}. El PDF '{filename}' ya ha sido recortado y guardado en '{input_pdf}'...\")\n",
    "                        processing_counter += 1\n",
    "                        continue\n",
    "                    print(f\"{processing_counter}. Procesando:\", pdf_file)\n",
    "                    \n",
    "                    pages_with_keywords = search_keywords(pdf_file, keywords)\n",
    "                    num_pages_new_pdf = trim_pdf(pdf_file, pages_with_keywords)\n",
    "                    if num_pages_new_pdf > 0:\n",
    "                        num_pdfs_trimmed += 1\n",
    "                        input_pdf_files.add(filename)\n",
    "                        processing_counter += 1\n",
    "            \n",
    "            write_input_pdf_files(input_pdf_files)\n",
    "\n",
    "            message = f\"{num_pdfs_trimmed} PDFs han sido recortados en la carpeta {folder}. ¿Desea continuar?\"\n",
    "            popup = PopupWindow(root, message)\n",
    "            root.wait_window(popup)\n",
    "            if not popup.result:\n",
    "                break\n",
    "                \n",
    "    print(\"Proceso completado para todos los PDFs en el directorio:\", input_pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643079a",
   "metadata": {},
   "source": [
    "### Ordenando pdf por años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Obtener la lista de archivos en el directorio\n",
    "archivos = os.listdir(input_pdf)\n",
    "\n",
    "# Iterar sobre cada archivo\n",
    "for archivo in archivos:\n",
    "    # Obtener el año del nombre del archivo\n",
    "    nombre, extension = os.path.splitext(archivo)\n",
    "    año = None\n",
    "    partes_nombre = nombre.split('-')\n",
    "    for parte in partes_nombre:\n",
    "        if parte.isdigit() and len(parte) == 4:\n",
    "            año = parte\n",
    "            break\n",
    "\n",
    "    # Si se encontró el año, mover el archivo a la carpeta correspondiente\n",
    "    if año:\n",
    "        carpeta_destino = os.path.join(input_pdf, año)\n",
    "        # Crear la carpeta si no existe\n",
    "        if not os.path.exists(carpeta_destino):\n",
    "            os.makedirs(carpeta_destino)\n",
    "        # Mover el archivo a la carpeta destino\n",
    "        shutil.move(os.path.join(input_pdf, archivo), carpeta_destino)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045beb7b",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a384d2",
   "metadata": {},
   "source": [
    "<div id=\"2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e1e92",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;; color: dark;\">2.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Data cleaning</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5a767",
   "metadata": {},
   "source": [
    "<div id=\"2-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357318e8",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">2.2.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Extracting tables and generating dataframes (includes data cleanup)\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100857d4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    We would like to get specific tables: information on GDP growth rates with annual, quarterly and monthly frequency. We don't need other tables also related to GDP that don't meet these requirements. Extraction will be easier if we use keywords.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77729e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to search in the page text\n",
    "keywords = [\"PRODUCTO BRUTO INTERNO\", \"SECTORES ECONÓMICOS\", \"PBI\", \"GDP\", \"Variaciones\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070fb47",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    The code iterates through each PDF and extracts the two required tables from each. The extracted information is then transformed into dataframes and the columns and values are cleaned up to conform to Python conventions (pythonic).\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af4d90",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df05629",
   "metadata": {},
   "source": [
    "### Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4190e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_characters_first_row(texto):\n",
    "    texto = re.sub(r'\\s*-\\s*', '-', texto)  # Remueve espacios alrededor de guiones\n",
    "    texto = re.sub(r'[^a-zA-Z0-9\\s-]', '', texto)  # Remueve caracteres raros excepto letras, dígitos y guiones\n",
    "    return texto\n",
    "\n",
    "def remove_rare_characters(texto):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', texto)\n",
    "\n",
    "def remove_tildes(texto):\n",
    "    return ''.join((c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8eb2f",
   "metadata": {},
   "source": [
    "### Común"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.\n",
    "def drop_nan_rows(df):\n",
    "    df = df.dropna(how='all')\n",
    "    return df\n",
    "\n",
    "# 1. \n",
    "def drop_nan_columns(df):\n",
    "    return df.dropna(axis=1, how='all')\n",
    "\n",
    "# 2.\n",
    "def swap_first_second_row(df):\n",
    "    temp = df.iloc[0, 0]\n",
    "    df.iloc[0, 0] = df.iloc[1, 0]\n",
    "    df.iloc[1, 0] = temp\n",
    "\n",
    "    temp = df.iloc[0, -1]\n",
    "    df.iloc[0, -1] = df.iloc[1, -1]\n",
    "    df.iloc[1, -1] = temp\n",
    "    return df\n",
    "\n",
    "# 8. \n",
    "def reset_index(df):\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "# 5.\n",
    "def remove_digit_slash(df):\n",
    "    # Aplica la función de reemplazo a la primera columna y a las dos últimas columnas\n",
    "    df.iloc[:, [0, -2, -1]] = df.iloc[:, [0, -2, -1]].apply(lambda x: x.str.replace(r'\\d+/', '', regex=True))\n",
    "    return df\n",
    "\n",
    "# 9. AUX (ROBUSTO)\n",
    "\n",
    "def separate_text_digits(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if any(char.isdigit() for char in str(row.iloc[-2])) and any(char.isalpha() for char in str(row.iloc[-2])):\n",
    "            if pd.isnull(row.iloc[-1]):\n",
    "                df.loc[index, df.columns[-1]] = ''.join(filter(lambda x: x.isalpha() or x == ' ', str(row.iloc[-2])))\n",
    "                df.loc[index, df.columns[-2]] = ''.join(filter(lambda x: not (x.isalpha() or x == ' '), str(row.iloc[-2])))\n",
    "            \n",
    "            # Check if comma or dot is used as decimal separator\n",
    "            if ',' in str(row.iloc[-2]):\n",
    "                split_values = str(row.iloc[-2]).split(',')\n",
    "            elif '.' in str(row.iloc[-2]):\n",
    "                split_values = str(row.iloc[-2]).split('.')\n",
    "            else:\n",
    "                # If neither comma nor dot found, assume no decimal part\n",
    "                split_values = [str(row.iloc[-2]), '']\n",
    "                \n",
    "            cleaned_integer = ''.join(filter(lambda x: x.isdigit() or x == '-', split_values[0]))\n",
    "            cleaned_decimal = ''.join(filter(lambda x: x.isdigit(), split_values[1]))\n",
    "            if cleaned_decimal:\n",
    "                # Use comma as decimal separator\n",
    "                cleaned_numeric = cleaned_integer + ',' + cleaned_decimal\n",
    "            else:\n",
    "                cleaned_numeric = cleaned_integer\n",
    "            df.loc[index, df.columns[-2]] = cleaned_numeric\n",
    "    return df\n",
    "\n",
    "\n",
    "# 4. \n",
    "def extract_years(df):\n",
    "    year_columns = [col for col in df.columns if re.match(r'\\b\\d{4}\\b', col)]\n",
    "    #print(\"Años (4 dígitos) extraídos:\")\n",
    "    #print(year_columns)\n",
    "    return year_columns\n",
    "\n",
    "# 6. \n",
    "def first_row_columns(df):\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.drop(df.index[0])\n",
    "    return df\n",
    "\n",
    "# 15.\n",
    "def clean_columns_values(df):\n",
    "    df.columns = df.columns.str.lower()\n",
    "    # Only normalize string column names\n",
    "    df.columns = [unicodedata.normalize('NFKD', col).encode('ASCII', 'ignore').decode('utf-8') if isinstance(col, str) else col for col in df.columns]\n",
    "    df.columns = df.columns.str.replace(' ', '_').str.replace('ano', 'year').str.replace('-', '_')\n",
    "    \n",
    "    text_columns = df.select_dtypes(include='object').columns\n",
    "    for col in df.columns:\n",
    "        df.loc[:, col] = df[col].apply(lambda x: remove_tildes(x) if isinstance(x, str) else x)\n",
    "        df.loc[:, col] = df[col].apply(lambda x: str(x).replace(',', '.') if isinstance(x, (int, float, str)) else x)\n",
    "    df.loc[:, 'sectores_economicos'] = df['sectores_economicos'].str.lower()\n",
    "    df.loc[:, 'economic_sectors'] = df['economic_sectors'].str.lower()\n",
    "    df.loc[:, 'sectores_economicos'] = df['sectores_economicos'].apply(remove_rare_characters)\n",
    "    df.loc[:, 'economic_sectors'] = df['economic_sectors'].apply(remove_rare_characters)\n",
    "    return df\n",
    "\n",
    "# 16.\n",
    "def convertir_float(df):\n",
    "    excluded_columns = ['sectores_economicos', 'economic_sectors']\n",
    "    columns_to_convert = [col for col in df.columns if col not in excluded_columns]\n",
    "    df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    return df\n",
    "\n",
    "# 15.\n",
    "def relocate_last_column(df):\n",
    "    last_column = df.pop(df.columns[-1])\n",
    "    df.insert(1, last_column.name, last_column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed8745",
   "metadata": {},
   "source": [
    "### Exclusiva Tabla 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATIPIC LAST COLUMNS\n",
    "def relocate_last_columns(df):\n",
    "    if not pd.isna(df.iloc[1, -1]):\n",
    "        # Create a new column with NaN\n",
    "        new_column = 'col_' + ''.join(map(str, np.random.randint(1, 5, size=1)))\n",
    "        df[new_column] = np.nan\n",
    "        \n",
    "        # Get 'ECONOMIC SECTORS' and relocate\n",
    "        insert_value_1 = df.iloc[0, -2]\n",
    "        # Convert the value to string before assignment\n",
    "        insert_value_1 = str(insert_value_1)\n",
    "        # Ensure the dtype of the last column is object (string) to accommodate string values\n",
    "        df.iloc[:, -1] = df.iloc[:, -1].astype('object')\n",
    "        df.iloc[0, -1] = insert_value_1\n",
    "        \n",
    "        # NaN first obs\n",
    "        df.iloc[0,-2] = np.nan\n",
    "    return df\n",
    "\n",
    "# Extraer meses\n",
    "\n",
    "def get_months_sublist_list(df, year_columns):\n",
    "    first_row = df.iloc[0]\n",
    "    # Initialize the list of sublists\n",
    "    months_sublist_list = []\n",
    "\n",
    "    # Initialize the current sublist\n",
    "    months_sublist = []\n",
    "\n",
    "    # Iterate over the elements of the first row\n",
    "    for item in first_row:\n",
    "        # Check if the item meets the requirements\n",
    "        if len(str(item)) == 3:\n",
    "            months_sublist.append(item)\n",
    "        elif '-' in item or str(item) == 'year':\n",
    "            months_sublist.append(item)\n",
    "            months_sublist_list.append(months_sublist)\n",
    "            months_sublist = []\n",
    "\n",
    "    # Add the last sublist if it's not empty\n",
    "    if months_sublist:\n",
    "        months_sublist_list.append(months_sublist)\n",
    "\n",
    "    new_elements = []\n",
    "\n",
    "    # Check if year_columns is not empty\n",
    "    if year_columns:\n",
    "        for i, year in enumerate(year_columns):\n",
    "            # Check if index i is valid for quarters_sublist_list\n",
    "            if i < len(months_sublist_list):\n",
    "                for element in months_sublist_list[i]:\n",
    "                    new_elements.append(f\"{year}_{element}\")\n",
    "                    \n",
    "    two_first_elements = df.iloc[0][:2].tolist()\n",
    "\n",
    "    # Ensure that the two_first_elements are added if they are not in new_elements\n",
    "    for index in range(len(two_first_elements) - 1, -1, -1):\n",
    "        if two_first_elements[index] not in new_elements:\n",
    "            new_elements.insert(0, two_first_elements[index])\n",
    "\n",
    "    # Ensure that the length of new_elements matches the number of columns in df\n",
    "    while len(new_elements) < len(df.columns):\n",
    "        new_elements.append(None)\n",
    "\n",
    "    temp_df = pd.DataFrame([new_elements], columns=df.columns)\n",
    "    df.iloc[0] = temp_df.iloc[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_year_column(df):\n",
    "    # List to store the found years\n",
    "    found_years = []\n",
    "\n",
    "    # Iterating over the column names of the DataFrame\n",
    "    for column in df.columns:\n",
    "        # Checking if the column name is a year (4 digits)\n",
    "        if column.isdigit() and len(column) == 4:\n",
    "            found_years.append(column)\n",
    "\n",
    "    # If more than one year is found, do nothing\n",
    "    if len(found_years) > 1:\n",
    "        pass\n",
    "    # If exactly one year is found, implement additional code\n",
    "    elif len(found_years) == 1:\n",
    "        # Getting the name of the found year\n",
    "        year_name = found_years[0]\n",
    "        print(\"The name of the column representing the year is:\", year_name)\n",
    "\n",
    "        # Getting the first row of the DataFrame\n",
    "        first_row = df.iloc[0]\n",
    "\n",
    "        # Searching for the first column containing the word \"year\" or some hyphen-separated expression\n",
    "        column_contains_year = first_row[first_row.astype(str).str.contains(r'\\byear\\b')]\n",
    "\n",
    "        if not column_contains_year.empty:\n",
    "            # Getting the name of the first column containing 'year' or some hyphen-separated expression in the first row\n",
    "            column_contains_year_name = column_contains_year.index[0]\n",
    "            print(\"The name of the first column containing 'year' or some hyphen-separated expression in the first row is:\", column_contains_year_name)\n",
    "\n",
    "            # Getting the indices of the columns\n",
    "            column_contains_year_index = df.columns.get_loc(column_contains_year_name)\n",
    "            year_name_index = df.columns.get_loc(year_name)\n",
    "            print(\"The index of the column containing 'year' is:\", column_contains_year_index)\n",
    "            print(\"The index of the column representing the year is:\", year_name_index)\n",
    "\n",
    "            # Checking if the column representing the year is to the right or to the left of column_contains_year\n",
    "            if column_contains_year_index < year_name_index:\n",
    "                print(\"The year column is to the right of the column containing 'year'.\")\n",
    "                # Adding one to the year\n",
    "                new_year = str(int(year_name) - 1)\n",
    "                # Renaming the column containing 'year' with the new year\n",
    "                df.rename(columns={column_contains_year_name: new_year}, inplace=True)\n",
    "                print(f\"The column containing 'year' is now named '{new_year}'.\")\n",
    "            elif column_contains_year_index > year_name_index:\n",
    "                print(\"The year column is to the left of the column containing 'year'.\")\n",
    "                # Subtracting one from the year\n",
    "                new_year = str(int(year_name) + 1)\n",
    "                # Renaming the year column with the new year\n",
    "                df.rename(columns={column_contains_year_name: new_year}, inplace=True)\n",
    "                print(f\"The column containing 'year' is now named '{new_year}'.\")\n",
    "            else:\n",
    "                print(\"The year column is in the same position as the column containing 'year'.\")\n",
    "        else:\n",
    "            print(\"No columns containing 'year' were found in the first row.\")\n",
    "    # If no year is found, print a message\n",
    "    else:\n",
    "        print(\"No years were found in the column names.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "def clean_first_row(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            if isinstance(df.at[0, col], str):\n",
    "                df.at[0, col] = df.at[0, col].lower()  # Convertir a minúsculas solo si es un objeto\n",
    "                df.at[0, col] = remove_tildes(df.at[0, col])\n",
    "                df.at[0, col] = remove_rare_characters_first_row(df.at[0, col])\n",
    "                # Reemplazar 'ano' por 'year'\n",
    "                df.at[0, col] = df.at[0, col].replace('ano', 'year')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def intercambiar_valores(df):\n",
    "    # Verificar si hay al menos dos columnas en el DataFrame\n",
    "    if len(df.columns) < 2:\n",
    "        print(\"El DataFrame tiene menos de dos columnas. No se pueden intercambiar valores.\")\n",
    "        return df\n",
    "\n",
    "    # Verificar si hay valores NaN en la última columna\n",
    "    if df.iloc[:, -1].isnull().any():\n",
    "        # Obtener índice de filas con NaN en la última columna\n",
    "        last_column_rows_nan = df[df.iloc[:, -1].isnull()].index\n",
    "\n",
    "        # Iterar sobre las filas con NaN en la última columna\n",
    "        for idx in last_column_rows_nan:\n",
    "            # Verificar si el índice está dentro del rango de las columnas\n",
    "            if -2 >= -len(df.columns):\n",
    "                # Intercambiar los valores de la última columna y la penúltima columna\n",
    "                df.iloc[idx, -1], df.iloc[idx, -2] = df.iloc[idx, -2], df.iloc[idx, -1]\n",
    "            else:\n",
    "                print(f\"Índice fuera de rango para la fila {idx}. No se pueden intercambiar valores.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def replace_var_perc_first_column(df):\n",
    "    # Regular expression to search for \"Var. %\" or \"Var.%\"\n",
    "    regex = re.compile(r'Var\\. ?%')\n",
    "\n",
    "    # Iterate over the rows of the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # Convert the value in the first column to a string\n",
    "        value = str(row.iloc[0])\n",
    "\n",
    "        # Check if the value matches the regular expression\n",
    "        if regex.search(value):\n",
    "            # Replace only the characters that match the regular expression\n",
    "            df.at[index, df.columns[0]] = regex.sub(\"variacion porcentual\", value)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# 8.\n",
    "number_moving_average = 'three' # Keep a space at the end\n",
    "\n",
    "def replace_number_moving_average(df):\n",
    "    for index, row in df.iterrows():\n",
    "        # Buscar la expresión regular en la penúltima o última columna\n",
    "        if pd.notnull(row.iloc[-1]) and re.search(r'(\\d\\s*-)', str(row.iloc[-1])):\n",
    "            df.at[index, df.columns[-1]] = re.sub(r'(\\d\\s*-)', f'{number_moving_average}-', str(row.iloc[-1]))\n",
    "        #elif pd.notnull(row.iloc[-2]) and re.search(r'(\\d\\s*-)', str(row.iloc[-2])):\n",
    "        #   df.at[index, df.columns[-2]] = re.sub(r'(\\d\\s*-)', f'{number_moving_average}-', str(row.iloc[-2]))\n",
    "    return df\n",
    "\n",
    "\n",
    "# 7.\n",
    "def replace_var_perc_last_columns(df):\n",
    "    # Expresión regular para buscar \"Var. %\" o \"Var.%\"\n",
    "    regex = re.compile(r'(Var\\. ?%)(.*)')\n",
    "\n",
    "    # Iterar sobre las filas del dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # Verificar si el valor en la penúltima columna es una cadena no nula\n",
    "        if isinstance(row.iloc[-2], str) and regex.search(row.iloc[-2]):\n",
    "            # Realizar el reemplazo al final del valor de la penúltima columna\n",
    "            replaced_text = regex.sub(r'\\2 percent change', row.iloc[-2])\n",
    "            df.at[index, df.columns[-2]] = replaced_text.strip()\n",
    "        \n",
    "        # Verificar si el valor en la última columna es una cadena no nula\n",
    "        if isinstance(row.iloc[-1], str) and regex.search(row.iloc[-1]):\n",
    "            # Realizar el reemplazo al final del valor de la última columna\n",
    "            replaced_text = regex.sub(r'\\2 percent change', row.iloc[-1])\n",
    "            df.at[index, df.columns[-1]] = replaced_text.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Función para buscar y reemplazar en la segunda fila del DataFrame\n",
    "def replace_first_dot(df):\n",
    "    second_row = df.iloc[1]  # Segunda fila del DataFrame\n",
    "    \n",
    "    # Verificar si al menos una observación cumple con el patrón\n",
    "    if any(isinstance(cell, str) and re.match(r'^\\w+\\.\\s?\\w+', cell) for cell in second_row):\n",
    "        for col in df.columns:\n",
    "            if isinstance(second_row[col], str):  # Verificar si el valor es una cadena\n",
    "                if re.match(r'^\\w+\\.\\s?\\w+', second_row[col]):  # Verificar si cumple con el patrón Xxx.Xxx o Xxx. Xxx.\n",
    "                    df.at[1, col] = re.sub(r'(\\w+)\\.(\\s?\\w+)', r'\\1-\\2', second_row[col], count=1)  # Reemplazar solo el primer punto\n",
    "    return df\n",
    "\n",
    "def drop_rare_caracter_row(df):\n",
    "    # Buscar el caracter solitario \"}\" en cada fila y obtener un booleano para cada fila\n",
    "    rare_caracter_row = df.apply(lambda row: '}' in row.values, axis=1)\n",
    "    \n",
    "    # Filtrar el DataFrame para eliminar las filas con el caracter solitario \"}\"\n",
    "    df = df[~rare_caracter_row]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def split_column_by_pattern(df):\n",
    "    # Iteramos sobre las columnas del dataframe\n",
    "    for col in df.columns:\n",
    "        # Verificamos si la segunda fila de la columna contiene el patrón\n",
    "        if re.match(r'^[A-Z][a-z]+\\.?\\s[A-Z][a-z]+\\.?$', str(df.iloc[1][col])):\n",
    "            # Realizamos el split de la columna usando como criterio el espacio\n",
    "            split_values = df[col].str.split(expand=True)\n",
    "            # Guardamos los primeros valores en la columna original\n",
    "            df[col] = split_values[0]\n",
    "            # Guardamos los segundos valores en una nueva columna con el sufijo \"_split\"\n",
    "            new_col_name = col + '_split'\n",
    "            df.insert(df.columns.get_loc(col) + 1, new_col_name, split_values[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c629c56",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2014\\_07}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728de5fd",
   "metadata": {},
   "source": [
    "se: sectores económicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2111c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_nan_se(df):\n",
    "    # Check if the first observation of the first column is NaN\n",
    "    if pd.isna(df.iloc[0, 0]) and df.iloc[0, 1] == \"SECTORES ECONÓMICOS\":\n",
    "        # Create a temporary copy of the values\n",
    "        column_1_value = df.iloc[0, 1]\n",
    "        # Swap values in the original row\n",
    "        df.iloc[0, 0] = column_1_value\n",
    "        df.iloc[0, 1] = np.nan\n",
    "        # Drop the second column\n",
    "        df = df.drop(df.columns[1], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12022c55",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2014\\_08}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9254dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_first_row_with_columns(df):\n",
    "    # Check if the first row contains at least one year\n",
    "    if any(isinstance(element, str) and element.isdigit() and len(element) == 4 for element in df.iloc[0]):\n",
    "        # Replace NaN values in the first row with random column names\n",
    "        for col_index, value in enumerate(df.iloc[0]):\n",
    "            if pd.isna(value):\n",
    "                df.iloc[0, col_index] = f\"column_{col_index + 1}\"\n",
    "        # Replace column names with the values of the first row\n",
    "        df.columns = df.iloc[0]\n",
    "        # Drop the first row after setting it as column names\n",
    "        df = df.drop(df.index[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17072860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_column(df):\n",
    "    columna_a_expandir = df.columns[-2]\n",
    "    \n",
    "    def reemplazar_guiones(match_obj):\n",
    "        return match_obj.group(1) + ' ' + match_obj.group(2)    \n",
    "\n",
    "    if df[columna_a_expandir].str.contains(r'\\d').any() and df[columna_a_expandir].str.contains(r'[a-zA-Z]').any():\n",
    "        df[columna_a_expandir] = df[columna_a_expandir].apply(lambda x: re.sub(r'([a-zA-Z]+)\\s*-\\s*([a-zA-Z]+)', reemplazar_guiones, str(x)) if pd.notnull(x) else x)\n",
    "\n",
    "        \n",
    "        # Expresión regular para extraer palabras\n",
    "        pattern = re.compile(r'[a-zA-Z\\s]+$')\n",
    "\n",
    "        # Función para aplicar la lógica de extracción y reemplazo a cada fila\n",
    "        def extract_replace(row):\n",
    "            if pd.notnull(row[columna_a_expandir]) and isinstance(row[columna_a_expandir], str):  # Verifica que el valor no sea NaN y sea de tipo string\n",
    "                if row.name != 0:  # Para que empiece desde la segunda fila\n",
    "                    value_to_replace = pattern.search(row[columna_a_expandir])\n",
    "                    if value_to_replace:\n",
    "                        value_to_replace = value_to_replace.group().strip()\n",
    "                        row[df.columns[-1]] = value_to_replace\n",
    "                        row[columna_a_expandir] = re.sub(pattern, '', row[columna_a_expandir]).strip()\n",
    "            return row\n",
    "\n",
    "        # Aplicar la función a cada fila del DataFrame\n",
    "        df = df.apply(extract_replace, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_values_1(df):\n",
    "    columna_a_expandir = df.columns[-2]\n",
    "    nuevas_columnas = df[columna_a_expandir].str.split(expand=True)\n",
    "    nuevas_columnas.columns = [f'{columna_a_expandir}_{i+1}' for i in range(nuevas_columnas.shape[1])]\n",
    "    posicion_insercion = len(df.columns) - 1\n",
    "    for col in reversed(nuevas_columnas.columns):\n",
    "        df.insert(posicion_insercion, col, nuevas_columnas[col])\n",
    "    df.drop(columns=[columna_a_expandir], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8245ec",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2015\\_11}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revisar_primera_fila(df):\n",
    "    primera_fila = df.iloc[0]  # Obtenemos la primera fila del DataFrame\n",
    "    \n",
    "    for i, (col, valor) in enumerate(primera_fila.items()):  # Iteramos sobre los índices y valores de la primera fila\n",
    "        # Comprobamos si algún valor de la primera fila tiene dos años juntos\n",
    "        if re.search(r'\\b\\d{4}\\s\\d{4}\\b', str(valor)):\n",
    "            # Si es así, extraemos los dos años\n",
    "            anios = valor.split()\n",
    "            primer_anio = anios[0]\n",
    "            segundo_anio = anios[1]\n",
    "            \n",
    "            # Nombre de la columna original\n",
    "            nombre_columna_original = f'col_{i}'\n",
    "            df.at[0, col] = nombre_columna_original\n",
    "            \n",
    "            # Actualizamos el valor de la primera columna si es NaN con el primer año\n",
    "            if pd.isna(df.iloc[0, 0]):\n",
    "                df.iloc[0, 0] = primer_anio\n",
    "            \n",
    "            # Actualizamos el valor de la segunda columna si es NaN con el segundo año\n",
    "            if pd.isna(df.iloc[0, 1]):\n",
    "                df.iloc[0, 1] = segundo_anio\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7df157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_with_previous_column_3(df):\n",
    "    columns = df.columns\n",
    "    \n",
    "    for i in range(len(columns) - 1):\n",
    "        # Añadir condición para verificar si la columna actual no es la última\n",
    "        if i != len(columns) - 1 and (columns[i].endswith('_year') and not df[columns[i]].isnull().any()):\n",
    "            # Revisar si la columna de la derecha tiene al menos un NaN y no termina en '_year'\n",
    "            if df[columns[i+1]].isnull().any() and not columns[i+1].endswith('_year'):\n",
    "                nan_indices = df[columns[i+1]].isnull()\n",
    "                df.loc[nan_indices, [columns[i], columns[i+1]]] = df.loc[nan_indices, [columns[i+1], columns[i]]].values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6fc072",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2016\\_15}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revisar_primera_fila_1(df):\n",
    "    # Comprobar si el valor en la primera fila y primera columna es NaN\n",
    "    if pd.isnull(df.iloc[0, 0]):\n",
    "        # Comprobar si el valor en la penúltima columna y primera fila es un año (4 dígitos)\n",
    "        penultimate_column = df.iloc[0, -2]\n",
    "        if isinstance(penultimate_column, str) and len(penultimate_column) == 4 and penultimate_column.isdigit():\n",
    "            # Intercambiar los valores\n",
    "            df.iloc[0, 0] = penultimate_column\n",
    "            df.iloc[0, -2] = np.nan\n",
    "    \n",
    "    # Comprobar si el valor en la segunda columna y primera fila es NaN\n",
    "    if pd.isnull(df.iloc[0, 1]):\n",
    "        # Comprobar si el valor en la última columna y primera fila es un año (4 dígitos)\n",
    "        last_column = df.iloc[0, -1]\n",
    "        if isinstance(last_column, str) and len(last_column) == 4 and last_column.isdigit():\n",
    "            # Intercambiar los valores\n",
    "            df.iloc[0, 1] = last_column\n",
    "            df.iloc[0, -1] = np.nan\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe304de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_values_2(df):\n",
    "    columna_a_expandir = df.columns[-4]\n",
    "    nuevas_columnas = df[columna_a_expandir].str.split(expand=True)\n",
    "    nuevas_columnas.columns = [f'{columna_a_expandir}_{i+1}' for i in range(nuevas_columnas.shape[1])]\n",
    "    posicion_insercion = len(df.columns) - 3\n",
    "    for col in reversed(nuevas_columnas.columns):\n",
    "        df.insert(posicion_insercion, col, nuevas_columnas[col])\n",
    "    df.drop(columns=[columna_a_expandir], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b3bf9",
   "metadata": {},
   "source": [
    "### Exclusiva Tabla 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68cc096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_first_row(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            if isinstance(df.at[0, col], str):\n",
    "                df.at[0, col] = df.at[0, col].lower()  # Convertir a minúsculas solo si es un objeto\n",
    "                df.at[0, col] = remove_tildes(df.at[0, col])\n",
    "                df.at[0, col] = remove_rare_characters_first_row(df.at[0, col])\n",
    "                # Reemplazar 'ano' por 'year'\n",
    "                df.at[0, col] = df.at[0, col].replace('ano', 'year')\n",
    "\n",
    "    return df\n",
    "\n",
    "# 2.\n",
    "def separate_years(df):\n",
    "    df = df.copy()  # Se crea una copia del DataFrame para evitar SettingWithCopyWarning\n",
    "    if isinstance(df.iloc[0, -2], str) and len(df.iloc[0, -2].split()) == 2:\n",
    "        years = df.iloc[0, -2].split()\n",
    "        if all(len(year) == 4 for year in years):\n",
    "            segundo_anio = years[1]\n",
    "            df.iloc[0, -2] = years[0]\n",
    "            df.insert(len(df.columns) - 1, 'new_column', [segundo_anio] + [None] * (len(df) - 1))\n",
    "    return df\n",
    "\n",
    "# 3.\n",
    "def find_roman_numerals(text):\n",
    "    pattern = r'\\b(?:I{1,3}|IV|V|VI{0,3}|IX|X)\\b'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "def relocate_roman_numerals(df):\n",
    "    numeros_romanos = find_roman_numerals(df.iloc[2, -1])\n",
    "    if numeros_romanos:\n",
    "        original_text = df.iloc[2, -1]\n",
    "        for roman_numeral in numeros_romanos:\n",
    "            original_text = original_text.replace(roman_numeral, '').strip()\n",
    "        df.iloc[2, -1] = original_text\n",
    "        df.at[2, 'new_column'] = ', '.join(numeros_romanos)\n",
    "        df.iloc[2, -1] = np.nan\n",
    "    return df\n",
    "\n",
    "# 4.\n",
    "def extract_mixed_values(df):\n",
    "    df = df.copy()  # Se crea una copia del DataFrame para evitar SettingWithCopyWarning\n",
    "    regex_pattern = r'(-?\\d+,\\d [a-zA-Z\\s]+)'\n",
    "    for index, row in df.iterrows():\n",
    "        antepenultima_obs = row.iloc[-3]\n",
    "        penultima_obs = row.iloc[-2]\n",
    "\n",
    "        if isinstance(antepenultima_obs, str) and pd.notnull(antepenultima_obs):\n",
    "            match = re.search(regex_pattern, antepenultima_obs)\n",
    "            if match:\n",
    "                parte_extraida = match.group(0)\n",
    "                if pd.isna(penultima_obs) or pd.isnull(penultima_obs):\n",
    "                    df.iloc[index, -2] = parte_extraida\n",
    "                    antepenultima_obs = re.sub(regex_pattern, '', antepenultima_obs).strip()\n",
    "                    df.iloc[index, -3] = antepenultima_obs\n",
    "    return df\n",
    "\n",
    "# 5.\n",
    "def replace_first_row_nan(df):\n",
    "    for col in df.columns:\n",
    "        if pd.isna(df.iloc[0][col]):\n",
    "            df.iloc[0, df.columns.get_loc(col)] = col\n",
    "    return df\n",
    "\n",
    "# 11. \n",
    "def split_values(df):\n",
    "    columna_a_expandir = df.columns[-3]\n",
    "    nuevas_columnas = df[columna_a_expandir].str.split(expand=True)\n",
    "    nuevas_columnas.columns = [f'{columna_a_expandir}_{i+1}' for i in range(nuevas_columnas.shape[1])]\n",
    "    posicion_insercion = len(df.columns) - 2\n",
    "    for col in reversed(nuevas_columnas.columns):\n",
    "        df.insert(posicion_insercion, col, nuevas_columnas[col])\n",
    "    df.drop(columns=[columna_a_expandir], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# 13.\n",
    "def roman_arabic(df):\n",
    "    primera_fila = df.iloc[0]\n",
    "    def convert_roman_number(numero):\n",
    "        try:\n",
    "            return str(roman.fromRoman(numero))\n",
    "        except roman.InvalidRomanNumeralError:\n",
    "            return numero\n",
    "\n",
    "    primera_fila_convertida = []\n",
    "    for valor in primera_fila:\n",
    "        if isinstance(valor, str) and not pd.isna(valor):\n",
    "            primera_fila_convertida.append(convert_roman_number(valor))\n",
    "        else:\n",
    "            primera_fila_convertida.append(valor)\n",
    "\n",
    "    df.iloc[0] = primera_fila_convertida\n",
    "    return df\n",
    "\n",
    "# 14.\n",
    "def fix_duplicates(df):\n",
    "    fila_segunda = df.iloc[0].copy()\n",
    "    prev_num = None\n",
    "    first_one_index = None\n",
    "\n",
    "    for i, num in enumerate(fila_segunda):\n",
    "        try:\n",
    "            num = int(num)\n",
    "            prev_num = int(prev_num) if prev_num is not None else None\n",
    "\n",
    "            if num == prev_num:\n",
    "                if num == 1:\n",
    "                    if first_one_index is None:\n",
    "                        first_one_index = i - 1\n",
    "                    next_num = int(fila_segunda[i - 1]) + 1\n",
    "                    for j in range(i, len(fila_segunda)):\n",
    "                        if fila_segunda.iloc[j].isdigit():\n",
    "                            fila_segunda.iloc[j] = str(next_num)\n",
    "                            next_num += 1\n",
    "                elif i - 1 >= 0:\n",
    "                    fila_segunda.iloc[i] = str(int(fila_segunda.iloc[i - 1]) + 1)\n",
    "\n",
    "            prev_num = num\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    df.iloc[0] = fila_segunda\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e03cbc",
   "metadata": {},
   "source": [
    "## More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6224f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarters_sublist_list(df, year_columns):\n",
    "    first_row = df.iloc[0]\n",
    "    # Initialize the list of sublists\n",
    "    quarters_sublist_list = []\n",
    "\n",
    "    # Initialize the current sublist\n",
    "    quarters_sublist = []\n",
    "\n",
    "    # Iterate over the elements of the first row\n",
    "    for item in first_row:\n",
    "        # Check if the item meets the requirements\n",
    "        if len(str(item)) == 1:\n",
    "            quarters_sublist.append(item)\n",
    "        elif str(item) == 'year':\n",
    "            quarters_sublist.append(item)\n",
    "            quarters_sublist_list.append(quarters_sublist)\n",
    "            quarters_sublist = []\n",
    "\n",
    "    # Add the last sublist if it's not empty\n",
    "    if quarters_sublist:\n",
    "        quarters_sublist_list.append(quarters_sublist)\n",
    "\n",
    "    new_elements = []\n",
    "\n",
    "    # Check if year_columns is not empty\n",
    "    if year_columns:\n",
    "        for i, year in enumerate(year_columns):\n",
    "            # Check if index i is valid for quarters_sublist_list\n",
    "            if i < len(quarters_sublist_list):\n",
    "                for element in quarters_sublist_list[i]:\n",
    "                    new_elements.append(f\"{year}_{element}\")\n",
    "\n",
    "    two_first_elements = df.iloc[0][:2].tolist()\n",
    "\n",
    "    # Ensure that the two_first_elements are added if they are not in new_elements\n",
    "    for index in range(len(two_first_elements) - 1, -1, -1):\n",
    "        if two_first_elements[index] not in new_elements:\n",
    "            new_elements.insert(0, two_first_elements[index])\n",
    "\n",
    "    # Ensure that the length of new_elements matches the number of columns in df\n",
    "    while len(new_elements) < len(df.columns):\n",
    "        new_elements.append(None)\n",
    "\n",
    "    temp_df = pd.DataFrame([new_elements], columns=df.columns)\n",
    "    df.iloc[0] = temp_df.iloc[0]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb3ff5",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2016\\_19}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_values_3(df):\n",
    "    columna_a_expandir = df.columns[-3]\n",
    "    nuevas_columnas = df[columna_a_expandir].str.split(expand=True)\n",
    "    nuevas_columnas.columns = [f'{columna_a_expandir}_{i+1}' for i in range(nuevas_columnas.shape[1])]\n",
    "    posicion_insercion = len(df.columns) - 2\n",
    "    for col in reversed(nuevas_columnas.columns):\n",
    "        df.insert(posicion_insercion, col, nuevas_columnas[col])\n",
    "    df.drop(columns=[columna_a_expandir], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e96a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_with_previous_column_1(df):\n",
    "    columns = df.columns\n",
    "    \n",
    "    for i in range(len(columns) - 1):\n",
    "        # Añadir condición para verificar si la columna actual no es la última\n",
    "        if i != len(columns) - 2 and not (columns[i].endswith('_year') and df[columns[i]].isnull().any()):\n",
    "            # Revisar si la columna de la derecha tiene al menos un NaN y no termina en '_year'\n",
    "            if df[columns[i+1]].isnull().any() and not columns[i+1].endswith('_year'):\n",
    "                nan_indices = df[columns[i+1]].isnull()\n",
    "                df.loc[nan_indices, [columns[i], columns[i+1]]] = df.loc[nan_indices, [columns[i+1], columns[i]]].values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d6eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_with_previous_column_2(df):\n",
    "    columns = df.columns\n",
    "    \n",
    "    for i in range(len(columns) - 1):\n",
    "        # Añadir condición para verificar si la columna actual no es la última\n",
    "        if i != len(columns) - 2 and not (columns[i].endswith('_year') and df[columns[i]].isnull().any()):\n",
    "            # Revisar si la columna de la derecha tiene al menos un NaN y no termina en '_year'\n",
    "            if df[columns[i+1]].isnull().any() and not columns[i+1].endswith('_year'):\n",
    "                nan_indices = df[columns[i+1]].isnull()\n",
    "                df.loc[nan_indices, [columns[i], columns[i+1]]] = df.loc[nan_indices, [columns[i+1], columns[i]]].values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bb4b7",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2016\\_20}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ab6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan_row(df):\n",
    "    if df.iloc[0].isnull().all():\n",
    "        df = df.drop(index=0)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c56742",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2019\\_17}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f58fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_column_es(df): # similar than relocate last columns\n",
    "    # Check if the DataFrame has at least two columns and the last column is a 4-digit year\n",
    "    #if len(df.columns) >= 2 and df.columns[-1].isdigit() and len(df[df.columns[-1]].iloc[:2]) >= 2:\n",
    "    # Check if the first observation of the last column is 'ECONOMIC SECTORS'\n",
    "    if df[df.columns[-1]].iloc[0] == 'ECONOMIC SECTORS':\n",
    "        # Check if the second observation of the last column is not empty\n",
    "        if pd.notnull(df[df.columns[-1]].iloc[1]):\n",
    "            # Create a new column with NaN values\n",
    "            new_column_name = f\"col_{len(df.columns)}\"\n",
    "            df[new_column_name] = np.nan\n",
    "\n",
    "            # Get 'ECONOMIC SECTORS' and relocate\n",
    "            insert_value_1 = df.iloc[0, -2]\n",
    "            # Convert the value to string before assignment\n",
    "            insert_value_1 = str(insert_value_1)\n",
    "            # Ensure the dtype of the last column is object (string) to accommodate string values\n",
    "            df.iloc[:, -1] = df.iloc[:, -1].astype('object')\n",
    "            df.iloc[0, -1] = insert_value_1\n",
    "\n",
    "            # NaN first obs\n",
    "            df.iloc[0,-2] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028154b1",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2019\\_26}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intercambiar_columnas(df):\n",
    "    # Buscar una columna con todos los valores NaN\n",
    "    columna_nan = None\n",
    "    for columna in df.columns:\n",
    "        if df[columna].isnull().all() and len(columna) == 4 and columna.isdigit():\n",
    "            columna_nan = columna\n",
    "            break\n",
    "    \n",
    "    if columna_nan:\n",
    "        # Revisar la columna de la izquierda\n",
    "        indice_columna = df.columns.get_loc(columna_nan)\n",
    "        if indice_columna > 0:\n",
    "            columna_izquierda = df.columns[indice_columna - 1]\n",
    "            # Verificar si no es un año (no tiene 4 dígitos)\n",
    "            if not (len(columna_izquierda) == 4 and columna_izquierda.isdigit()):\n",
    "                # Intercambiar nombres de columnas\n",
    "                df.rename(columns={columna_nan: columna_izquierda, columna_izquierda: columna_nan}, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673fdcb",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2019\\_29}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc879a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exchange_roman_nan(df):\n",
    "    for col_idx, valor in enumerate(df.iloc[1]):\n",
    "        if isinstance(valor, str):\n",
    "            if valor.upper() == 'AÑO' or (valor.isalpha() and roman.fromRoman(valor.upper())):\n",
    "                siguiente_col = col_idx + 1\n",
    "                if siguiente_col < len(df.columns) and pd.isna(df.iloc[1, siguiente_col]):\n",
    "                    col_actual = df.iloc[:, col_idx].drop(index=1)\n",
    "                    col_siguiente = df.iloc[:, siguiente_col].drop(index=1)\n",
    "                    if col_actual.isna().all():\n",
    "                        df.iloc[1, col_idx], df.iloc[1, siguiente_col] = df.iloc[1, siguiente_col], df.iloc[1, col_idx]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e09b92",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{set \\ by \\ sep}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_set_sep(df):\n",
    "    # Get the column names of the DataFrame\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Iterate over the columns\n",
    "    for column in columns:\n",
    "        # Check if the column contains the expression 'set'\n",
    "        if 'set' in column:\n",
    "            # Replace 'set' with 'sep' in the column name\n",
    "            new_column = column.replace('set', 'sep')\n",
    "            # Rename the column in the DataFrame\n",
    "            df.rename(columns={column: new_column}, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8039a17b",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{strip}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spaces_se_es(df):\n",
    "    # Aplicar strip a las columnas 'sectores_economicos' y 'economic_sectors'\n",
    "    df['sectores_economicos'] = df['sectores_economicos'].str.strip()\n",
    "    df['economic_sectors'] = df['economic_sectors'].str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22cbe20",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{Replace \\ other \\ services}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deedce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_services(df):\n",
    "    # Verificamos si se encuentran los valores 'otros servicios' y 'other services'\n",
    "    if ('servicios' in df['sectores_economicos'].values) and ('services' in df['economic_sectors'].values):\n",
    "        # Reemplazamos los valores\n",
    "        df['sectores_economicos'].replace({'servicios': 'otros servicios'}, inplace=True)\n",
    "        df['economic_sectors'].replace({'services': 'other services'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0db3b",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{Redondear \\ a \\ un \\ decimal}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c611c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redondear_valores(df, decimales=1):\n",
    "    # Iterar sobre todas las columnas del DataFrame\n",
    "    for col in df.columns:\n",
    "        # Verificar si la columna es de tipo float\n",
    "        if df[col].dtype == 'float64':\n",
    "            # Redondear los valores de la columna al número de decimales especificado\n",
    "            df[col] = df[col].round(decimales)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88524d6",
   "metadata": {},
   "source": [
    "# Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ce026",
   "metadata": {},
   "source": [
    "# Con registro de carpetas procesdas 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f028da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import locale\n",
    "from tkinter import Tk, messagebox, TOP, YES, NO\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Establecer la localización en español\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Palabras clave para buscar en el texto de la página\n",
    "keywords = [\"ECONOMIC SECTORS\"]\n",
    "\n",
    "# Diccionario para almacenar los DataFrames generados\n",
    "dataframes_dict_1 = {}\n",
    "\n",
    "# Ruta del archivo de registro de carpetas procesadas\n",
    "registro_path = 'dataframes_record/carpetas_procesadas_1.txt'\n",
    "\n",
    "# Función para corregir los nombres de los meses\n",
    "def corregir_nombre_mes(mes):\n",
    "    meses_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Agrega más mapeos si es necesario para otros nombres de meses\n",
    "    }\n",
    "    return meses_mapping.get(mes, mes)\n",
    "\n",
    "def registrar_carpeta_procesada(carpeta, num_archivos_procesados):\n",
    "    with open(registro_path, 'a') as file:\n",
    "        file.write(f\"{carpeta}:{num_archivos_procesados}\\n\")\n",
    "\n",
    "def carpeta_procesada(carpeta):\n",
    "    if not os.path.exists(registro_path):\n",
    "        return False\n",
    "    with open(registro_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(carpeta):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def obtener_fecha(df, engine):\n",
    "    id_ns = df['id_ns'].iloc[0]\n",
    "    year = df['year'].iloc[0]\n",
    "    query = f\"SELECT date FROM dates_growth_rates WHERE id_ns = '{id_ns}' AND year = '{year}';\"\n",
    "    fecha = pd.read_sql(query, engine)\n",
    "    return fecha.iloc[0, 0] if not fecha.empty else None\n",
    "\n",
    "def procesar_pdf(pdf_path):\n",
    "    tables_dict_1 = {}  # Diccionario local para cada PDF\n",
    "    table_counter = 1\n",
    "    keyword_count = 0 \n",
    "\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No se encontraron coincidencias para id_ns y year en el nombre del archivo:\", filename)\n",
    "        return None, None, None, None, None  # Return None for tables_dict_1 as well\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(pdf_path))[0].replace('-', '_')\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            text = page.extract_text()\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                keyword_count += 1\n",
    "                if keyword_count == 1:  # Solo procesar la primera ocurrencia\n",
    "                    tables = tabula.read_pdf(pdf_path, pages=i, multiple_tables=False, stream=True) # change stream to another option if desired\n",
    "                    for j, table_df in enumerate(tables, start=1):\n",
    "                        nombre_dataframe = f\"{new_filename}_{keyword_count}\"\n",
    "                        tables_dict_1[nombre_dataframe] = table_df\n",
    "                        table_counter += 1\n",
    "\n",
    "                    break  # Salir del bucle después de encontrar la primera ocurrencia\n",
    "\n",
    "    return id_ns, year, tables_dict_1, keyword_count  # No retornar date aquí\n",
    "\n",
    "def procesar_carpeta(carpeta, engine):\n",
    "    print(f\"Procesando la carpeta {os.path.basename(carpeta)}\")\n",
    "    pdf_files = [os.path.join(carpeta, f) for f in os.listdir(carpeta) if f.endswith('.pdf')]\n",
    "\n",
    "    num_pdfs_procesados = 0\n",
    "    num_dataframes_generados = 0\n",
    "\n",
    "    table_counter = 1  # Inicializar el contador de tabla aquí\n",
    "    tables_dict_1 = {}  # Declarar tables_dict_1 fuera del bucle principal\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        id_ns, year, tables_dict_temp, keyword_count = procesar_pdf(pdf_file)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for nombre_df, df in tables_dict_temp.items():\n",
    "                nombre_archivo = os.path.splitext(os.path.basename(pdf_file))[0].replace('-', '_')\n",
    "                nombre_df = f\"{nombre_archivo}_{keyword_count}\"\n",
    "                \n",
    "                # Almacenar DataFrame sin procesar en tables_dict_1\n",
    "                tables_dict_1[nombre_df] = df.copy()\n",
    "                \n",
    "                # Aplicar las 20 líneas de funciones de limpieza a una copia del DataFrame\n",
    "                df_clean = df.copy()\n",
    "\n",
    "                if any(col.isdigit() and len(col) == 4 for col in df_clean.columns):\n",
    "                    # Si hay al menos una columna que representa un año\n",
    "                    df_clean = swap_nan_se(df_clean)\n",
    "                    df_clean = split_column_by_pattern(df_clean)\n",
    "                    df_clean = drop_rare_caracter_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = relocate_last_columns(df_clean)\n",
    "                    df_clean = replace_first_dot(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = replace_var_perc_first_column(df_clean)\n",
    "                    df_clean = replace_var_perc_last_columns(df_clean)\n",
    "                    df_clean = replace_number_moving_average(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = intercambiar_valores(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = find_year_column(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = convertir_float(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = redondear_valores(df_clean, decimales=1)\n",
    "                else: # 2014 ns 08\n",
    "                    # Si no hay columnas que representen años\n",
    "                    df_clean = revisar_primera_fila(df_clean)\n",
    "                    df_clean = revisar_primera_fila_1(df_clean)\n",
    "                    df_clean = replace_first_row_with_columns(df_clean)\n",
    "                    df_clean = swap_nan_se(df_clean)\n",
    "                    df_clean = split_column_by_pattern(df_clean)\n",
    "                    df_clean = drop_rare_caracter_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = relocate_last_columns(df_clean)\n",
    "                    #df_clean = replace_first_dot(df_clean) # comment for 2014 ns 08\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = replace_var_perc_first_column(df_clean)\n",
    "                    df_clean = replace_var_perc_last_columns(df_clean)\n",
    "                    df_clean = replace_number_moving_average(df_clean)\n",
    "                    df_clean = expand_column(df_clean) # 2014 ns 08\n",
    "                    df_clean = split_values_1(df_clean) # 2014 ns 08\n",
    "                    df_clean = split_values_2(df_clean) # 2016 ns 15\n",
    "                    df_clean = split_values_3(df_clean) # 2016 ns 19\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = intercambiar_valores(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = find_year_column(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = convertir_float(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column_1(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column_2(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column_3(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = redondear_valores(df_clean, decimales=1)\n",
    "                \n",
    "                # Añadir la columna 'year' al DataFrame limpio\n",
    "                df_clean.insert(0, 'year', year)\n",
    "                \n",
    "                # Añadir la columna 'id_ns' al DataFrame limpio\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                \n",
    "                # Obtener la fecha correspondiente de la base de datos\n",
    "                fecha = obtener_fecha(df_clean, engine)\n",
    "                if fecha:\n",
    "                    # Añadir la columna 'date' al DataFrame limpio\n",
    "                    df_clean.insert(2, 'date', fecha)\n",
    "                else:\n",
    "                    print(\"No se encontró fecha en la base de datos para id_ns:\", id_ns, \"y year:\", year)\n",
    "                \n",
    "                # Almacenar DataFrame limpio en dataframes_dict_1\n",
    "                dataframes_dict_1[nombre_df] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. El dataframe generado para el archivo {pdf_file} es: {nombre_df}')\n",
    "                num_dataframes_generados += 1\n",
    "                table_counter += 1  # Incrementar el contador de tabla aquí\n",
    "        \n",
    "        num_pdfs_procesados += 1  # Incrementar el número de PDFs procesados por cada PDF en la carpeta\n",
    "\n",
    "    return num_pdfs_procesados, num_dataframes_generados, tables_dict_1\n",
    "\n",
    "\n",
    "def procesar_carpetas():\n",
    "    pdf_folder = 'input_pdf'\n",
    "    carpetas = [os.path.join(pdf_folder, d) for d in os.listdir(pdf_folder) if os.path.isdir(os.path.join(pdf_folder, d))]\n",
    "    \n",
    "    tables_dict_1 = {}  # Inicializar tables_dict_1 aquí\n",
    "    \n",
    "    for carpeta in carpetas:\n",
    "        if carpeta_procesada(carpeta):\n",
    "            print(f\"La carpeta {carpeta} ya ha sido procesada.\")\n",
    "            continue\n",
    "        \n",
    "        num_pdfs_procesados, num_dataframes_generados, tables_dict_temp = procesar_carpeta(carpeta, engine)\n",
    "        \n",
    "        # Actualizar tables_dict_1 con los valores devueltos de procesar_carpeta()\n",
    "        tables_dict_1.update(tables_dict_temp)\n",
    "        \n",
    "        registrar_carpeta_procesada(carpeta, num_pdfs_procesados)\n",
    "\n",
    "        # Preguntar al usuario si desea continuar con la siguiente carpeta\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Para asegurar que la ventana esté en primer plano\n",
    "        \n",
    "        mensaje = f\"Se han generado {num_dataframes_generados} dataframes en la carpeta {carpeta}. ¿Deseas continuar con la siguiente carpeta?\"\n",
    "        continuar = messagebox.askyesno(\"Continuar\", mensaje)\n",
    "        root.destroy()\n",
    "\n",
    "        if not continuar:\n",
    "            print(\"Procesamiento detenido por el usuario.\")\n",
    "            break  # Romper el bucle for si el usuario decide no continuar\n",
    "\n",
    "    print(\"Procesamiento completado para todas las carpetas.\")  # Add a message to indicate completion\n",
    "\n",
    "    return tables_dict_1  # Devolver tables_dict_1 al final de la función\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get environment variables\n",
    "    user = os.environ.get('CIUP_SQL_USER')\n",
    "    password = os.environ.get('CIUP_SQL_PASS')\n",
    "    host = os.environ.get('CIUP_SQL_HOST')\n",
    "    port = 5432\n",
    "    database = 'gdp_revisions_datasets'\n",
    "\n",
    "    # Check if all environment variables are defined\n",
    "    if not all([host, user, password]):\n",
    "        raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "    # Create connection string\n",
    "    connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    tables_dict_1 = procesar_carpetas()  # Capturar el valor devuelto de procesar_carpetas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf076a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc21c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff21f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6cc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_1['ns_01_2024_1'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ecc28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_1 = dataframes_dict_1['ns_01_2024_1']\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee02360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[(df_1['sectores_economicos'] == 'agropecuario') | (df_1['economic_sectors'] == 'agriculture and livestock')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105bd81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edee4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5ed43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc3862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a301c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5108832",
   "metadata": {},
   "source": [
    "# Table 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28075a94",
   "metadata": {},
   "source": [
    "# Con registro de carpetas procesdas 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import roman\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import locale\n",
    "from tkinter import Tk, messagebox, TOP, YES, NO\n",
    "import os\n",
    "\n",
    "\n",
    "# Establecer la localización en español\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Palabras clave para buscar en el texto de la página\n",
    "keywords = [\"ECONOMIC SECTORS\"]\n",
    "\n",
    "# Diccionario para almacenar los DataFrames generados\n",
    "dataframes_dict_2 = {}\n",
    "\n",
    "# Ruta del archivo de registro de carpetas procesadas\n",
    "registro_path = 'dataframes_record/carpetas_procesadas_2.txt'\n",
    "\n",
    "# Función para corregir los nombres de los meses\n",
    "def corregir_nombre_mes(mes):\n",
    "    meses_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Agrega más mapeos si es necesario para otros nombres de meses\n",
    "    }\n",
    "    return meses_mapping.get(mes, mes)\n",
    "\n",
    "def registrar_carpeta_procesada(carpeta, num_archivos_procesados):\n",
    "    with open(registro_path, 'a') as file:\n",
    "        file.write(f\"{carpeta}:{num_archivos_procesados}\\n\")\n",
    "\n",
    "def carpeta_procesada(carpeta):\n",
    "    if not os.path.exists(registro_path):\n",
    "        return False\n",
    "    with open(registro_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(carpeta):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def obtener_fecha(df, engine):\n",
    "    id_ns = df['id_ns'].iloc[0]\n",
    "    year = df['year'].iloc[0]\n",
    "    query = f\"SELECT date FROM dates_growth_rates WHERE id_ns = '{id_ns}' AND year = '{year}';\"\n",
    "    fecha = pd.read_sql(query, engine)\n",
    "    return fecha.iloc[0, 0] if not fecha.empty else None\n",
    "\n",
    "def procesar_pdf(pdf_path):\n",
    "    tables_dict_2 = {}  # Diccionario local para cada PDF\n",
    "    table_counter = 1\n",
    "    keyword_count = 0 \n",
    "\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No se encontraron coincidencias para id_ns y year en el nombre del archivo:\", filename)\n",
    "        return None, None, None, None\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(pdf_path))[0].replace('-', '_')\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            text = page.extract_text()\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                keyword_count += 1\n",
    "                if keyword_count == 2:\n",
    "                    tables = tabula.read_pdf(pdf_path, pages=i, multiple_tables=False)\n",
    "                    for j, table_df in enumerate(tables, start=1):\n",
    "                        nombre_dataframe = f\"{new_filename}_{keyword_count}\"\n",
    "                        tables_dict_2[nombre_dataframe] = table_df\n",
    "                        table_counter += 1\n",
    "\n",
    "    return id_ns, year, tables_dict_2, keyword_count\n",
    "\n",
    "\n",
    "def procesar_carpeta(carpeta, engine):\n",
    "    print(f\"Procesando la carpeta {os.path.basename(carpeta)}\")\n",
    "    pdf_files = [os.path.join(carpeta, f) for f in os.listdir(carpeta) if f.endswith('.pdf')]\n",
    "\n",
    "    num_pdfs_procesados = 0\n",
    "    num_dataframes_generados = 0\n",
    "\n",
    "    table_counter = 1  # Inicializar el contador de tabla aquí\n",
    "    tables_dict_2 = {}  # Declarar tables_dict fuera del bucle principal\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        id_ns, year, tables_dict_temp, keyword_count = procesar_pdf(pdf_file)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for nombre_df, df in tables_dict_temp.items():\n",
    "                nombre_archivo = os.path.splitext(os.path.basename(pdf_file))[0].replace('-', '_')\n",
    "                nombre_df = f\"{nombre_archivo}_{keyword_count}\"\n",
    "\n",
    "                # Almacenar DataFrame sin procesar en tables_dict\n",
    "                tables_dict_2[nombre_df] = df.copy()\n",
    "\n",
    "                # Aplicar las 20 líneas de funciones de limpieza a una copia del DataFrame\n",
    "                df_clean = df.copy()\n",
    "                if df_clean.iloc[0, 0] is np.nan:\n",
    "                    # Aplicar las 20 líneas de limpieza\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = separate_years(df_clean)\n",
    "                    df_clean = relocate_roman_numerals(df_clean)\n",
    "                    df_clean = extract_mixed_values(df_clean)\n",
    "                    df_clean = replace_first_row_nan(df_clean)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = drop_nan_row(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = split_values(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = roman_arabic(df_clean)\n",
    "                    df_clean = fix_duplicates(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = convertir_float(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = redondear_valores(df_clean, decimales=1)\n",
    "                else:\n",
    "                    # Aplicar las 15 líneas de limpieza\n",
    "                    df_clean = exchange_roman_nan(df_clean)\n",
    "                    df_clean = intercambiar_columnas(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = last_column_es(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = roman_arabic(df_clean)\n",
    "                    df_clean = fix_duplicates(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = convertir_float(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = redondear_valores(df_clean, decimales=1)\n",
    "\n",
    "                # Añadir la columna 'year' al DataFrame limpio\n",
    "                df_clean.insert(0, 'year', year)\n",
    "                \n",
    "                # Añadir la columna 'id_ns' al DataFrame limpio\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                \n",
    "                # Obtener la fecha correspondiente de la base de datos\n",
    "                fecha = obtener_fecha(df_clean, engine)\n",
    "                if fecha:\n",
    "                    # Añadir la columna 'date' al DataFrame limpio\n",
    "                    df_clean.insert(2, 'date', fecha)\n",
    "                else:\n",
    "                    print(\"No se encontró fecha en la base de datos para id_ns:\", id_ns, \"y year:\", year)\n",
    "\n",
    "                # Almacenar DataFrame limpio en dataframes_dict\n",
    "                dataframes_dict_2[nombre_df] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. El dataframe generado para el archivo {pdf_file} es: {nombre_df}')\n",
    "                num_dataframes_generados += 1\n",
    "                table_counter += 1  # Incrementar el contador de tabla aquí\n",
    "                    \n",
    "        num_pdfs_procesados += 1  # Incrementar el número de PDFs procesados por cada PDF en la carpeta\n",
    "\n",
    "    return num_pdfs_procesados, num_dataframes_generados, tables_dict_2\n",
    "        \n",
    "def procesar_carpetas():\n",
    "    pdf_folder = 'input_pdf'\n",
    "    carpetas = [os.path.join(pdf_folder, d) for d in os.listdir(pdf_folder) if os.path.isdir(os.path.join(pdf_folder, d))]\n",
    "\n",
    "    tables_dict_2 = {}  # Inicializar tables_dict aquí\n",
    "    \n",
    "    for carpeta in carpetas:\n",
    "        if carpeta_procesada(carpeta):\n",
    "            print(f\"La carpeta {carpeta} ya ha sido procesada.\")\n",
    "            continue\n",
    "        \n",
    "        num_pdfs_procesados, num_dataframes_generados, tables_dict_temp = procesar_carpeta(carpeta, engine)\n",
    "        \n",
    "        # Actualizar tables_dict con los valores devueltos de procesar_carpeta()\n",
    "        tables_dict_2.update(tables_dict_temp)\n",
    "        \n",
    "        registrar_carpeta_procesada(carpeta, num_pdfs_procesados)\n",
    "\n",
    "        # Preguntar al usuario si desea continuar con la siguiente carpeta\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Para asegurar que la ventana esté en primer plano\n",
    "        \n",
    "        mensaje = f\"Se han generado {num_dataframes_generados} dataframes en la carpeta {carpeta}. ¿Deseas continuar con la siguiente carpeta?\"\n",
    "        continuar = messagebox.askyesno(\"Continuar\", mensaje)\n",
    "        root.destroy()\n",
    "\n",
    "        if not continuar:\n",
    "            print(\"Procesamiento detenido por el usuario.\")\n",
    "            break  # Romper el bucle for si el usuario decide no continuar\n",
    "\n",
    "    print(\"Procesamiento completado para todas las carpetas.\")  # Add a message to indicate completion\n",
    "\n",
    "    return tables_dict_2  # Devolver tables_dict al final de la función\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Get environment variables\n",
    "    user = os.environ.get('CIUP_SQL_USER')\n",
    "    password = os.environ.get('CIUP_SQL_PASS')\n",
    "    host = os.environ.get('CIUP_SQL_HOST')\n",
    "    port = 5432\n",
    "    database = 'gdp_revisions_datasets'\n",
    "\n",
    "    # Check if all environment variables are defined\n",
    "    if not all([host, user, password]):\n",
    "        raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "    # Create connection string\n",
    "    connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "    tables_dict_2 = procesar_carpetas() # Capturar el valor devuelto de procesar_carpetas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb27b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a46bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_2['ns_01_2024_2'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict_2['ns_01_2024_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c8410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e0f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709d3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "667d97b8",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217183e7",
   "metadata": {},
   "source": [
    "<div id=\"3\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8989d65",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">SQL Tables</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a04abd",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Finally, after obtaining and cleaning all the necessary data, we can create the three most important datasets to store realeses, vintages, and revisions. These datasets will be stored as tables in SQL and can be loaded into any software or programming language.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0b233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b689b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a26633b0",
   "metadata": {},
   "source": [
    "<div id=\"sector\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0dda6f",
   "metadata": {},
   "source": [
    "# Chose sector_economico and economic_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "# Definir la lista de opciones\n",
    "opciones = [\n",
    "    \"pbi\",\n",
    "    \"agropecuario\",\n",
    "    \"pesca\",\n",
    "    \"mineria e hidrocarburos\",\n",
    "    \"manufactura\",\n",
    "    \"electricidad y agua\",\n",
    "    \"construccion\",\n",
    "    \"comercio\",\n",
    "    \"otros servicios\"\n",
    "]\n",
    "\n",
    "# Función para guardar la opción seleccionada y cerrar la ventana\n",
    "def guardar_opcion():\n",
    "    global sector_economico\n",
    "    sector_economico = opcion_seleccionada.get()\n",
    "    root.destroy()  # Cerrar la ventana después de seleccionar una opción\n",
    "\n",
    "# Crear la ventana emergente\n",
    "root = tk.Tk()\n",
    "root.title(\"Seleccionar opción\")\n",
    "\n",
    "# Variable para almacenar la opción seleccionada\n",
    "opcion_seleccionada = tk.StringVar(root)\n",
    "opcion_seleccionada.set(opciones[0])  # Opción predeterminada\n",
    "\n",
    "# Crear el menú de opciones\n",
    "menu = tk.OptionMenu(root, opcion_seleccionada, *opciones)\n",
    "menu.pack(pady=10)\n",
    "\n",
    "# Botón para confirmar la selección\n",
    "boton_confirmar = tk.Button(root, text=\"Confirmar\", command=guardar_opcion)\n",
    "boton_confirmar.pack()\n",
    "\n",
    "# Mostrar la ventana\n",
    "root.update_idletasks()\n",
    "root.wait_window()\n",
    "\n",
    "# Mostrar el valor seleccionado\n",
    "print(\"Sector económico seleccionado:\", opcion_seleccionada.get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "# Definir la lista de opciones\n",
    "opciones = [\n",
    "    \"gdp\",\n",
    "    \"agriculture and livestock\",\n",
    "    \"fishing\",\n",
    "    \"mining and fuel\",\n",
    "    \"manufacturing\",\n",
    "    \"electricity and water\",\n",
    "    \"construction\",\n",
    "    \"commerce\",\n",
    "    \"other services\"\n",
    "]\n",
    "\n",
    "# Función para guardar la opción seleccionada y cerrar la ventana\n",
    "def guardar_opcion():\n",
    "    global economic_sector\n",
    "    economic_sector = opcion_seleccionada.get()\n",
    "    root.destroy()  # Cerrar la ventana después de seleccionar una opción\n",
    "\n",
    "# Crear la ventana emergente\n",
    "root = tk.Tk()\n",
    "root.title(\"Seleccionar opción\")\n",
    "\n",
    "# Variable para almacenar la opción seleccionada\n",
    "opcion_seleccionada = tk.StringVar(root)\n",
    "opcion_seleccionada.set(opciones[0])  # Opción predeterminada\n",
    "\n",
    "# Crear el menú de opciones\n",
    "menu = tk.OptionMenu(root, opcion_seleccionada, *opciones)\n",
    "menu.pack(pady=10)\n",
    "\n",
    "# Botón para confirmar la selección\n",
    "boton_confirmar = tk.Button(root, text=\"Confirmar\", command=guardar_opcion)\n",
    "boton_confirmar.pack()\n",
    "\n",
    "# Mostrar la ventana\n",
    "root.update_idletasks()\n",
    "root.wait_window()\n",
    "\n",
    "# Mostrar el valor seleccionado\n",
    "print(\"Sector económico seleccionado:\", opcion_seleccionada.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb61ed",
   "metadata": {},
   "source": [
    "# Chose the year and label datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f31ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "# Crear una ventana principal\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Ocultar la ventana principal\n",
    "\n",
    "# Pedir al usuario que introduzca el valor de sector_economico\n",
    "sector = simpledialog.askstring(\"Sector Económico\", \"Introduce el valor del sector:\")\n",
    "\n",
    "# Pedir al usuario que introduzca el valor de economic_sector\n",
    "#year = simpledialog.askstring(\"Year\", \"Introduce el valor de year:\")\n",
    "\n",
    "# Mostrar los valores introducidos por el usuario\n",
    "print(\"Valor del sector:\", sector)\n",
    "#print(\"Valor de economic_sector:\", year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6952a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84e6ca98",
   "metadata": {},
   "source": [
    "<div id=\"3-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b142e",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Annual Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_annual_df(dataframes_dict):\n",
    "    # List to store the names of dataframes that meet the criterion of ending in '_2'\n",
    "    dataframes_ending_with_2 = []\n",
    "\n",
    "    # List to store the names of dataframes to be concatenated\n",
    "    dataframes_to_concatenate = []\n",
    "\n",
    "    # Iterate over the dataframe names in the all_dataframes dictionary\n",
    "    for df_name in dataframes_dict.keys():\n",
    "        # Check if the dataframe name ends with '_2' and add it to the corresponding list\n",
    "        if df_name.endswith('_2'):\n",
    "            dataframes_ending_with_2.append(df_name)\n",
    "            dataframes_to_concatenate.append(dataframes_dict[df_name])\n",
    "\n",
    "    # Print the names of dataframes that meet the criterion of ending in '_2'\n",
    "    print(\"DataFrames ending with '_2' that will be concatenated:\")\n",
    "    for df_name in dataframes_ending_with_2:\n",
    "        print(df_name)\n",
    "\n",
    "    # Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "    if dataframes_to_concatenate:\n",
    "        # Concatenate only rows that meet the specified conditions\n",
    "        annual_growth_rates = pd.concat([df[(df['sectores_economicos'] == sector_economico) | (df['economic_sectors'] == economic_sector)] \n",
    "                                    for df in dataframes_to_concatenate \n",
    "                                    if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Keep only columns that start with 'year' and the 'id_ns', 'year', and 'date' columns\n",
    "        columns_to_keep = ['year', 'id_ns', 'date'] + [col for col in annual_growth_rates.columns if col.endswith('_year')]\n",
    "\n",
    "        # Drop unwanted columns\n",
    "        annual_growth_rates = annual_growth_rates[columns_to_keep]\n",
    "        \n",
    "        # Remove duplicate columns if any\n",
    "        annual_growth_rates = annual_growth_rates.loc[:,~annual_growth_rates.columns.duplicated()]\n",
    "    \n",
    "        # Cambia el nombre de las columnas a partir de la cuarta columna\n",
    "        annual_growth_rates.columns = [col.split('_')[1] + '_' + col.split('_')[0] if '_' in col and idx >= 3 else col for idx, col in enumerate(annual_growth_rates.columns)]\n",
    "\n",
    "        # Print the number of rows in the concatenated dataframe\n",
    "        print(\"Number of rows in the concatenated dataframe:\", len(annual_growth_rates))\n",
    "        \n",
    "        return annual_growth_rates\n",
    "    else:\n",
    "        print(\"No dataframes were found to concatenate.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()[f\"{sector}_annual_growth_rates\"] = concatenate_annual_df(dataframes_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "globals()[f\"{sector}_annual_growth_rates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f8d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61bbaac2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d9bac",
   "metadata": {},
   "source": [
    "<div id=\"3-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50087e95",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.2.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Quarterly Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6467ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concatenate_quarterly_df(dataframes_dict):\n",
    "    # List to store the names of dataframes that meet the criterion of ending in '_2'\n",
    "    dataframes_ending_with_2 = []\n",
    "\n",
    "    # List to store the names of dataframes to be concatenated\n",
    "    dataframes_to_concatenate = []\n",
    "\n",
    "    # Iterate over the dataframe names in the all_dataframes dictionary\n",
    "    for df_name in dataframes_dict.keys():\n",
    "        # Check if the dataframe name ends with '_2' and add it to the corresponding list\n",
    "        if df_name.endswith('_2'):\n",
    "            dataframes_ending_with_2.append(df_name)\n",
    "            dataframes_to_concatenate.append(dataframes_dict[df_name])\n",
    "\n",
    "    # Print the names of dataframes that meet the criterion of ending in '_2'\n",
    "    print(\"DataFrames ending with '_2' that will be concatenated:\")\n",
    "    for df_name in dataframes_ending_with_2:\n",
    "        print(df_name)\n",
    "\n",
    "    # Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "    if dataframes_to_concatenate:\n",
    "        # Concatenate only rows that meet the specified conditions\n",
    "        quarterly_growth_rates = pd.concat([df[(df['sectores_economicos'] == sector_economico) | (df['economic_sectors'] == economic_sector)] \n",
    "                                    for df in dataframes_to_concatenate \n",
    "                                    if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Keep all columns except those starting with 'year_', in addition to the 'id_ns', 'year', and 'date' columns\n",
    "        columns_to_keep = ['year', 'id_ns', 'date'] + [col for col in quarterly_growth_rates.columns if not col.endswith('_year')]\n",
    "\n",
    "        # Select unwanted columns\n",
    "        quarterly_growth_rates = quarterly_growth_rates[columns_to_keep]\n",
    "\n",
    "        # Drop the 'sectores_economicos' and 'economic_sectors' columns\n",
    "        quarterly_growth_rates.drop(columns=['sectores_economicos', 'economic_sectors'], inplace=True)\n",
    "\n",
    "        # Remove duplicate columns if any\n",
    "        quarterly_growth_rates = quarterly_growth_rates.loc[:,~quarterly_growth_rates.columns.duplicated()]\n",
    "\n",
    "        # Cambia el nombre de las columnas a partir de la cuarta columna\n",
    "        #quarterly_growth_rates.columns = [col.split('_')[0] + '_q' + col.split('_')[1] if '_' in col and idx >= 3 else col\n",
    "        #for idx, col in enumerate(quarterly_growth_rates.columns)]\n",
    "        \n",
    "        # Print the number of rows in the concatenated dataframe\n",
    "        print(\"Number of rows in the concatenated dataframe:\", len(quarterly_growth_rates))\n",
    "        \n",
    "        return quarterly_growth_rates\n",
    "    else:\n",
    "        print(\"No dataframes were found to concatenate.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b797aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()[f\"{sector}_quarterly_growth_rates\"] = concatenate_quarterly_df(dataframes_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "globals()[f\"{sector}_quarterly_growth_rates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d99fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24dd0fc0",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca271f",
   "metadata": {},
   "source": [
    "<div id=\"3-3\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453bb965",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.3.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Monthly Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a86c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concatenate_monthly_df(dataframes_dict):\n",
    "    # List to store the names of dataframes that meet the criterion of ending in '_1'\n",
    "    dataframes_ending_with_1 = []\n",
    "\n",
    "    # List to store the names of dataframes to be concatenated\n",
    "    dataframes_to_concatenate = []\n",
    "\n",
    "    # Iterate over the dataframe names in the all_dataframes dictionary\n",
    "    for df_name in dataframes_dict.keys():\n",
    "        # Check if the dataframe name ends with '_1' and add it to the corresponding list\n",
    "        if df_name.endswith('_1'):\n",
    "            dataframes_ending_with_1.append(df_name)\n",
    "            dataframes_to_concatenate.append(dataframes_dict[df_name])\n",
    "\n",
    "    # Print the names of dataframes that meet the criterion of ending with '_1'\n",
    "    print(\"DataFrames ending with '_1' that will be concatenated:\")\n",
    "    for df_name in dataframes_ending_with_1:\n",
    "        print(df_name)\n",
    "\n",
    "    # Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "    if dataframes_to_concatenate:\n",
    "        # Concatenate only rows that meet the specified conditions\n",
    "        monthly_growth_rates = pd.concat([df[(df['sectores_economicos'] == sector_economico) | (df['economic_sectors'] == economic_sector)] \n",
    "                                    for df in dataframes_to_concatenate \n",
    "                                    if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Keep all columns except those starting with 'year_', in addition to the 'id_ns', 'year', and 'date' columns\n",
    "        columns_to_keep = ['year', 'id_ns', 'date'] + [col for col in monthly_growth_rates.columns if not col.endswith('_year')]\n",
    "\n",
    "        # Select unwanted columns\n",
    "        monthly_growth_rates = monthly_growth_rates[columns_to_keep]\n",
    "\n",
    "        # Drop the 'sectores_economicos' and 'economic_sectors' columns\n",
    "        monthly_growth_rates.drop(columns=['sectores_economicos', 'economic_sectors'], inplace=True)\n",
    "\n",
    "        # Remove duplicate columns if any\n",
    "        monthly_growth_rates = monthly_growth_rates.loc[:,~monthly_growth_rates.columns.duplicated()]\n",
    "        \n",
    "        # Drop columns with at least two underscores in their names\n",
    "        columns_to_drop = [col for col in monthly_growth_rates.columns if col.count('_') >= 2]\n",
    "        monthly_growth_rates.drop(columns=columns_to_drop, inplace=True)\n",
    "        \n",
    "        # Cambia el nombre de las columnas a partir de la cuarta columna\n",
    "        monthly_growth_rates.columns = [col.split('_')[1] + '_' + col.split('_')[0] if '_' in col and idx >= 3 else col for idx, col in enumerate(monthly_growth_rates.columns)]\n",
    "        \n",
    "        # Diccionario de mapeo de nombres de meses\n",
    "        #meses = {\n",
    "        #    'ene': 'm1', 'feb': 'm2', 'mar': 'm3', 'abr': 'm4',\n",
    "        #    'may': 'm5', 'jun': 'm6', 'jul': 'm7', 'ago': 'm8',\n",
    "        #    'sep': 'm9', 'oct': 'm10', 'nov': 'm11', 'dic': 'm12'\n",
    "        #}\n",
    "        \n",
    "        # Función para reemplazar las claves por los valores del diccionario en el nombre de las columnas\n",
    "        #def replace_months(column_name, meses):\n",
    "        #    for key, value in meses.items():\n",
    "        #        if key in column_name:\n",
    "        #            return column_name.replace(key, value)\n",
    "        #    return column_name\n",
    "\n",
    "        # Aplicar la función a todas las columnas del DataFrame\n",
    "        #monthly_growth_rates.columns = monthly_growth_rates.columns.map(lambda x: replace_months(x, meses))\n",
    "\n",
    "        # Print the number of rows in the concatenated dataframe\n",
    "        print(\"Number of rows in the concatenated dataframe:\", len(monthly_growth_rates))\n",
    "\n",
    "        return monthly_growth_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671223c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "globals()[f\"{sector}_monthly_growth_rates\"] = concatenate_monthly_df(dataframes_dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "globals()[f\"{sector}_monthly_growth_rates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01064582",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #1E3B58; color: white; padding: 10px;\">\n",
    "    <h2>Create revision dataset (annual)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e71f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calcular la diferencia entre el último y el primer valor no NaN para cada columna, excepto 'year', 'ns_id' y 'date'\n",
    "revision = globals()[f\"{sector}_annual_growth_rates\"].drop(columns=['year', 'id_ns', 'date']).apply(lambda x: x.loc[x.last_valid_index()] - x.loc[x.first_valid_index()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4ec44",
   "metadata": {},
   "source": [
    "#  Nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Crear un nuevo DataFrame con los resultados\n",
    "globals()[f\"{sector}_annual_revisions\"] = pd.DataFrame({'revision_date': revision.index, f'{sector}_revision': revision.values})\n",
    "# Suponiendo que 'gdp_monthly_growth_rates' es tu DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "globals()[f\"{sector}_annual_revisions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b7e2d",
   "metadata": {},
   "source": [
    "# Para graficar en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extraer el año de la cadena y convertirlo a un tipo entero\n",
    "globals()[f\"{sector}_annual_revisions\"]['year'] = globals()[f\"{sector}_annual_revisions\"]['revision_date'].str.extract(r'(\\d+)')\n",
    "globals()[f\"{sector}_annual_revisions\"]['year'] = globals()[f\"{sector}_annual_revisions\"]['year'].astype(int)\n",
    "\n",
    "# Crear una nueva columna de tipo fecha\n",
    "globals()[f\"{sector}_annual_revisions\"]['revision_date'] = pd.to_datetime(globals()[f\"{sector}_annual_revisions\"]['year'], format='%Y')\n",
    "\n",
    "# Eliminar la columna 'year' si ya no es necesaria\n",
    "globals()[f\"{sector}_annual_revisions\"].drop(columns=['year'], inplace=True)\n",
    "\n",
    "# Mostrar el resultado\n",
    "globals()[f\"{sector}_annual_revisions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ccfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c21868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed00dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f59fc6d7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #1E3B58; color: white; padding: 10px;\">\n",
    "    <h2>Create revision dataset (quarterly)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac55f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calcular la diferencia entre el último y el primer valor no NaN para cada columna, excepto 'year', 'ns_id' y 'date'\n",
    "revision = globals()[f\"{sector}_quarterly_growth_rates\"].drop(columns=['year', 'id_ns', 'date']).apply(lambda x: x.loc[x.last_valid_index()] - x.loc[x.first_valid_index()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc5ee4",
   "metadata": {},
   "source": [
    "#  Nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782ba95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Crear un nuevo DataFrame con los resultados\n",
    "globals()[f\"{sector}_quarterly_revisions\"] = pd.DataFrame({'revision_date': revision.index, f'{sector}_revision': revision.values})\n",
    "# Suponiendo que 'gdp_monthly_growth_rates' es tu DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "globals()[f\"{sector}_quarterly_revisions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009913b",
   "metadata": {},
   "source": [
    "# Para graficar en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99e594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convertir la columna 'revision_date' a tipo de datos de fecha\n",
    "globals()[f\"{sector}_quarterly_revisions\"]['revision_date'] = pd.to_datetime(globals()[f\"{sector}_quarterly_revisions\"]['revision_date'], format='%Y_%m')\n",
    "\n",
    "# Mostrar el resultado\n",
    "globals()[f\"{sector}_quarterly_revisions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484e7df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #1E3B58; color: white; padding: 10px;\">\n",
    "    <h2>Create revision dataset (monthly)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a1a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302703a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Calcular la diferencia entre el último y el primer valor no NaN para cada columna, excepto 'year', 'ns_id' y 'date'\n",
    "revision = globals()[f\"{sector}_monthly_growth_rates\"].drop(columns=['year', 'id_ns', 'date']).apply(lambda x: x.loc[x.last_valid_index()] - x.loc[x.first_valid_index()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e6a3d6",
   "metadata": {},
   "source": [
    "#  Nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd75ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Crear un nuevo DataFrame con los resultados\n",
    "globals()[f\"{sector}_monthly_revisions\"] = pd.DataFrame({'revision_date': revision.index, f'{sector}_revision': revision.values})\n",
    "# Suponiendo que 'gdp_monthly_growth_rates' es tu DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "globals()[f\"{sector}_monthly_revisions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6897cc9",
   "metadata": {},
   "source": [
    "# Para graficar en python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea547e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extraer el mes y el año de la columna 'revision_date'\n",
    "globals()[f\"{sector}_monthly_revisions\"]['month'] = globals()[f\"{sector}_monthly_revisions\"]['revision_date'].str.split('_').str[0]\n",
    "globals()[f\"{sector}_monthly_revisions\"]['year'] = globals()[f\"{sector}_monthly_revisions\"]['revision_date'].str.split('_').str[1]\n",
    "\n",
    "# Mapear los nombres de los meses a sus respectivos números\n",
    "month_mapping = {\n",
    "    'ene': '01', 'feb': '02', 'mar': '03', 'abr': '04',\n",
    "    'may': '05', 'jun': '06', 'jul': '07', 'ago': '08',\n",
    "    'sep': '09', 'oct': '10', 'nov': '11', 'dic': '12'\n",
    "}\n",
    "\n",
    "globals()[f\"{sector}_monthly_revisions\"]['month'] = globals()[f\"{sector}_monthly_revisions\"]['month'].map(month_mapping)\n",
    "\n",
    "# Crear una nueva columna con la fecha en formato YYYY-MM-DD\n",
    "globals()[f\"{sector}_monthly_revisions\"]['revision_date'] = globals()[f\"{sector}_monthly_revisions\"]['year'] + '-' + globals()[f\"{sector}_monthly_revisions\"]['month']\n",
    "\n",
    "# Convertir la columna 'revision_date' a tipo de datos de fecha\n",
    "globals()[f\"{sector}_monthly_revisions\"]['revision_date'] = pd.to_datetime(globals()[f\"{sector}_monthly_revisions\"]['revision_date'], format='%Y-%m')\n",
    "\n",
    "# Eliminar columnas temporales 'month' y 'year'\n",
    "globals()[f\"{sector}_monthly_revisions\"].drop(['month', 'year'], axis=1, inplace=True)\n",
    "\n",
    "# Mostrar el resultado\n",
    "globals()[f\"{sector}_monthly_revisions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d089739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79df4c62",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda85bba",
   "metadata": {},
   "source": [
    "<div id=\"3-4\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d3554",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.4.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Loading SQL\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e2d194",
   "metadata": {},
   "source": [
    "# Growth Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Get environment variables\n",
    "user = os.environ.get('CIUP_SQL_USER')\n",
    "password = os.environ.get('CIUP_SQL_PASS')\n",
    "host = os.environ.get('CIUP_SQL_HOST')\n",
    "port = 5432\n",
    "database = 'gdp_revisions_datasets'\n",
    "\n",
    "# Check if all environment variables are defined\n",
    "if not all([host, user, password]):\n",
    "    raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "# Create connection string\n",
    "connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# gdp_monthly_growth_rates is the DataFrame you want to save to the database\n",
    "#gdp_annual_growth_rates.to_sql('gdp_annual_growth_rates_2013', engine, index=False, if_exists='replace')\n",
    "#gdp_quarterly_growth_rates.to_sql('gdp_quarterly_growth_rates', engine, index=False, if_exists='replace')\n",
    "#gdp_monthly_growth_rates.to_sql(f'{sector}_monthly_growth_rates', engine, index=False, if_exists='replace')\n",
    "\n",
    "# REVISIONES\n",
    "\n",
    "globals()[f\"{sector}_monthly_growth_rates\"].to_sql(f'{sector}_monthly_growth_rates', engine, index=False, if_exists='replace')\n",
    "#globals()[f\"{sector}_quarterly_growth_rates\"].to_sql(f'{sector}_quarterly_growth_rates', engine, index=False, if_exists='replace')\n",
    "#globals()[f\"{sector}_annual_growth_rates\"].to_sql(f'{sector}_annual_growth_rates', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb622ad1",
   "metadata": {},
   "source": [
    "# Revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132cb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Get environment variables\n",
    "user = os.environ.get('CIUP_SQL_USER')\n",
    "password = os.environ.get('CIUP_SQL_PASS')\n",
    "host = os.environ.get('CIUP_SQL_HOST')\n",
    "port = 5432\n",
    "database = 'gdp_revisions_datasets'\n",
    "\n",
    "# Check if all environment variables are defined\n",
    "if not all([host, user, password]):\n",
    "    raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "# Create connection string\n",
    "connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# gdp_monthly_growth_rates is the DataFrame you want to save to the database\n",
    "#gdp_annual_growth_rates.to_sql('gdp_annual_growth_rates_2013', engine, index=False, if_exists='replace')\n",
    "#gdp_quarterly_growth_rates.to_sql('gdp_quarterly_growth_rates', engine, index=False, if_exists='replace')\n",
    "#gdp_monthly_growth_rates.to_sql(f'{sector}_monthly_growth_rates', engine, index=False, if_exists='replace')\n",
    "\n",
    "# REVISIONES\n",
    "\n",
    "globals()[f\"{sector}_monthly_revisions\"].to_sql(f'{sector}_monthly_revisions', engine, index=False, if_exists='replace')\n",
    "#globals()[f\"{sector}_quarterly_revisions\"].to_sql(f'{sector}_quarterly_revisions', engine, index=False, if_exists='replace')\n",
    "#globals()[f\"{sector}_annual_revisions\"].to_sql(f'{sector}_annual_revisions', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443ebfd",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af48bc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15cd18b4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #3d30a2;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc920fd",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eae11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf105ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdp_revisions",
   "language": "python",
   "name": "gdp_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
