{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0ee00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0953333",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter bt pro roman'; color: rgb(0, 65, 75);\">\n",
    "<h1>\n",
    "Old GDP Revisions Datasets\n",
    "</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c323e5f0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter bt pro roman'; color: rgb(0, 65, 75);\">\n",
    "<h3>\n",
    "Documentation\n",
    "<br>\n",
    "____________________\n",
    "<br>\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc075bd7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'PT Serif Pro Book'; color: rgb(0, 65, 75); font-size: 16px;\">\n",
    "    Jason Cruz\n",
    "    <br>\n",
    "    <a href=\"mailto:jj.cruza@up.edu.pe\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">\n",
    "        jj.cruza@up.edu.pe\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0f23d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "This <span style=\"color: rgb(0, 65, 75);\">jupyter notebook</span> documents step-by-step the <b>construction of old datasets</b> for the project <b>'Revisions and Biases in Preliminary GDP Estimates in Peru'</b>.\n",
    "\n",
    "This jupyter notebook goes from the cleaning of the tables (Weekly Reports, WR) provided confidentially by the Central Bank to the construction of the <b>vintages</b> and <b>revisions</b> datasets from 1994-2024.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4addcdf2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;line-height: 1.5;\">\n",
    "<span style=\"font-size: 24px;\">&#128196;</span> The Weekly Report/<i>Nota Semanal</i> (WR/<i>NS</i>) of the Central Bank.\n",
    "    <br>\n",
    "    <span style=\"font-size: 24px;\">&#8987;</span> Available since <b>1994-2012</b> (Table 1) and since <b>1997-2012</b> (Table 2). \n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516ae6c",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Amaya; text-align: left; color: rgb(0, 65, 75); font-size:16px\">The following <b>outline is functional</b>. By utilising the provided buttons, users are able to enhance their experience by browsing this script.<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d69587",
   "metadata": {},
   "source": [
    "<div id=\"outilne\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15589700",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #292929; padding: 10px; line-height: 1.5; font-family: 'PT Serif Pro Book';\">\n",
    "    <h2 style=\"text-align: left; color: #E0E0E0;\">\n",
    "        Outline\n",
    "    </h2>\n",
    "    <br>\n",
    "    <a href=\"#libraries\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        Libraries</a>\n",
    "    <br>\n",
    "    <a href=\"#setup\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        Initial set-up</a>\n",
    "    <br>\n",
    "    <a href=\"#1\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        1. Duplicate tables for all other NS ids</a>\n",
    "    <br>\n",
    "    <a href=\"#1-1\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        1.1. Table 1.</a>\n",
    "    <br>\n",
    "    <a href=\"#1-2\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        1.2. Table 2.</a>\n",
    "    <br>\n",
    "    <a href=\"#2\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        2. Data cleaning</a>\n",
    "    <br>\n",
    "    <a href=\"#2-1\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        2.1. Extracting tables and data cleanup.</a>\n",
    "    <br>\n",
    "    <a href=\"#2-1-1\" style=\"color: #94FFD8; font-size: 14px; margin-left: 40px;\">\n",
    "        2.1.1. Table 1. Extraction and cleaning of data from tables on monthly real GDP growth rates.</a>\n",
    "    <br>\n",
    "    <a href=\"#2-1-2\" style=\"color: #94FFD8; font-size: 14px; margin-left: 40px;\">\n",
    "        2.1.2. Table 2. Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.</a>\n",
    "    <br>\n",
    "    <a href=\"#3\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">3. Real-time data of Peru's GDP growth rates</a>\n",
    "    <br>\n",
    "    <a href=\"#3-1\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        3.1. Annual vintages concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-2\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        3.2. Quarterly vintages concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-3\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        3.3. Monthly vintages concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-4\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        3.4. Loading SQL.</a>\n",
    "    <br>\n",
    "    <a href=\"#4\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">4. GDP final revision dataset</a>\n",
    "    <br>\n",
    "    <a href=\"#4-1\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        4.1. Annual revisions.</a>\n",
    "    <br>\n",
    "    <a href=\"#4-2\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        4.2. Quarterly revisions.</a>\n",
    "    <br>\n",
    "    <a href=\"#4-3\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        4.3. Monthly revisions.</a>\n",
    "    <br>\n",
    "    <a href=\"#5\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">5. Uploading data to SQL</a>\n",
    "    <br>\n",
    "    <a href=\"#5-1\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        5.1. Vintages.</a>\n",
    "    <br>\n",
    "    <a href=\"#5-2\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        5.2. Revisions.</a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a7c4d6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    Any questions or issues regarding the coding, please email Jason Cruz <a href=\"mailto:jj.cruza@alum.up.edu.pe\" style=\"color: rgb(0, 153, 123); text-decoration: none;\"><span style=\"font-size: 24px;\">&#x2709;</span>\n",
    "    </a>.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff8772",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    If you don't have the libraries below, please use the following code (as example) to install the required libraries.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f517f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea21974a",
   "metadata": {},
   "source": [
    "<div id=\"libraries\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752d8fe",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    <h2>\n",
    "    Libraries\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00415459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Duplicate tables for all other NS ids\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# 1.1. Table 1\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# 2. Data cleaning\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 2.2. Extracting tables and data cleanup\n",
    "\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import unicodedata  # For manipulating Unicode data\n",
    "import re  # For regular expressions operations\n",
    "from datetime import datetime  # For working with dates and times\n",
    "import locale  # For locale-specific formatting of numbers, dates, and currencies\n",
    "import numpy as np\n",
    "import unidecode\n",
    "\n",
    "# 2.2.1. Table 1. Extraction and cleaning of data from tables on monthly real GDP growth rates.\n",
    "\n",
    "import tabula  # Used to extract tables from PDF files into pandas DataFrames\n",
    "from tkinter import Tk, messagebox, TOP, YES, NO  # Used for creating graphical user interfaces\n",
    "from sqlalchemy import create_engine  # Used for connecting to and interacting with SQL databases\n",
    "\n",
    "# 2.2.2. Table 2. Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.\n",
    "\n",
    "import roman\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# 3. Real-time data of Peru's GDP growth rates\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import psycopg2  # For interacting with PostgreSQL databases\n",
    "from sqlalchemy import create_engine, text  # For creating and executing SQL queries using SQLAlchemy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8167e",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d73758",
   "metadata": {},
   "source": [
    "<div id=\"setup\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4fbf38",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark;\">\n",
    "    <h2>\n",
    "    Initial set-up\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8675b",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\"> Setting the base path. <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f382fef7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your base path to set your main working directory, then press enter: \n",
      "C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\Research Assistant\\CIUP\\GDP Revisions\\gdp_revisions\\old_database\n",
      "Correctly defined base path.\n"
     ]
    }
   ],
   "source": [
    "# Ask the user for the base path\n",
    "base_path = input(\"Please enter your base path to set your main working directory, then press enter: \\n\")\n",
    "\n",
    "# Check if the path is valid\n",
    "if os.path.isdir(base_path):\n",
    "    print(f\"Correctly defined base path.\")\n",
    "    os.chdir(base_path)\n",
    "else:\n",
    "    print(\"The entered path is not valid. Please try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16931de9",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    <span style=\"font-size: 24px; color: rgb(255, 32, 78); font-weight: bold;\">&#9888;</span>\n",
    "    Download the <code>.zip</code> files of <code>table_1</code> and <code>table_2</code> and paste them into <code>raw_data</code> and <code>input_data</code> paths set below. \n",
    "    <p>\n",
    "     Note that the missing data will be filled in <code>input_data</code> in <a href=\"#1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Section 1</a>. Then the changes will actually occur in the <code>input_data</code> folder. However, we created <code>raw_data</code> to map the tables as they were delivered by the Central Bank, i.e. the raw data delivered (without any transformation).\n",
    "       </p>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651a1eb",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\"> The following code lines will create folders in your current path, call them to import and export your outputs. <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a14ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save csv files delivered by Central Bank \n",
    "raw_data = 'raw_data' # to save raw data (.csv).\n",
    "if not os.path.exists(raw_data):\n",
    "    os.mkdir(raw_data) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbad78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save csv files delivered by Central Bank as inputs (duplicated for all NS id) \n",
    "input_data = 'input_data' # to save input data (.csv).\n",
    "if not os.path.exists(input_data):\n",
    "    os.mkdir(input_data) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7bc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save csv files delivered by Central Bank (Table 1)\n",
    "table_1_folder = os.path.join(input_data, 'table_1') # to save raw data (.pdf).\n",
    "if not os.path.exists(table_1_folder):\n",
    "    os.mkdir(table_1_folder) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cbf5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save csv files delivered by Central Bank (Table 2)\n",
    "table_2_folder = os.path.join(input_data, 'table_2') # to save raw data (.pdf).\n",
    "if not os.path.exists(table_2_folder):\n",
    "    os.mkdir(table_2_folder) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb31bc",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\"> The following code lines will create folders in your current path, call them to import and export your outputs. <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41a493",
   "metadata": {},
   "source": [
    "<p style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\"> The following function will establish a connection to the <code>gdp_revisions_datasets</code> database in <code>PostgreSQL</code>. The <b>input data</b> used in this jupyter notebook will be loaded from this <code>PostgreSQL</code> database, and similarly, all <b>output data</b> generated by this jupyter notebook will be stored in that database. Ensure that you set the necessary parameters to access the server once you have obtained the required permissions.<p/>\n",
    "    \n",
    "<p style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "To request permissions, please email Jason Cruz <a href=\"mailto:jj.cruza@alum.up.edu.pe\" style=\"color: rgb(0, 153, 123); text-decoration: none;\"> <span style=\"font-size: 24px;\">&#x2709;</span>\n",
    "    </a>.\n",
    "<p/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4457bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sqlalchemy_engine():\n",
    "    \"\"\"\n",
    "    Function to create an SQLAlchemy engine using environment variables.\n",
    "    \n",
    "    Returns:\n",
    "        engine: SQLAlchemy engine object.\n",
    "    \"\"\"\n",
    "    # Get environment variables\n",
    "    user = os.environ.get('CIUP_SQL_USER')  # Get the SQL user from environment variables\n",
    "    password = os.environ.get('CIUP_SQL_PASS')  # Get the SQL password from environment variables\n",
    "    host = os.environ.get('CIUP_SQL_HOST')  # Get the SQL host from environment variables\n",
    "    port = 5432  # Set the SQL port to 5432\n",
    "    database = 'gdp_revisions_datasets'  # Set the database name 'gdp_revisions_datasets' from SQL\n",
    "\n",
    "    # Check if all environment variables are defined\n",
    "    if not all([host, user, password]):\n",
    "        raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "    # Create connection string\n",
    "    connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    return engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8fa3d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"font-size: 24px; color: rgb(255, 32, 78); font-weight: bold;\">&#9888;</span>\n",
    "    <span style=\"font-family: PT Serif Pro Book; color: black; font-size: 16px;\">\n",
    "        Import all other functions required by this jupyter notebook.\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ecd38",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\"> Please, check the script <code>gdp_revisions_datasets_functions.py</code> which contains all the functions required by this jupyter notebook. The functions there are ordered according to the <a href=\"#outilne\" style=\"color: #3d30a2;\">sections</a> of this jupyter notebok.<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cda8404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from old_gdp_revisions_datasets_functions import *\n",
    "from gdp_revisions_datasets_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e10d5d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465bedbd",
   "metadata": {},
   "source": [
    "<div id=\"1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54b25f",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;; color: dark;\">1.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Duplicate tables for all other NS ids</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd1034",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    <p>The Central Bank has provided us with confidential data for Table 1 (1994-2012) and Table 2 (1997-2012). However, in each year we have been provided from the tables where the revisions occurred or, more generally, where the Central Bank has received from INEI the updated information of all NS tables.</p>\n",
    "    \n",
    "   <p>So, we will duplicate these tables to complete the total NS per year (50 approximately due to the number of weeks per year).</p>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9443ed",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "The following code imports the data containing the dummies of the tables that were delivered by the Central Bank (1 if the table was delivered and 0 if it was not) from SQL \n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb719841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to PostgresSQL\n",
    "engine = create_sqlalchemy_engine()\n",
    "\n",
    "# Define SQL query to import data\n",
    "query = f\"SELECT * FROM old_raw_data_delivered\"\n",
    "\n",
    "# Importing data into a pandas DataFrame\n",
    "df = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d5875",
   "metadata": {},
   "source": [
    "<div id=\"1-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5b4f6",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">1.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Table 1\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1686bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing file to duplicate for ns-01-2010.csv\n",
      "No existing file to duplicate for ns-02-2010.csv\n",
      "Duplicated ns-04-2010.csv to ns-05-2010.csv\n",
      "Duplicated ns-04-2010.csv to ns-06-2010.csv\n",
      "Duplicated ns-08-2010.csv to ns-09-2010.csv\n",
      "Duplicated ns-08-2010.csv to ns-10-2010.csv\n",
      "Duplicated ns-11-2010.csv to ns-12-2010.csv\n",
      "Duplicated ns-11-2010.csv to ns-13-2010.csv\n",
      "Duplicated ns-11-2010.csv to ns-14-2010.csv\n",
      "Duplicated ns-15-2010.csv to ns-16-2010.csv\n",
      "Duplicated ns-15-2010.csv to ns-17-2010.csv\n",
      "Duplicated ns-15-2010.csv to ns-18-2010.csv\n",
      "Duplicated ns-19-2010.csv to ns-20-2010.csv\n",
      "Duplicated ns-19-2010.csv to ns-21-2010.csv\n",
      "Duplicated ns-19-2010.csv to ns-22-2010.csv\n",
      "Duplicated ns-23-2010.csv to ns-24-2010.csv\n",
      "Duplicated ns-23-2010.csv to ns-25-2010.csv\n",
      "Duplicated ns-23-2010.csv to ns-26-2010.csv\n",
      "Duplicated ns-23-2010.csv to ns-27-2010.csv\n",
      "Duplicated ns-28-2010.csv to ns-29-2010.csv\n",
      "Duplicated ns-28-2010.csv to ns-30-2010.csv\n",
      "Duplicated ns-31-2010.csv to ns-32-2010.csv\n",
      "Duplicated ns-31-2010.csv to ns-33-2010.csv\n",
      "Duplicated ns-31-2010.csv to ns-34-2010.csv\n",
      "Duplicated ns-31-2010.csv to ns-35-2010.csv\n",
      "Duplicated ns-36-2010.csv to ns-37-2010.csv\n",
      "Duplicated ns-36-2010.csv to ns-38-2010.csv\n",
      "Duplicated ns-36-2010.csv to ns-39-2010.csv\n",
      "Duplicated ns-40-2010.csv to ns-41-2010.csv\n",
      "Duplicated ns-40-2010.csv to ns-42-2010.csv\n",
      "Duplicated ns-40-2010.csv to ns-43-2010.csv\n",
      "Duplicated ns-44-2010.csv to ns-45-2010.csv\n",
      "Duplicated ns-44-2010.csv to ns-46-2010.csv\n",
      "Duplicated ns-44-2010.csv to ns-47-2010.csv\n",
      "Duplicated ns-48-2010.csv to ns-49-2010.csv\n",
      "Duplicated ns-49-2010.csv to ns-01-2011.csv\n",
      "Duplicated ns-49-2010.csv to ns-02-2011.csv\n",
      "Duplicated ns-03-2011.csv to ns-04-2011.csv\n",
      "Duplicated ns-03-2011.csv to ns-05-2011.csv\n",
      "Duplicated ns-03-2011.csv to ns-06-2011.csv\n",
      "Duplicated ns-08-2011.csv to ns-09-2011.csv\n",
      "Duplicated ns-08-2011.csv to ns-10-2011.csv\n",
      "Duplicated ns-11-2011.csv to ns-12-2011.csv\n",
      "Duplicated ns-11-2011.csv to ns-13-2011.csv\n",
      "Duplicated ns-11-2011.csv to ns-14-2011.csv\n",
      "Duplicated ns-11-2011.csv to ns-15-2011.csv\n",
      "Duplicated ns-16-2011.csv to ns-17-2011.csv\n",
      "Duplicated ns-16-2011.csv to ns-18-2011.csv\n",
      "Duplicated ns-19-2011.csv to ns-20-2011.csv\n",
      "Duplicated ns-19-2011.csv to ns-21-2011.csv\n",
      "Duplicated ns-19-2011.csv to ns-22-2011.csv\n",
      "Duplicated ns-24-2011.csv to ns-25-2011.csv\n",
      "Duplicated ns-24-2011.csv to ns-26-2011.csv\n",
      "Duplicated ns-24-2011.csv to ns-27-2011.csv\n",
      "Duplicated ns-28-2011.csv to ns-29-2011.csv\n",
      "Duplicated ns-28-2011.csv to ns-30-2011.csv\n",
      "Duplicated ns-31-2011.csv to ns-32-2011.csv\n",
      "Duplicated ns-31-2011.csv to ns-33-2011.csv\n",
      "Duplicated ns-31-2011.csv to ns-34-2011.csv\n",
      "Duplicated ns-31-2011.csv to ns-35-2011.csv\n",
      "Duplicated ns-36-2011.csv to ns-37-2011.csv\n",
      "Duplicated ns-36-2011.csv to ns-38-2011.csv\n",
      "Duplicated ns-36-2011.csv to ns-39-2011.csv\n",
      "Duplicated ns-40-2011.csv to ns-41-2011.csv\n",
      "Duplicated ns-40-2011.csv to ns-42-2011.csv\n",
      "Duplicated ns-40-2011.csv to ns-43-2011.csv\n",
      "Duplicated ns-44-2011.csv to ns-45-2011.csv\n",
      "Duplicated ns-44-2011.csv to ns-46-2011.csv\n",
      "Duplicated ns-44-2011.csv to ns-47-2011.csv\n",
      "Duplicated ns-44-2011.csv to ns-48-2011.csv\n",
      "Duplicated ns-49-2011.csv to ns-01-2012.csv\n",
      "Duplicated ns-49-2011.csv to ns-02-2012.csv\n",
      "Duplicated ns-03-2012.csv to ns-04-2012.csv\n",
      "Duplicated ns-03-2012.csv to ns-05-2012.csv\n",
      "Duplicated ns-03-2012.csv to ns-06-2012.csv\n",
      "Duplicated ns-03-2012.csv to ns-07-2012.csv\n",
      "Duplicated ns-08-2012.csv to ns-09-2012.csv\n",
      "Duplicated ns-08-2012.csv to ns-10-2012.csv\n",
      "Duplicated ns-08-2012.csv to ns-11-2012.csv\n",
      "Duplicated ns-12-2012.csv to ns-13-2012.csv\n",
      "Duplicated ns-12-2012.csv to ns-14-2012.csv\n",
      "Duplicated ns-15-2012.csv to ns-16-2012.csv\n",
      "Duplicated ns-15-2012.csv to ns-17-2012.csv\n",
      "Duplicated ns-15-2012.csv to ns-18-2012.csv\n",
      "Duplicated ns-19-2012.csv to ns-20-2012.csv\n",
      "Duplicated ns-19-2012.csv to ns-21-2012.csv\n",
      "Duplicated ns-19-2012.csv to ns-22-2012.csv\n",
      "Duplicated ns-19-2012.csv to ns-23-2012.csv\n",
      "Duplicated ns-25-2012.csv to ns-26-2012.csv\n",
      "Duplicated ns-25-2012.csv to ns-27-2012.csv\n",
      "Duplicated ns-28-2012.csv to ns-29-2012.csv\n",
      "Duplicated ns-28-2012.csv to ns-30-2012.csv\n",
      "Duplicated ns-31-2012.csv to ns-32-2012.csv\n",
      "Duplicated ns-31-2012.csv to ns-33-2012.csv\n",
      "Duplicated ns-31-2012.csv to ns-34-2012.csv\n",
      "Duplicated ns-31-2012.csv to ns-35-2012.csv\n",
      "Duplicated ns-36-2012.csv to ns-37-2012.csv\n",
      "Duplicated ns-36-2012.csv to ns-38-2012.csv\n",
      "Duplicated ns-36-2012.csv to ns-39-2012.csv\n",
      "Duplicated ns-40-2012.csv to ns-41-2012.csv\n",
      "Duplicated ns-40-2012.csv to ns-42-2012.csv\n",
      "Duplicated ns-40-2012.csv to ns-43-2012.csv\n",
      "Duplicated ns-40-2012.csv to ns-44-2012.csv\n",
      "Duplicated ns-45-2012.csv to ns-46-2012.csv\n",
      "Duplicated ns-45-2012.csv to ns-47-2012.csv\n",
      "Duplicated ns-45-2012.csv to ns-48-2012.csv\n"
     ]
    }
   ],
   "source": [
    "# Process each year\n",
    "for year in range(2010, 2013): # Please change your preferred year range\n",
    "    df_year = df[df['year'] == year]\n",
    "    duplicate_files_table_1(year, df_year, table_1_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16bcdef",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866a9694",
   "metadata": {},
   "source": [
    "<div id=\"1-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff12be5",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">1.2.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Table 2\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e87957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated ns-45-2010.csv to ns-01-2011.csv\n",
      "Duplicated ns-45-2010.csv to ns-02-2011.csv\n",
      "Duplicated ns-45-2010.csv to ns-03-2011.csv\n",
      "Duplicated ns-45-2010.csv to ns-04-2011.csv\n",
      "Duplicated ns-45-2010.csv to ns-05-2011.csv\n",
      "Duplicated ns-45-2010.csv to ns-06-2011.csv\n",
      "Duplicated ns-45-2010.csv to ns-07-2011.csv\n",
      "Duplicated ns-08-2011.csv to ns-09-2011.csv\n",
      "Duplicated ns-08-2011.csv to ns-10-2011.csv\n",
      "Duplicated ns-11-2011.csv to ns-12-2011.csv\n",
      "Duplicated ns-11-2011.csv to ns-13-2011.csv\n",
      "Duplicated ns-11-2011.csv to ns-14-2011.csv\n",
      "Duplicated ns-11-2011.csv to ns-15-2011.csv\n",
      "Duplicated ns-11-2011.csv to ns-16-2011.csv\n",
      "Duplicated ns-11-2011.csv to ns-17-2011.csv\n",
      "Duplicated ns-11-2011.csv to ns-18-2011.csv\n",
      "Duplicated ns-11-2011.csv to ns-19-2011.csv\n",
      "Duplicated ns-20-2011.csv to ns-21-2011.csv\n",
      "Duplicated ns-20-2011.csv to ns-22-2011.csv\n",
      "Duplicated ns-23-2011.csv to ns-24-2011.csv\n",
      "Duplicated ns-23-2011.csv to ns-25-2011.csv\n",
      "Duplicated ns-23-2011.csv to ns-26-2011.csv\n",
      "Duplicated ns-23-2011.csv to ns-27-2011.csv\n",
      "Duplicated ns-23-2011.csv to ns-28-2011.csv\n",
      "Duplicated ns-23-2011.csv to ns-29-2011.csv\n",
      "Duplicated ns-23-2011.csv to ns-30-2011.csv\n",
      "Duplicated ns-23-2011.csv to ns-31-2011.csv\n",
      "Duplicated ns-32-2011.csv to ns-33-2011.csv\n",
      "Duplicated ns-32-2011.csv to ns-34-2011.csv\n",
      "Duplicated ns-32-2011.csv to ns-35-2011.csv\n",
      "Duplicated ns-32-2011.csv to ns-36-2011.csv\n",
      "Duplicated ns-32-2011.csv to ns-37-2011.csv\n",
      "Duplicated ns-32-2011.csv to ns-38-2011.csv\n",
      "Duplicated ns-32-2011.csv to ns-39-2011.csv\n",
      "Duplicated ns-32-2011.csv to ns-40-2011.csv\n",
      "Duplicated ns-32-2011.csv to ns-41-2011.csv\n",
      "Duplicated ns-32-2011.csv to ns-42-2011.csv\n",
      "Duplicated ns-32-2011.csv to ns-43-2011.csv\n",
      "Duplicated ns-32-2011.csv to ns-44-2011.csv\n",
      "Duplicated ns-45-2011.csv to ns-46-2011.csv\n",
      "Duplicated ns-45-2011.csv to ns-47-2011.csv\n",
      "Duplicated ns-45-2011.csv to ns-48-2011.csv\n",
      "Duplicated ns-45-2011.csv to ns-49-2011.csv\n"
     ]
    }
   ],
   "source": [
    "# Process each year\n",
    "for year in range(2011, 2012): # Please change your preferred year range\n",
    "    df_year = df[df['year'] == year]\n",
    "    duplicate_files_table_2(year, df_year, table_2_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710b388",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1baf20e",
   "metadata": {},
   "source": [
    "<div id=\"2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86728175",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;; color: dark;\">2.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Data cleaning</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56e8cc",
   "metadata": {},
   "source": [
    "<div id=\"2-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9f258",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">2.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Extracting tables and data cleanup\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84b6a7",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "PENDING\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbde93",
   "metadata": {},
   "source": [
    "<div id=\"2-1-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c9908",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">2.1.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    <span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">Table 1.</span> Extraction and cleaning of data from tables on monthly real GDP growth rates.\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245d93f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"font-size: 24px; color: rgb(255, 32, 78); font-weight: bold;\">&#9888;</span>\n",
    "    <span style=\"font-family: PT Serif Pro Book; color: black; font-size: 16px;\">\n",
    "        Please check that the flat file <b>\"ns_dates.csv\"</b> is updated with the dates, years and ids for the newly downloaded PDF <span style=\"font-size: 24px;\">&#128462;</span> (WR). That file is located in the <b>\"ns_dates\"</b> folder and is uploaded to SQL from the jupyeter notebook <code>aux_files_to_sql.ipynb</code>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b77b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ffb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the locale to Spanish\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Dictionary to store generated DataFrames\n",
    "old_dataframes_dict_1 = {}\n",
    "\n",
    "# Path for the processed folders log file\n",
    "record_path  = 'dataframes_record/old_processed_folders_1.txt'\n",
    "\n",
    "# Function to correct month names\n",
    "def correct_month_name(month):\n",
    "    months_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Add more mappings as needed for other month names\n",
    "    }\n",
    "    return months_mapping.get(month, month)\n",
    "\n",
    "# Function to register processed folder\n",
    "def register_processed_folder(folder, num_processed_files):\n",
    "    with open(record_path, 'a') as file:\n",
    "        file.write(f\"{folder}:{num_processed_files}\\n\")\n",
    "\n",
    "# Function to check if folder has been processed\n",
    "def folder_processed(folder):\n",
    "    if not os.path.exists(record_path):\n",
    "        return False\n",
    "    with open(record_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(folder):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Function to fetch date from database\n",
    "def get_date(df, engine):\n",
    "    id_ns = df['id_ns'].iloc[0]\n",
    "    year = df['year'].iloc[0]\n",
    "    query = f\"SELECT date FROM dates_growth_rates WHERE id_ns = '{id_ns}' AND year = '{year}';\"\n",
    "    date_result = pd.read_sql(query, engine)\n",
    "    return date_result.iloc[0, 0] if not date_result.empty else None\n",
    "\n",
    "def process_csv(csv_path, engine):\n",
    "    old_tables_dict_1 = {}  # Local dictionary for each CSV\n",
    "    table_counter = 1\n",
    "\n",
    "    filename = os.path.basename(csv_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No matches found for id_ns and year in filename:\", filename)\n",
    "        return None, None, None, None # Return None for tables_dict_1 as well\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(csv_path))[0].replace('-', '_')\n",
    "\n",
    "    df = pd.read_csv(csv_path, delimiter=';')\n",
    "    \n",
    "    dataframe_name = f\"{new_filename}_{table_counter}\"\n",
    "    old_tables_dict_1[dataframe_name] = df.copy()\n",
    "\n",
    "    # Apply cleanup functions to a copy of the DataFrame\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Cleaning functions (set as required)\n",
    "    df_clean = clean_column_names(df_clean)\n",
    "    df_clean = adjust_column_names(df_clean)\n",
    "    df_clean = drop_rare_caracter_row(df_clean)\n",
    "    df_clean = drop_nan_rows(df_clean)\n",
    "    df_clean = drop_nan_columns(df_clean)\n",
    "    df_clean = reset_index(df_clean)\n",
    "    df_clean = remove_digit_slash(df_clean)\n",
    "    df_clean = replace_var_perc_first_column(df_clean)\n",
    "    df_clean = replace_var_perc_last_columns(df_clean)\n",
    "    df_clean = replace_number_moving_average(df_clean)\n",
    "    df_clean = relocate_last_column(df_clean)\n",
    "    df_clean = clean_first_row(df_clean)\n",
    "    df_clean = find_year_column(df_clean)\n",
    "    year_columns = extract_years(df_clean)\n",
    "    df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "    df_clean = first_row_columns(df_clean)\n",
    "    df_clean = clean_columns_values(df_clean)\n",
    "    df_clean = convert_float(df_clean)\n",
    "    df_clean = replace_set_sep(df_clean)\n",
    "    df_clean = spaces_se_es(df_clean)\n",
    "    df_clean = replace_services(df_clean)\n",
    "    df_clean = rounding_values(df_clean, decimales=1)\n",
    "\n",
    "    # Add the column 'year' to the clean DataFrame\n",
    "    df_clean.insert(0, 'year', year)\n",
    "    \n",
    "    # Add the column 'id_ns' to the clean DataFrame\n",
    "    df_clean.insert(1, 'id_ns', id_ns)\n",
    "    \n",
    "    # Get corresponding date from database\n",
    "    date = get_date(df_clean, engine)\n",
    "    if date:\n",
    "        # Add 'date' column to cleaned DataFrame\n",
    "        df_clean.insert(2, 'date', date)\n",
    "    else:\n",
    "        print(\"Date not found in database for id_ns:\", id_ns, \"and year:\", year)\n",
    "\n",
    "    # Store cleaned DataFrame in old_dataframes_dict_1\n",
    "    old_dataframes_dict_1[dataframe_name] = df_clean\n",
    "\n",
    "    return id_ns, year, old_tables_dict_1\n",
    "\n",
    "# Function to process folder\n",
    "def process_folder(folder, engine):\n",
    "    print(f\"Processing folder {os.path.basename(folder)}\")\n",
    "    pdf_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.csv')]\n",
    "\n",
    "    num_csv_processed = 0\n",
    "    num_dataframes_generated = 0\n",
    "\n",
    "    table_counter = 1  # Initialize table counter here\n",
    "    old_tables_dict_1 = {}  # Declare old_tables_dict_1 outside main loop\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        id_ns, year, tables_dict_temp = process_csv(csv_file, engine)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for dataframe_name, df in tables_dict_temp.items():\n",
    "                file_name = os.path.splitext(os.path.basename(csv_file))[0].replace('-', '_')\n",
    "                dataframe_name = f\"{file_name}_{table_counter}\"\n",
    "                \n",
    "                # Store raw DataFrame in old_tables_dict_1\n",
    "                old_tables_dict_1[dataframe_name] = df.copy()\n",
    "                \n",
    "                # Apply cleaning functions to a copy of the DataFrame\n",
    "                df_clean = df.copy()\n",
    "                \n",
    "                # Cleaning functions (set as required)\n",
    "                df_clean = clean_column_names(df_clean)\n",
    "                df_clean = adjust_column_names(df_clean)\n",
    "                df_clean = drop_rare_caracter_row(df_clean)\n",
    "                df_clean = drop_nan_rows(df_clean)\n",
    "                df_clean = drop_nan_columns(df_clean)\n",
    "                df_clean = reset_index(df_clean)\n",
    "                df_clean = remove_digit_slash(df_clean)\n",
    "                df_clean = replace_var_perc_first_column(df_clean)\n",
    "                df_clean = replace_var_perc_last_columns(df_clean)\n",
    "                df_clean = replace_number_moving_average(df_clean)\n",
    "                df_clean = relocate_last_column(df_clean)\n",
    "                df_clean = clean_first_row(df_clean)\n",
    "                df_clean = find_year_column(df_clean)\n",
    "                year_columns = extract_years(df_clean)\n",
    "                df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "                df_clean = first_row_columns(df_clean)\n",
    "                df_clean = clean_columns_values(df_clean)\n",
    "                df_clean = convert_float(df_clean)\n",
    "                df_clean = replace_set_sep(df_clean)\n",
    "                df_clean = spaces_se_es(df_clean)\n",
    "                df_clean = replace_services(df_clean)\n",
    "                df_clean = rounding_values(df_clean, decimales=1)\n",
    "                \n",
    "                # Add the column 'year' to the clean DataFrame\n",
    "                df_clean.insert(0, 'year', year)\n",
    "\n",
    "                # Add the column 'id_ns' to the clean DataFrame\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "\n",
    "                # Get corresponding date from database\n",
    "                date = get_date(df_clean, engine)\n",
    "                if date:\n",
    "                    # Add 'date' column to cleaned DataFrame\n",
    "                    df_clean.insert(2, 'date', date)\n",
    "                else:\n",
    "                    print(\"Date not found in database for id_ns:\", id_ns, \"and year:\", year)\n",
    "\n",
    "                # Store cleaned DataFrame in old_dataframes_dict_1\n",
    "                old_dataframes_dict_1[dataframe_name] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. El dataframe generado para el archivo {csv_file} es: {nombre_df}')\n",
    "                num_dataframes_generated += 1\n",
    "                table_counter += 1  # Increment table counter here\n",
    "        \n",
    "        num_csv_processed += 1  # Increment number of processed CSV for each CSV in folder\n",
    "\n",
    "    return num_csv_processed, num_dataframes_generated, old_tables_dict_1\n",
    "\n",
    "def process_folders():\n",
    "    csv_folder = 'input_pdf'\n",
    "    folders = [os.path.join(csv_folder, d) for d in os.listdir(csv_folder) if os.path.isdir(os.path.join(base_folder, d)) and re.match(r'\\d{4}', d)]\n",
    "    \n",
    "    old_tables_dict_1 = {}  # Initialize old_tables_dict_1 aquí\n",
    "    \n",
    "    for folder in folders:\n",
    "        if folder_processed(folder):\n",
    "            print(f\"Folder {folder} has already been processed.\")\n",
    "            continue\n",
    "        \n",
    "        num_csv_processed, num_dataframes_generated, tables_dict_temp = process_folder(folder, engine)\n",
    "        \n",
    "        # Update old_tables_dict_1 with values returned from process_folder()\n",
    "        old_tables_dict_1.update(tables_dict_temp)\n",
    "        \n",
    "        register_processed_folder(folder, num_csv_processed)\n",
    "\n",
    "        # Ask user if they want to continue with next folder\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Ensure the messagebox is in front\n",
    "        message = f\"Process {folder} complete. Processed {num_pdfs_processed} PDF(s) and generated {num_dataframes_generated} DataFrame(s). Continue with next folder?\"\n",
    "        if not messagebox.askyesno(\"Continue?\", message):\n",
    "            break\n",
    "            \n",
    "    print(\"Processing completed for all folders.\")  # Add a message to indicate completion\n",
    "    \n",
    "    return old_tables_dict_1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    engine = create_sqlalchemy_engine()\n",
    "\n",
    "    old_tables_dict_1 = process_folders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda07520",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tables_dict_1['ns_03_2010_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dataframes_dict_1['ns_03_2010_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb917b",
   "metadata": {},
   "source": [
    "<div id=\"2-1-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d489b",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">2.1.2.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    <span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">Table 2.</span>Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21687b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"font-size: 24px; color: rgb(255, 32, 78); font-weight: bold;\">&#9888;</span>\n",
    "    <span style=\"font-family: PT Serif Pro Book; color: black; font-size: 16px;\">\n",
    "        Please check that the flat file <b>\"ns_dates.csv\"</b> is updated with the dates, years and ids for the newly downloaded PDF <span style=\"font-size: 24px;\">&#128462;</span> (WR). That file is located in the <b>\"ns_dates\"</b> folder and is uploaded to SQL from the jupyeter notebook <code>aux_files_to_sql.ipynb</code>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e73d42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab041892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import locale\n",
    "from tkinter import Tk, messagebox, TOP, YES, NO\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Establecer la localización en español\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Diccionario para almacenar los DataFrames generados\n",
    "old_dataframes_dict_2 = {}\n",
    "\n",
    "# Ruta del archivo de registro de carpetas procesadas\n",
    "registro_path = 'dataframes_record/old_carpetas_procesadas_2.txt'\n",
    "\n",
    "# Función para corregir los nombres de los meses\n",
    "def corregir_nombre_mes(mes):\n",
    "    meses_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Agrega más mapeos si es necesario para otros nombres de meses\n",
    "    }\n",
    "    return meses_mapping.get(mes, mes)\n",
    "\n",
    "def registrar_carpeta_procesada(carpeta, num_archivos_procesados):\n",
    "    with open(registro_path, 'a') as file:\n",
    "        file.write(f\"{carpeta}:{num_archivos_procesados}\\n\")\n",
    "\n",
    "def carpeta_procesada(carpeta):\n",
    "    if not os.path.exists(registro_path):\n",
    "        return False\n",
    "    with open(registro_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(carpeta):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def obtener_fecha(df, engine):\n",
    "    id_ns = df['id_ns'].iloc[0]\n",
    "    year = df['year'].iloc[0]\n",
    "    query = f\"SELECT date FROM dates_growth_rates WHERE id_ns = '{id_ns}' AND year = '{year}';\"\n",
    "    fecha = pd.read_sql(query, engine)\n",
    "    return fecha.iloc[0, 0] if not fecha.empty else None\n",
    "\n",
    "def procesar_archivo_csv(csv_path, engine):\n",
    "    old_tables_dict_2 = {}  # Diccionario local para cada archivo CSV\n",
    "    table_counter = 1\n",
    "\n",
    "    filename = os.path.basename(csv_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No se encontraron coincidencias para id_ns y year en el nombre del archivo:\", filename)\n",
    "        return None, None, None, None\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(csv_path))[0].replace('-', '_')\n",
    "\n",
    "    df = pd.read_csv(csv_path, delimiter=';')\n",
    "    \n",
    "    nombre_dataframe = f\"{new_filename}_{table_counter}\"\n",
    "    old_tables_dict_2[nombre_dataframe] = df.copy()\n",
    "\n",
    "    # Aplicar las funciones de limpieza a una copia del DataFrame\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Funciones de limpieza (ajustar según sea necesario)\n",
    "    df_clean = replace_total_with_year(df_clean)\n",
    "    df_clean = drop_nan_rows(df_clean)\n",
    "    year_columns = extract_years(df_clean)\n",
    "    df_clean = roman_arabic(df_clean)\n",
    "    df_clean = fix_duplicates(df_clean)\n",
    "    df_clean = relocate_last_column(df_clean)\n",
    "    df_clean = replace_first_row_nan(df_clean)\n",
    "    df_clean = clean_first_row(df_clean)\n",
    "    df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "    df_clean = reset_index(df_clean)\n",
    "    df_clean = first_row_columns(df_clean)\n",
    "    df_clean = clean_columns_values(df_clean)\n",
    "    df_clean = reset_index(df_clean)\n",
    "    df_clean = convert_float(df_clean)\n",
    "    df_clean = replace_set_sep(df_clean)\n",
    "    df_clean = spaces_se_es(df_clean)\n",
    "    df_clean = replace_services(df_clean)\n",
    "    df_clean = rounding_values(df_clean, decimales=1)\n",
    "\n",
    "    # Añadir la columna 'year' al DataFrame limpio\n",
    "    df_clean.insert(0, 'year', year)\n",
    "    \n",
    "    # Añadir la columna 'id_ns' al DataFrame limpio\n",
    "    df_clean.insert(1, 'id_ns', id_ns)\n",
    "    \n",
    "    # Obtener la fecha correspondiente de la base de datos\n",
    "    fecha = obtener_fecha(df_clean, engine)\n",
    "    if fecha:\n",
    "        # Añadir la columna 'date' al DataFrame limpio\n",
    "        df_clean.insert(2, 'date', fecha)\n",
    "    else:\n",
    "        print(\"No se encontró fecha en la base de datos para id_ns:\", id_ns, \"y year:\", year)\n",
    "    \n",
    "    # Almacenar DataFrame limpio en old_dataframes_dict_2\n",
    "    old_dataframes_dict_2[nombre_dataframe] = df_clean\n",
    "\n",
    "    return id_ns, year, old_tables_dict_2\n",
    "\n",
    "def procesar_carpeta(carpeta, engine):\n",
    "    print(f\"Procesando la carpeta {os.path.basename(carpeta)}\")\n",
    "    csv_files = [os.path.join(carpeta, f) for f in os.listdir(carpeta) if f.endswith('.csv')]\n",
    "\n",
    "    num_csv_procesados = 0\n",
    "    num_dataframes_generados = 0\n",
    "\n",
    "    table_counter = 1  # Inicializar el contador de tabla aquí\n",
    "    old_tables_dict_2 = {}  # Declarar old_tables_dict_2 fuera del bucle principal\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        id_ns, year, tables_dict_temp = procesar_archivo_csv(csv_file, engine)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for nombre_df, df in tables_dict_temp.items():\n",
    "                nombre_archivo = os.path.splitext(os.path.basename(csv_file))[0].replace('-', '_')\n",
    "                nombre_df = f\"{nombre_archivo}_{table_counter}\"\n",
    "                \n",
    "                # Almacenar DataFrame sin procesar en old_tables_dict_2\n",
    "                old_tables_dict_2[nombre_df] = df.copy()\n",
    "                \n",
    "                # Procesar y limpiar el DataFrame\n",
    "                df_clean = df.copy()\n",
    "                \n",
    "                # Aplicar las funciones de limpieza a una copia del DataFrame\n",
    "                df_clean = replace_total_with_year(df_clean)\n",
    "                df_clean = drop_nan_rows(df_clean)\n",
    "                year_columns = extract_years(df_clean)\n",
    "                df_clean = roman_arabic(df_clean)\n",
    "                df_clean = fix_duplicates(df_clean)\n",
    "                df_clean = relocate_last_column(df_clean)\n",
    "                df_clean = replace_first_row_nan(df_clean)\n",
    "                df_clean = clean_first_row(df_clean)\n",
    "                df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "                df_clean = reset_index(df_clean)\n",
    "                df_clean = first_row_columns(df_clean)\n",
    "                df_clean = clean_columns_values(df_clean)\n",
    "                df_clean = reset_index(df_clean)\n",
    "                df_clean = convert_float(df_clean)\n",
    "                df_clean = replace_set_sep(df_clean)\n",
    "                df_clean = spaces_se_es(df_clean)\n",
    "                df_clean = replace_services(df_clean)\n",
    "                df_clean = rounding_values(df_clean, decimales=1)\n",
    "                \n",
    "                # Añadir la columna 'year' al DataFrame limpio\n",
    "                df_clean.insert(0, 'year', year)\n",
    "                \n",
    "                # Añadir la columna 'id_ns' al DataFrame limpio\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                \n",
    "                # Obtener la fecha correspondiente de la base de datos\n",
    "                fecha = obtener_fecha(df_clean, engine)\n",
    "                if fecha:\n",
    "                    # Añadir la columna 'date' al DataFrame limpio\n",
    "                    df_clean.insert(2, 'date', fecha)\n",
    "                else:\n",
    "                    print(\"No se encontró fecha en la base de datos para id_ns:\", id_ns, \"y year:\", year)\n",
    "                \n",
    "                # Almacenar DataFrame limpio en old_dataframes_dict_2\n",
    "                old_dataframes_dict_2[nombre_df] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. El dataframe generado para el archivo {csv_file} es: {nombre_df}')\n",
    "                num_dataframes_generados += 1\n",
    "                table_counter += 1  # Incrementar el contador de tabla aquí\n",
    "        \n",
    "        num_csv_procesados += 1  # Incrementar el número de CSVs procesados por cada archivo en la carpeta\n",
    "\n",
    "    return num_csv_procesados, num_dataframes_generados, old_tables_dict_2\n",
    "\n",
    "def procesar_carpetas():\n",
    "    base_folder = r'C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\coding_training\\old_dataset\\raw_data\\tabla 2'\n",
    "    carpetas = [os.path.join(base_folder, d) for d in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, d)) and re.match(r'\\d{4}', d)]\n",
    "    \n",
    "    old_tables_dict_2 = {}  # Inicializar old_tables_dict_2 aquí\n",
    "    \n",
    "    for carpeta in carpetas:\n",
    "        if carpeta_procesada(carpeta):\n",
    "            print(f\"La carpeta {carpeta} ya ha sido procesada.\")\n",
    "            continue\n",
    "        \n",
    "        num_csv_procesados, num_dataframes_generados, tables_dict_temp = procesar_carpeta(carpeta, engine)\n",
    "        \n",
    "        # Actualizar old_tables_dict_2 con los valores devueltos de procesar_carpeta()\n",
    "        old_tables_dict_2.update(tables_dict_temp)\n",
    "        \n",
    "        registrar_carpeta_procesada(carpeta, num_csv_procesados)\n",
    "\n",
    "        # Preguntar al usuario si desea continuar con la siguiente carpeta\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Para asegurar que la ventana esté en primer plano\n",
    "        \n",
    "        mensaje = f\"Se han generado {num_dataframes_generados} dataframes en la carpeta {carpeta}. ¿Deseas continuar con la siguiente carpeta?\"\n",
    "        continuar = messagebox.askyesno(\"Continuar\", mensaje)\n",
    "        root.destroy()\n",
    "\n",
    "        if not continuar:\n",
    "            print(\"Procesamiento detenido por el usuario.\")\n",
    "            break  # Romper el bucle for si el usuario decide no continuar\n",
    "\n",
    "    print(\"Procesamiento completado para todas las carpetas.\")  # Add a message to indicate completion\n",
    "\n",
    "    return old_tables_dict_2  # Devolver old_tables_dict_2 al final de la función\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Get environment variables\n",
    "    user = os.environ.get('CIUP_SQL_USER')\n",
    "    password = os.environ.get('CIUP_SQL_PASS')\n",
    "    host = os.environ.get('CIUP_SQL_HOST')\n",
    "    port = 5432\n",
    "    database = 'gdp_revisions_datasets'\n",
    "\n",
    "    # Check if all environment variables are defined\n",
    "    if not all([host, user, password]):\n",
    "        raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "    # Create connection string\n",
    "    connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "    old_tables_dict_2 = procesar_carpetas()  # Capturar el valor devuelto de procesar_carpetas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tables_dict_2['ns_04_2010_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b05cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dataframes_dict_2['ns_32_2010_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6942ad4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7156504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdp_revisions",
   "language": "python",
   "name": "gdp_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
