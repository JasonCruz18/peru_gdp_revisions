{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd9a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a055b3b9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter'; color: rgb(0, 65, 75);\">\n",
    "    <h1>\n",
    "    GDP Revisions Datasets\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd88a4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter'; color: rgb(0, 65, 75);\">\n",
    "    <h4>\n",
    "        Documentation\n",
    "        <br>\n",
    "        ____________________\n",
    "            </br>\n",
    "    </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72f519",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color: dark;\">\n",
    "    This \n",
    "    <span style=\"color: rgb(61, 48, 162);\">jupyter notebook</span>\n",
    "    provides a step-by-step guide to <b>data building</b> regarding the project <b>'Revisiones y sesgos en las estimaciones preliminares del PBI en el Perú'</b>. The guide covers downloading PDF files containing tables with information on annual, quarterly, and monthly Peru's GDP growth rates (including sectoral GDP) and extracting this information into SQL tables. These data sets will be used for data analysis.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d22837",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter'; color: rgb(0, 65, 75);\">\n",
    "    Jason Cruz\n",
    "    <br>\n",
    "    <a href=\"mailto:jj.cruza@up.edu.pe\" style=\"color: rgb(0, 153, 123)\">\n",
    "        jj.cruza@up.edu.pe\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28540df2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Times New Roman; text-align: left; color: rgb(61, 48, 162)\">The provided outline is functional. Use the buttons to enhance the experience of this script.<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc04c9",
   "metadata": {},
   "source": [
    "<div id=\"outilne\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0fa13",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #141414; padding: 10px;\">\n",
    "<h2 style=\"text-align: left; font-family: 'charter'; color: #E0E0E0;\">\n",
    "    Outline\n",
    "    </h2>\n",
    "    <br>\n",
    "    <a href=\"#1\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        1. PDF Downloader</a>\n",
    "    <br>\n",
    "    <a href=\"#2\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        2. Extracting Tables (and data cleaning)</a>\n",
    "    <br>\n",
    "    <a href=\"#2-1\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        2.1. 'pdfplumber' demo.</a>\n",
    "    <br>\n",
    "    <a href=\"#2-1-1\" style=\"color: #E0E0E0; font-size: 12px;\">\n",
    "        2.1.1. What data would we get if we used the default settings?.</a>   \n",
    "    <br>\n",
    "    <a href=\"#2-1-2\" style=\"color: #E0E0E0; font-size: 12px;\">\n",
    "        2.1.2. Using custom '.extract_table' settings.</a>\n",
    "    <br> \n",
    "    <a href=\"#2-2\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        2.2. Extracting tables and generating dataframes (includes data cleanup).</a>\n",
    "    <br>\n",
    "    <a href=\"#3\" style=\"color: #687EFF; font-size: 18px;\">3. SQL Tables</a>\n",
    "    <br>\n",
    "    <a href=\"#3-1\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.1. Annual Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-2\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.2. Quarterly Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-3\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.3. Monthly Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-4\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.4. Loading SQL.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90323106",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    Any questions or issues regarding the coding, please <a href=\"mailto:jj.cruza@alum.up.edu.pe\" style=\"color: rgb(0, 153, 123)\">email Jason Cruz\n",
    "    </a>.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac40ed0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    If you don't have the libraries below, please use the following code (as example) to install the required libraries.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c73193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907046a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    <h2>\n",
    "    Libraries\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214f5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Downloader\n",
    "\n",
    "import os  # for file and directory manipulation\n",
    "import random  # to generate random numbers\n",
    "import time  # to manage time and take breaks in the script\n",
    "import requests  # to make HTTP requests to web servers\n",
    "from selenium import webdriver  # for automating web browsers\n",
    "from selenium.webdriver.common.by import By  # to locate elements on a webpage\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # to wait until certain conditions are met on a webpage.\n",
    "from selenium.webdriver.support import expected_conditions as EC  # to define expected conditions\n",
    "from selenium.common.exceptions import StaleElementReferenceException  # To handle exceptions related to elements on the webpage that are no longer available.\n",
    "\n",
    "\n",
    "# Extracting Tables (and data cleaning)\n",
    "\n",
    "import pdfplumber  # for extracting text and metadata from PDF files\n",
    "import pandas as pd  # for data manipulation and analysis\n",
    "import os  # for interacting with the operating system\n",
    "import unicodedata  # for manipulating Unicode data\n",
    "import re  # for regular expressions operations\n",
    "from datetime import datetime  # for working with dates and times\n",
    "import locale  # for locale-specific formatting of numbers, dates, and currencies\n",
    "\n",
    "\n",
    "# SQL tables\n",
    "\n",
    "import psycopg2  # for interacting with PostgreSQL databases\n",
    "from sqlalchemy import create_engine, text  # for creating and executing SQL queries using SQLAlchemy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ef8f8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    <h2>\n",
    "    Initial set-up\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc1c80",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\"> The next 3 code lines will create folders in your current path, call them to import and export your outputs. <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8899a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to download PDF files\n",
    "\n",
    "raw_pdf = 'raw_pdf' # to save raw data (.pdf).\n",
    "if not os.path.exists(raw_pdf):\n",
    "    os.mkdir(raw_pdf) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26b4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save text file with the names of already downloaded files\n",
    "\n",
    "download_record = 'download_record'\n",
    "if not os.path.exists(download_record):\n",
    "    os.mkdir(download_record) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147227ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to download the trimmed PDF files (these are PDF inputs for the extraction and cleanup code)\n",
    "\n",
    "input_pdf = 'input_pdf'\n",
    "if not os.path.exists(input_pdf):\n",
    "    os.makedirs(input_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6dc316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to download the trimmed PDF files (these are PDF inputs for the extraction and cleanup code)\n",
    "\n",
    "trimmed_record = 'trimmed_record'\n",
    "if not os.path.exists(trimmed_record):\n",
    "    os.makedirs(trimmed_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0038d2",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7cf410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba6e051c",
   "metadata": {},
   "source": [
    "<div id=\"1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f3192",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: charter;\">1.</span> <span style = \"color: dark; font-family: charter;\">PDF Downloader</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98250a64",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Our main source for data collection is the <a href=\"https://www.bcrp.gob.pe/\" style=\"color: rgb(0, 153, 123)\">BCRP's web page</a> (.../publicaciones/nota-semanal). The BCRP publishes \"Notas Semanales\", documents that contain, among other information, tables of GDP and sectoral GDP growth rate values for annual, quarterly and monthly frequencies.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3a388",
   "metadata": {},
   "source": [
    "-- (pending) Selenium tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcc4e3",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    The provided code will download all the 'Notas Semanales' files in PDF format from this web page.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the BCRP URL\n",
    "bcrp_url = \"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\"  # Never replace this URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269c03d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    The provided code will download all the 'Notas Semanales' files in PDF format from this web page.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68363e06",
   "metadata": {},
   "source": [
    "# Con ventana input al usuario y alarma por cada lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0199bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pygame\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Inicializar pygame\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Carpeta donde se almacenarán los archivos de sonido\n",
    "sound_folder = \"sound\"\n",
    "\n",
    "# Lista de archivos de sonido disponibles\n",
    "available_sounds = os.listdir(sound_folder)\n",
    "\n",
    "# Seleccionar un sonido aleatorio\n",
    "random_sound = random.choice(available_sounds)\n",
    "\n",
    "# Ruta completa del sonido aleatorio\n",
    "sound_path = os.path.join(sound_folder, random_sound)\n",
    "\n",
    "# Cargar el sonido seleccionado\n",
    "pygame.mixer.music.load(sound_path)\n",
    "\n",
    "# Función para reproducir el sonido\n",
    "def play_sound():\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "# List to keep track of successfully downloaded files\n",
    "downloaded_files = []\n",
    "\n",
    "# Folder where downloaded PDF files will be saved\n",
    "raw_pdf = \"raw_pdf\"  # Replace with the actual path\n",
    "\n",
    "# Folder where the download record file will be saved\n",
    "download_record = \"download_record\"  # Replace with the actual path\n",
    "\n",
    "# Load the list of previously downloaded files if it exists\n",
    "if os.path.exists(os.path.join(download_record, \"downloaded_files.txt\")):\n",
    "    with open(os.path.join(download_record, \"downloaded_files.txt\"), \"r\") as f:\n",
    "        downloaded_files = f.read().splitlines()\n",
    "\n",
    "# Web driver setup\n",
    "driver_path = os.environ.get('driver_path')\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "def random_wait(min_time, max_time):\n",
    "    wait_time = random.uniform(min_time, max_time)\n",
    "    print(f\"Waiting randomly for {wait_time:.2f} seconds\")\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "def download_pdf(pdf_link):\n",
    "    # Click the link using JavaScript\n",
    "    driver.execute_script(\"arguments[0].click();\", pdf_link)\n",
    "\n",
    "    # Wait for the new page to fully open (adjust timing as necessary)\n",
    "    wait.until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "    # Switch to the new window or tab\n",
    "    windows = driver.window_handles\n",
    "    driver.switch_to.window(windows[1])\n",
    "\n",
    "    # Get the current URL (may vary based on site-specific logic)\n",
    "    new_url = driver.current_url\n",
    "    print(f\"{download_counter}. New URL: {new_url}\")\n",
    "\n",
    "    # Get the file name from the URL\n",
    "    file_name = new_url.split(\"/\")[-1]\n",
    "\n",
    "    # Form the full destination path\n",
    "    destination_path = os.path.join(raw_pdf, file_name)\n",
    "\n",
    "    # Download the PDF\n",
    "    response = requests.get(new_url, stream=True)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Save the PDF content to the local file\n",
    "        with open(destination_path, 'wb') as pdf_file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                pdf_file.write(chunk)\n",
    "\n",
    "        print(f\"PDF downloaded successfully at: {destination_path}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error downloading the PDF. Response code: {response.status_code}\")\n",
    "\n",
    "    # Close the new window or tab\n",
    "    driver.close()\n",
    "\n",
    "    # Switch back to the main window\n",
    "    driver.switch_to.window(windows[0])\n",
    "\n",
    "# Number of downloads per batch\n",
    "downloads_per_batch = 5\n",
    "# Total number of downloads\n",
    "total_downloads = 25\n",
    "\n",
    "try:\n",
    "    # Open the test page\n",
    "    driver.get(bcrp_url)\n",
    "    print(\"Site opened successfully\")\n",
    "\n",
    "    # Wait for the container area to be present\n",
    "    wait = WebDriverWait(driver, 60)\n",
    "    container_area = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"rightside\"]')))\n",
    "\n",
    "    # Get all the links within the container area\n",
    "    pdf_links = container_area.find_elements(By.XPATH, './/a')\n",
    "\n",
    "    # Reverse the order of links\n",
    "    pdf_links = list(reversed(pdf_links))\n",
    "\n",
    "    # Initialize download counter\n",
    "    download_counter = 0\n",
    "\n",
    "    # Iterate over reversed links and download PDFs in batches\n",
    "    for pdf_link in pdf_links:\n",
    "        download_counter += 1\n",
    "\n",
    "        # Get the file name from the URL\n",
    "        new_url = pdf_link.get_attribute(\"href\")\n",
    "        file_name = new_url.split(\"/\")[-1]\n",
    "\n",
    "        # Check if the file has already been downloaded\n",
    "        if file_name in downloaded_files:\n",
    "            print(f\"{download_counter}. The file {file_name} has already been downloaded previously. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Try to download the file\n",
    "        try:\n",
    "            download_pdf(pdf_link)\n",
    "\n",
    "            # Update the list of downloaded files\n",
    "            downloaded_files.append(file_name)\n",
    "\n",
    "            # Save the file name in the record\n",
    "            with open(os.path.join(download_record, \"downloaded_files.txt\"), \"a\") as f:\n",
    "                f.write(file_name + \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading the file {file_name}: {str(e)}\")\n",
    "\n",
    "        # If the download count reaches a multiple of batch size, notify\n",
    "        if download_counter % downloads_per_batch == 0:\n",
    "            print(f\"Batch {download_counter // downloads_per_batch} of {downloads_per_batch} completed\")\n",
    "\n",
    "        # If the download count reaches a multiple of 25, ask the user if they want to continue\n",
    "        if download_counter % 25 == 0:\n",
    "            play_sound()\n",
    "            user_input = input(\"Do you want to continue downloading? (Enter 's' to continue, any other key to stop): \")\n",
    "            pygame.mixer.music.stop()\n",
    "            if user_input.lower() != 's':\n",
    "                break\n",
    "\n",
    "        # Random wait before the next iteration\n",
    "        random_wait(5, 10)\n",
    "\n",
    "        # If total downloads reached, break out of loop\n",
    "        if download_counter == total_downloads:\n",
    "            print(f\"All downloads completed ({total_downloads} in total)\")\n",
    "            break\n",
    "\n",
    "except StaleElementReferenceException:\n",
    "    print(\"StaleElementReferenceException occurred. Retrying...\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser when finished\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168995d",
   "metadata": {},
   "source": [
    "### Ordenando pdf por años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee016a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Obtener la lista de archivos en el directorio\n",
    "archivos = os.listdir(raw_pdf)\n",
    "\n",
    "# Iterar sobre cada archivo\n",
    "for archivo in archivos:\n",
    "    # Obtener el año del nombre del archivo\n",
    "    nombre, extension = os.path.splitext(archivo)\n",
    "    año = None\n",
    "    partes_nombre = nombre.split('-')\n",
    "    for parte in partes_nombre:\n",
    "        if parte.isdigit() and len(parte) == 4:\n",
    "            año = parte\n",
    "            break\n",
    "\n",
    "    # Si se encontró el año, mover el archivo a la carpeta correspondiente\n",
    "    if año:\n",
    "        carpeta_destino = os.path.join(raw_pdf, año)\n",
    "        # Crear la carpeta si no existe\n",
    "        if not os.path.exists(carpeta_destino):\n",
    "            os.makedirs(carpeta_destino)\n",
    "        # Mover el archivo a la carpeta destino\n",
    "        shutil.move(os.path.join(raw_pdf, archivo), carpeta_destino)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d24d2",
   "metadata": {},
   "source": [
    "# Recortando PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd2370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import tkinter as tk\n",
    "\n",
    "# Rutas de directorios\n",
    "trimmed_record_dir = 'trimmed_record'\n",
    "trimmed_record_file = 'trimmed_files.txt'\n",
    "\n",
    "class PopupWindow(tk.Toplevel):\n",
    "    def __init__(self, root, message):\n",
    "        super().__init__(root)\n",
    "        self.root = root\n",
    "        self.title(\"Atención!\")\n",
    "        self.message = message\n",
    "        self.result = None\n",
    "        self.configure_window()\n",
    "        self.create_widgets()\n",
    "\n",
    "    def configure_window(self):\n",
    "        self.resizable(False, False)  # Evita cambiar el tamaño de la ventana\n",
    "\n",
    "    def create_widgets(self):\n",
    "        self.label = tk.Label(self, text=self.message, wraplength=250)  # Ajusta el texto si es demasiado largo\n",
    "        self.label.pack(pady=10, padx=10)\n",
    "        self.btn_frame = tk.Frame(self)\n",
    "        self.btn_frame.pack(pady=5)\n",
    "        self.btn_yes = tk.Button(self.btn_frame, text=\"Sí\", command=self.yes)\n",
    "        self.btn_yes.pack(side=tk.LEFT, padx=5)\n",
    "        self.btn_no = tk.Button(self.btn_frame, text=\"No\", command=self.no)\n",
    "        self.btn_no.pack(side=tk.RIGHT, padx=5)\n",
    "\n",
    "        # Calcula el tamaño de la ventana en función del tamaño del texto\n",
    "        width = self.label.winfo_reqwidth() + 20\n",
    "        height = self.label.winfo_reqheight() + 100\n",
    "        self.geometry(f\"{width}x{height}\")\n",
    "\n",
    "    def yes(self):\n",
    "        self.result = True\n",
    "        self.destroy()\n",
    "\n",
    "    def no(self):\n",
    "        self.result = False\n",
    "        self.destroy()\n",
    "\n",
    "def search_keywords(pdf_file, keywords):\n",
    "    pages_with_keywords = []\n",
    "    with fitz.open(pdf_file) as doc:\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc.load_page(page_num)\n",
    "            text = page.get_text()\n",
    "            if any(keyword in text for keyword in keywords):\n",
    "                pages_with_keywords.append(page_num)\n",
    "    return pages_with_keywords\n",
    "\n",
    "def trim_pdf(pdf_file, pages):\n",
    "    if not pages:\n",
    "        print(f\"No se encontraron páginas con palabras clave en {pdf_file}\")\n",
    "        return 0\n",
    "    \n",
    "    new_pdf_file = os.path.join(input_pdf, os.path.basename(pdf_file))\n",
    "    \n",
    "    with fitz.open(pdf_file) as doc:\n",
    "        new_doc = fitz.open()\n",
    "        new_doc.insert_pdf(doc, from_page=0, to_page=0)\n",
    "        for page_num in pages:\n",
    "            new_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)\n",
    "        new_doc.save(new_pdf_file)\n",
    "    \n",
    "    num_pages_new_pdf = new_doc.page_count\n",
    "    print(f\"El PDF recortado '{new_pdf_file}' tiene {num_pages_new_pdf} páginas.\")\n",
    "\n",
    "    if num_pages_new_pdf == 5:\n",
    "        final_doc = fitz.open()\n",
    "        final_doc.insert_pdf(new_doc, from_page=0, to_page=0)\n",
    "        final_doc.insert_pdf(new_doc, from_page=1, to_page=1)\n",
    "        final_doc.insert_pdf(new_doc, from_page=3, to_page=3)\n",
    "        final_doc.save(new_pdf_file)\n",
    "\n",
    "        num_pages_new_pdf = final_doc.page_count\n",
    "        print(f\"Solo se conservaron la portada y las páginas con 2 tablas de interés en el PDF recortado '{new_pdf_file}'.\")\n",
    "    else:\n",
    "        print(f\"Se conservaron todas las páginas en el PDF recortado '{new_pdf_file}'.\")\n",
    "\n",
    "    return num_pages_new_pdf\n",
    "\n",
    "def read_trimmed_files():\n",
    "    trimmed_files_path = os.path.join(trimmed_record_dir, trimmed_record_file)\n",
    "    if not os.path.exists(trimmed_files_path):\n",
    "        return set()\n",
    "    \n",
    "    with open(trimmed_files_path, 'r') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "def write_trimmed_files(trimmed_files):\n",
    "    trimmed_files_path = os.path.join(trimmed_record_dir, trimmed_record_file)\n",
    "    sorted_filenames = sorted(trimmed_files)  # Sort the filenames\n",
    "    with open(trimmed_files_path, 'w') as file:\n",
    "        for filename in sorted_filenames:\n",
    "            file.write(filename + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = [\"ECONOMIC SECTORS\"]\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Oculta la ventana principal de Tkinter\n",
    "\n",
    "    trimmed_files = read_trimmed_files()\n",
    "    processing_counter = 1\n",
    "\n",
    "    for folder in os.listdir(raw_pdf):\n",
    "        folder_path = os.path.join(raw_pdf, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(\"Procesando carpeta:\", folder)\n",
    "            num_pdfs_trimmed = 0\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".pdf\"):\n",
    "                    pdf_file = os.path.join(folder_path, filename)\n",
    "                    if filename in trimmed_files:\n",
    "                        print(f\"{processing_counter}. El PDF '{filename}' ya ha sido recortado y guardado en '{raw_pdf}'...\")\n",
    "                        processing_counter += 1\n",
    "                        continue\n",
    "                    print(f\"{processing_counter}. Procesando:\", pdf_file)\n",
    "                    \n",
    "                    pages_with_keywords = search_keywords(pdf_file, keywords)\n",
    "                    num_pages_new_pdf = trim_pdf(pdf_file, pages_with_keywords)\n",
    "                    if num_pages_new_pdf > 0:\n",
    "                        num_pdfs_trimmed += 1\n",
    "                        trimmed_files.add(filename)\n",
    "                        processing_counter += 1\n",
    "            \n",
    "            write_trimmed_files(trimmed_files)\n",
    "\n",
    "            message = f\"{num_pdfs_trimmed} PDFs han sido recortados en la carpeta {folder}. ¿Desea continuar?\"\n",
    "            popup = PopupWindow(root, message)\n",
    "            root.wait_window(popup)\n",
    "            if not popup.result:\n",
    "                break\n",
    "                \n",
    "    print(\"Proceso completado para todos los PDFs en el directorio:\", input_pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643079a",
   "metadata": {},
   "source": [
    "### Ordenando pdf por años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Obtener la lista de archivos en el directorio\n",
    "archivos = os.listdir(input_pdf)\n",
    "\n",
    "# Iterar sobre cada archivo\n",
    "for archivo in archivos:\n",
    "    # Obtener el año del nombre del archivo\n",
    "    nombre, extension = os.path.splitext(archivo)\n",
    "    año = None\n",
    "    partes_nombre = nombre.split('-')\n",
    "    for parte in partes_nombre:\n",
    "        if parte.isdigit() and len(parte) == 4:\n",
    "            año = parte\n",
    "            break\n",
    "\n",
    "    # Si se encontró el año, mover el archivo a la carpeta correspondiente\n",
    "    if año:\n",
    "        carpeta_destino = os.path.join(input_pdf, año)\n",
    "        # Crear la carpeta si no existe\n",
    "        if not os.path.exists(carpeta_destino):\n",
    "            os.makedirs(carpeta_destino)\n",
    "        # Mover el archivo a la carpeta destino\n",
    "        shutil.move(os.path.join(input_pdf, archivo), carpeta_destino)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045beb7b",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a384d2",
   "metadata": {},
   "source": [
    "<div id=\"2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e1e92",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.</span> <span style = \"color: dark; font-family: charter;\">Extracting Tables (and data cleaning)</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ed1db",
   "metadata": {},
   "source": [
    "<div id=\"2-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af999f",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.1.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        pdfplumber\n",
    "    </span> \n",
    "    demo\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebeb3a",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Import\n",
    "    <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        pdfplumber\n",
    "    </span>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "print(f'This library version is: {pdfplumber.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b481d4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Load the PDF\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(\".\\\\ns-10-2013.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad376ace",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Get the page 82\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_82 = pdf.pages[81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33facebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the page to a higher resolution image (e.g., 300 DPI).\n",
    "image = p_82.to_image(resolution=300)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629ee4d",
   "metadata": {},
   "source": [
    "<div id=\"2-1-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49929c",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.1.1.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    What data would we get if we used the default settings?\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ea8f5e",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    We can check by using <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        PageImage.debug_tablefinder()\n",
    "    </span>:\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd775601",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.reset().debug_tablefinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ae18d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    The default settings correctly identify the table's vertical demarcations, but don't capture the horizontal demarcations between each group of five states/territories. So:\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b70f0",
   "metadata": {},
   "source": [
    "<div id=\"2-1-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8947130a",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.1.2.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Using custom <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        <b>.extract_table\n",
    "            </b>\n",
    "    </span>'s settings\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c473c71b",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    <ul>\n",
    "        <li>Because the columns are separated by lines, we use <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        vertical_strategy=\"lines\"\n",
    "    </span>.\n",
    "            </li>\n",
    "        <li>Because the rows are, primarily, separated by gutters between the text, we use <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        horizontal_strategy=\"text\"\n",
    "    </span>.\n",
    "            <li>To snap together a handful of the gutters at the top which aren't fully flush with one another, we use <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        snap_y_tolerance\n",
    "    </span>which snaps horizontal lines within a certain distance to the same vertical alignment.\n",
    "                </li>\n",
    "        <li>And because the left and right-hand extremities of the text aren't quite flush with the vertical lines, we use <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        \"intersection_tolerance\": 15\n",
    "    </span>.\n",
    "            </li>\n",
    "        </ul>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_settings = {\n",
    "    \"vertical_strategy\": \"lines\", \n",
    "    \"horizontal_strategy\": \"lines\",\n",
    "    \"explicit_vertical_lines\": [],\n",
    "    \"explicit_horizontal_lines\": [],\n",
    "    \"snap_tolerance\": 3,\n",
    "    \"snap_x_tolerance\": 3,\n",
    "    \"snap_y_tolerance\": 3,\n",
    "    \"join_tolerance\": 3,\n",
    "    \"join_x_tolerance\": 3,\n",
    "    \"join_y_tolerance\": 3,\n",
    "    \"edge_min_length\": 3,\n",
    "    \"min_words_vertical\": 3,\n",
    "    \"min_words_horizontal\": 1,\n",
    "    \"text_keep_blank_chars\": False,\n",
    "    \"text_tolerance\": 3,\n",
    "    \"text_x_tolerance\": 3,\n",
    "    \"text_y_tolerance\": 3,\n",
    "    \"intersection_tolerance\": 3,\n",
    "    \"intersection_x_tolerance\": 3,\n",
    "    \"intersection_y_tolerance\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c25c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.reset().debug_tablefinder(table_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2093bf",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5a767",
   "metadata": {},
   "source": [
    "<div id=\"2-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357318e8",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.2.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Extracting tables and generating dataframes (includes data cleanup)\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100857d4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    We would like to get specific tables: information on GDP growth rates with annual, quarterly and monthly frequency. We don't need other tables also related to GDP that don't meet these requirements. Extraction will be easier if we use keywords.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77729e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to search in the page text\n",
    "keywords = [\"PRODUCTO BRUTO INTERNO\", \"SECTORES ECONÓMICOS\", \"PBI\", \"GDP\", \"Variaciones\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070fb47",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    The code iterates through each PDF and extracts the two required tables from each. The extracted information is then transformed into dataframes and the columns and values are cleaned up to conform to Python conventions (pythonic).\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af4d90",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df05629",
   "metadata": {},
   "source": [
    "### Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f4190e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_characters_first_row(texto):\n",
    "    texto = re.sub(r'\\s*-\\s*', '-', texto)  # Remueve espacios alrededor de guiones\n",
    "    texto = re.sub(r'[^a-zA-Z0-9\\s-]', '', texto)  # Remueve caracteres raros excepto letras, dígitos y guiones\n",
    "    return texto\n",
    "\n",
    "def remove_rare_characters(texto):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', texto)\n",
    "\n",
    "def remove_tildes(texto):\n",
    "    return ''.join((c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8eb2f",
   "metadata": {},
   "source": [
    "### Común"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25bf119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.\n",
    "def drop_nan_rows(df):\n",
    "    df = df.dropna(how='all')\n",
    "    return df\n",
    "\n",
    "# 1. \n",
    "def drop_nan_columns(df):\n",
    "    return df.dropna(axis=1, how='all')\n",
    "\n",
    "# 2.\n",
    "def swap_first_second_row(df):\n",
    "    temp = df.iloc[0, 0]\n",
    "    df.iloc[0, 0] = df.iloc[1, 0]\n",
    "    df.iloc[1, 0] = temp\n",
    "\n",
    "    temp = df.iloc[0, -1]\n",
    "    df.iloc[0, -1] = df.iloc[1, -1]\n",
    "    df.iloc[1, -1] = temp\n",
    "    return df\n",
    "\n",
    "# 8. \n",
    "def reset_index(df):\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "# 5.\n",
    "def remove_digit_slash(df):\n",
    "    # Aplica la función de reemplazo a la primera columna y a las dos últimas columnas\n",
    "    df.iloc[:, [0, -2, -1]] = df.iloc[:, [0, -2, -1]].apply(lambda x: x.str.replace(r'\\d+/', '', regex=True))\n",
    "    return df\n",
    "\n",
    "# 9. AUX (ROBUSTO)\n",
    "\n",
    "def separate_text_digits(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if any(char.isdigit() for char in str(row.iloc[-2])) and any(char.isalpha() for char in str(row.iloc[-2])):\n",
    "            if pd.isnull(row.iloc[-1]):\n",
    "                df.loc[index, df.columns[-1]] = ''.join(filter(lambda x: x.isalpha() or x == ' ', str(row.iloc[-2])))\n",
    "                df.loc[index, df.columns[-2]] = ''.join(filter(lambda x: not (x.isalpha() or x == ' '), str(row.iloc[-2])))\n",
    "            \n",
    "            # Check if comma or dot is used as decimal separator\n",
    "            if ',' in str(row.iloc[-2]):\n",
    "                split_values = str(row.iloc[-2]).split(',')\n",
    "            elif '.' in str(row.iloc[-2]):\n",
    "                split_values = str(row.iloc[-2]).split('.')\n",
    "            else:\n",
    "                # If neither comma nor dot found, assume no decimal part\n",
    "                split_values = [str(row.iloc[-2]), '']\n",
    "                \n",
    "            cleaned_integer = ''.join(filter(lambda x: x.isdigit() or x == '-', split_values[0]))\n",
    "            cleaned_decimal = ''.join(filter(lambda x: x.isdigit(), split_values[1]))\n",
    "            if cleaned_decimal:\n",
    "                # Use comma as decimal separator\n",
    "                cleaned_numeric = cleaned_integer + ',' + cleaned_decimal\n",
    "            else:\n",
    "                cleaned_numeric = cleaned_integer\n",
    "            df.loc[index, df.columns[-2]] = cleaned_numeric\n",
    "    return df\n",
    "\n",
    "\n",
    "# 4. \n",
    "def extract_years(df):\n",
    "    year_columns = [col for col in df.columns if re.match(r'\\b\\d{4}\\b', col)]\n",
    "    #print(\"Años (4 dígitos) extraídos:\")\n",
    "    #print(year_columns)\n",
    "    return year_columns\n",
    "\n",
    "# 6. \n",
    "def first_row_columns(df):\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.drop(df.index[0])\n",
    "    return df\n",
    "\n",
    "# 15.\n",
    "def clean_columns_values(df):\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = [unicodedata.normalize('NFKD', col).encode('ASCII', 'ignore').decode('utf-8') for col in df.columns]\n",
    "    df.columns = df.columns.str.replace(' ', '_').str.replace('ano', 'year').str.replace('-', '_')\n",
    "    \n",
    "    text_columns = df.select_dtypes(include='object').columns\n",
    "    for col in df.columns:\n",
    "        df.loc[:, col] = df[col].apply(lambda x: remove_tildes(x) if isinstance(x, str) else x)\n",
    "        df.loc[:, col] = df[col].apply(lambda x: str(x).replace(',', '.') if isinstance(x, (int, float, str)) else x)\n",
    "    df.loc[:, 'sectores_economicos'] = df['sectores_economicos'].str.lower()\n",
    "    df.loc[:, 'economic_sectors'] = df['economic_sectors'].str.lower()\n",
    "    df.loc[:, 'sectores_economicos'] = df['sectores_economicos'].apply(remove_rare_characters)\n",
    "    df.loc[:, 'economic_sectors'] = df['economic_sectors'].apply(remove_rare_characters)\n",
    "    return df\n",
    "\n",
    "# 16.\n",
    "def convertir_float(df):\n",
    "    excluded_columns = ['sectores_economicos', 'economic_sectors']\n",
    "    columns_to_convert = [col for col in df.columns if col not in excluded_columns]\n",
    "    df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    return df\n",
    "\n",
    "# 15.\n",
    "def relocate_last_column(df):\n",
    "    last_column = df.pop(df.columns[-1])\n",
    "    df.insert(1, last_column.name, last_column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed8745",
   "metadata": {},
   "source": [
    "### Exclusiva Tabla 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6981f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATIPIC LAST COLUMNS\n",
    "def relocate_last_columns(df):\n",
    "    if not pd.isna(df.iloc[1, -1]):\n",
    "        # Create a new column with NaN\n",
    "        new_column = 'col_' + ''.join(map(str, np.random.randint(1, 5, size=1)))\n",
    "        df[new_column] = np.nan\n",
    "        \n",
    "        # Get 'ECONOMIC SECTORS' and relocate\n",
    "        insert_value_1 = df.iloc[0, -2]\n",
    "        # Convert the value to string before assignment\n",
    "        insert_value_1 = str(insert_value_1)\n",
    "        # Ensure the dtype of the last column is object (string) to accommodate string values\n",
    "        df.iloc[:, -1] = df.iloc[:, -1].astype('object')\n",
    "        df.iloc[0, -1] = insert_value_1\n",
    "        \n",
    "        # NaN first obs\n",
    "        df.iloc[0,-2] = np.nan\n",
    "    return df\n",
    "\n",
    "# Extraer meses\n",
    "\n",
    "def get_months_sublist_list(df):\n",
    "    first_row = df.iloc[0]\n",
    "    # Initialize the list of sublists\n",
    "    months_sublist_list = []\n",
    "\n",
    "    # Initialize the current sublist\n",
    "    months_sublist = []\n",
    "\n",
    "    # Iterate over the elements of the first row\n",
    "    for item in first_row:\n",
    "        # Convertir el elemento a cadena de texto, si no lo es ya\n",
    "        item = str(item)\n",
    "        # Check if the item meets the requirements\n",
    "        if len(item) == 3:\n",
    "            months_sublist.append(item)\n",
    "        elif '-' in item or item == 'year':\n",
    "            months_sublist.append(item)\n",
    "            months_sublist_list.append(months_sublist)\n",
    "            months_sublist = []\n",
    "\n",
    "    # Add the last sublist if it's not empty\n",
    "    if months_sublist:\n",
    "        months_sublist_list.append(months_sublist)\n",
    "\n",
    "    new_elements = []\n",
    "\n",
    "    for i, year in enumerate(year_columns):\n",
    "        for element in months_sublist_list[i]:\n",
    "            new_elements.append(f\"{year}_{element}\")\n",
    "\n",
    "    two_first_elements = df.iloc[0][:2].tolist()\n",
    "\n",
    "    if two_first_elements[1] not in new_elements:\n",
    "        new_elements.insert(0, two_first_elements[1])\n",
    "\n",
    "    if two_first_elements[0] not in new_elements:\n",
    "        new_elements.insert(0, two_first_elements[0])\n",
    "\n",
    "    temp_df = pd.DataFrame([new_elements], columns=df.columns)\n",
    "    df.iloc[0] = temp_df.iloc[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def find_year_column(df):\n",
    "    # List to store the found years\n",
    "    found_years = []\n",
    "\n",
    "    # Iterating over the column names of the DataFrame\n",
    "    for column in df.columns:\n",
    "        # Checking if the column name is a year (4 digits)\n",
    "        if column.isdigit() and len(column) == 4:\n",
    "            found_years.append(column)\n",
    "\n",
    "    # If more than one year is found, do nothing\n",
    "    if len(found_years) > 1:\n",
    "        pass\n",
    "    # If exactly one year is found, implement additional code\n",
    "    elif len(found_years) == 1:\n",
    "        # Getting the name of the found year\n",
    "        year_name = found_years[0]\n",
    "        print(\"The name of the column representing the year is:\", year_name)\n",
    "\n",
    "        # Getting the first row of the DataFrame\n",
    "        first_row = df.iloc[0]\n",
    "\n",
    "        # Searching for the first column containing the word \"year\" or some hyphen-separated expression\n",
    "        column_contains_year = first_row[first_row.astype(str).str.contains(r'\\byear\\b')]\n",
    "\n",
    "        if not column_contains_year.empty:\n",
    "            # Getting the name of the first column containing 'year' or some hyphen-separated expression in the first row\n",
    "            column_contains_year_name = column_contains_year.index[0]\n",
    "            print(\"The name of the first column containing 'year' or some hyphen-separated expression in the first row is:\", column_contains_year_name)\n",
    "\n",
    "            # Getting the indices of the columns\n",
    "            column_contains_year_index = df.columns.get_loc(column_contains_year_name)\n",
    "            year_name_index = df.columns.get_loc(year_name)\n",
    "            print(\"The index of the column containing 'year' is:\", column_contains_year_index)\n",
    "            print(\"The index of the column representing the year is:\", year_name_index)\n",
    "\n",
    "            # Checking if the column representing the year is to the right or to the left of column_contains_year\n",
    "            if column_contains_year_index < year_name_index:\n",
    "                print(\"The year column is to the right of the column containing 'year'.\")\n",
    "                # Adding one to the year\n",
    "                new_year = str(int(year_name) - 1)\n",
    "                # Renaming the column containing 'year' with the new year\n",
    "                df.rename(columns={column_contains_year_name: new_year}, inplace=True)\n",
    "                print(f\"The column containing 'year' is now named '{new_year}'.\")\n",
    "            elif column_contains_year_index > year_name_index:\n",
    "                print(\"The year column is to the left of the column containing 'year'.\")\n",
    "                # Subtracting one from the year\n",
    "                new_year = str(int(year_name) + 1)\n",
    "                # Renaming the year column with the new year\n",
    "                df.rename(columns={column_contains_year_name: new_year}, inplace=True)\n",
    "                print(f\"The column containing 'year' is now named '{new_year}'.\")\n",
    "            else:\n",
    "                print(\"The year column is in the same position as the column containing 'year'.\")\n",
    "        else:\n",
    "            print(\"No columns containing 'year' were found in the first row.\")\n",
    "    # If no year is found, print a message\n",
    "    else:\n",
    "        print(\"No years were found in the column names.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "def clean_first_row(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            if isinstance(df.at[0, col], str):\n",
    "                df.at[0, col] = df.at[0, col].lower()  # Convertir a minúsculas solo si es un objeto\n",
    "                df.at[0, col] = remove_tildes(df.at[0, col])\n",
    "                df.at[0, col] = remove_rare_characters_first_row(df.at[0, col])\n",
    "                # Reemplazar 'ano' por 'year'\n",
    "                df.at[0, col] = df.at[0, col].replace('ano', 'year')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def intercambiar_valores(df):\n",
    "    # Verificar si hay al menos dos columnas en el DataFrame\n",
    "    if len(df.columns) < 2:\n",
    "        print(\"El DataFrame tiene menos de dos columnas. No se pueden intercambiar valores.\")\n",
    "        return df\n",
    "\n",
    "    # Verificar si hay valores NaN en la última columna\n",
    "    if df.iloc[:, -1].isnull().any():\n",
    "        # Obtener índice de filas con NaN en la última columna\n",
    "        last_column_rows_nan = df[df.iloc[:, -1].isnull()].index\n",
    "\n",
    "        # Iterar sobre las filas con NaN en la última columna\n",
    "        for idx in last_column_rows_nan:\n",
    "            # Verificar si el índice está dentro del rango de las columnas\n",
    "            if -2 >= -len(df.columns):\n",
    "                # Intercambiar los valores de la última columna y la penúltima columna\n",
    "                df.iloc[idx, -1], df.iloc[idx, -2] = df.iloc[idx, -2], df.iloc[idx, -1]\n",
    "            else:\n",
    "                print(f\"Índice fuera de rango para la fila {idx}. No se pueden intercambiar valores.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def replace_var_perc_first_column(df):\n",
    "    # Regular expression to search for \"Var. %\" or \"Var.%\"\n",
    "    regex = re.compile(r'Var\\. ?%')\n",
    "\n",
    "    # Iterate over the rows of the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # Convert the value in the first column to a string\n",
    "        value = str(row.iloc[0])\n",
    "\n",
    "        # Check if the value matches the regular expression\n",
    "        if regex.search(value):\n",
    "            # Replace only the characters that match the regular expression\n",
    "            df.at[index, df.columns[0]] = regex.sub(\"variacion porcentual\", value)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# 8.\n",
    "number_moving_average = 'three ' # Keep a space at the end\n",
    "\n",
    "def replace_number_moving_average(df):\n",
    "    for index, row in df.iterrows():\n",
    "        # Buscar la expresión regular en la penúltima o última columna\n",
    "        if pd.notnull(row.iloc[-1]) and re.search(r'(\\d\\s*-)', str(row.iloc[-1])):\n",
    "            df.at[index, df.columns[-1]] = re.sub(r'(\\d\\s*-)', f'{number_moving_average}-', str(row.iloc[-1]))\n",
    "        elif pd.notnull(row.iloc[-2]) and re.search(r'(\\d\\s*-)', str(row.iloc[-2])):\n",
    "            df.at[index, df.columns[-2]] = re.sub(r'(\\d\\s*-)', f'{number_moving_average}-', str(row.iloc[-2]))\n",
    "    return df\n",
    "\n",
    "\n",
    "# 7.\n",
    "def replace_var_perc_last_columns(df):\n",
    "    # Expresión regular para buscar \"Var. %\" o \"Var.%\"\n",
    "    regex = re.compile(r'(Var\\. ?%)(.*)')\n",
    "\n",
    "    # Iterar sobre las filas del dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # Verificar si el valor en la penúltima columna es una cadena no nula\n",
    "        if isinstance(row.iloc[-2], str) and regex.search(row.iloc[-2]):\n",
    "            # Realizar el reemplazo al final del valor de la penúltima columna\n",
    "            replaced_text = regex.sub(r'\\2 percent change', row.iloc[-2])\n",
    "            df.at[index, df.columns[-2]] = replaced_text.strip()\n",
    "        \n",
    "        # Verificar si el valor en la última columna es una cadena no nula\n",
    "        if isinstance(row.iloc[-1], str) and regex.search(row.iloc[-1]):\n",
    "            # Realizar el reemplazo al final del valor de la última columna\n",
    "            replaced_text = regex.sub(r'\\2 percent change', row.iloc[-1])\n",
    "            df.at[index, df.columns[-1]] = replaced_text.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Función para buscar y reemplazar en la segunda fila del DataFrame\n",
    "def replace_first_dot(df):\n",
    "    second_row = df.iloc[1]  # Segunda fila del DataFrame\n",
    "    \n",
    "    # Verificar si al menos una observación cumple con el patrón\n",
    "    if any(isinstance(cell, str) and re.match(r'^\\w+\\.\\s?\\w+', cell) for cell in second_row):\n",
    "        for col in df.columns:\n",
    "            if isinstance(second_row[col], str):  # Verificar si el valor es una cadena\n",
    "                if re.match(r'^\\w+\\.\\s?\\w+', second_row[col]):  # Verificar si cumple con el patrón Xxx.Xxx o Xxx. Xxx.\n",
    "                    df.at[1, col] = re.sub(r'(\\w+)\\.(\\s?\\w+)', r'\\1-\\2', second_row[col], count=1)  # Reemplazar solo el primer punto\n",
    "    return df\n",
    "\n",
    "def drop_rare_caracter_row(df):\n",
    "    # Buscar el caracter solitario \"}\" en cada fila y obtener un booleano para cada fila\n",
    "    rare_caracter_row = df.apply(lambda row: '}' in row.values, axis=1)\n",
    "    \n",
    "    # Filtrar el DataFrame para eliminar las filas con el caracter solitario \"}\"\n",
    "    df = df[~rare_caracter_row]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b3bf9",
   "metadata": {},
   "source": [
    "### Exclusiva Tabla 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8911f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_first_row(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            if isinstance(df.at[0, col], str):\n",
    "                df.at[0, col] = df.at[0, col].lower()  # Convertir a minúsculas solo si es un objeto\n",
    "                df.at[0, col] = remove_tildes(df.at[0, col])\n",
    "                df.at[0, col] = remove_rare_characters_first_row(df.at[0, col])\n",
    "                # Reemplazar 'ano' por 'year'\n",
    "                df.at[0, col] = df.at[0, col].replace('ano', 'year')\n",
    "\n",
    "    return df\n",
    "\n",
    "# 2.\n",
    "def separate_years(df):\n",
    "    df = df.copy()  # Se crea una copia del DataFrame para evitar SettingWithCopyWarning\n",
    "    if isinstance(df.iloc[0, -2], str) and len(df.iloc[0, -2].split()) == 2:\n",
    "        years = df.iloc[0, -2].split()\n",
    "        if all(len(year) == 4 for year in years):\n",
    "            segundo_anio = years[1]\n",
    "            df.iloc[0, -2] = years[0]\n",
    "            df.insert(len(df.columns) - 1, 'new_column', [segundo_anio] + [None] * (len(df) - 1))\n",
    "    return df\n",
    "\n",
    "# 3.\n",
    "def find_roman_numerals(text):\n",
    "    pattern = r'\\b(?:I{1,3}|IV|V|VI{0,3}|IX|X)\\b'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "def relocate_roman_numerals(df):\n",
    "    numeros_romanos = find_roman_numerals(df.iloc[2, -1])\n",
    "    if numeros_romanos:\n",
    "        original_text = df.iloc[2, -1]\n",
    "        for roman_numeral in numeros_romanos:\n",
    "            original_text = original_text.replace(roman_numeral, '').strip()\n",
    "        df.iloc[2, -1] = original_text\n",
    "        df.at[2, 'new_column'] = ', '.join(numeros_romanos)\n",
    "        df.iloc[2, -1] = np.nan\n",
    "    return df\n",
    "\n",
    "# 4.\n",
    "def extract_mixed_values(df):\n",
    "    df = df.copy()  # Se crea una copia del DataFrame para evitar SettingWithCopyWarning\n",
    "    regex_pattern = r'(-?\\d+,\\d [a-zA-Z\\s]+)'\n",
    "    for index, row in df.iterrows():\n",
    "        antepenultima_obs = row.iloc[-3]\n",
    "        penultima_obs = row.iloc[-2]\n",
    "\n",
    "        if isinstance(antepenultima_obs, str) and pd.notnull(antepenultima_obs):\n",
    "            match = re.search(regex_pattern, antepenultima_obs)\n",
    "            if match:\n",
    "                parte_extraida = match.group(0)\n",
    "                if pd.isna(penultima_obs) or pd.isnull(penultima_obs):\n",
    "                    df.iloc[index, -2] = parte_extraida\n",
    "                    antepenultima_obs = re.sub(regex_pattern, '', antepenultima_obs).strip()\n",
    "                    df.iloc[index, -3] = antepenultima_obs\n",
    "    return df\n",
    "\n",
    "# 5.\n",
    "def replace_first_row_nan(df):\n",
    "    for col in df.columns:\n",
    "        if pd.isna(df.iloc[0][col]):\n",
    "            df.iloc[0, df.columns.get_loc(col)] = col\n",
    "    return df\n",
    "\n",
    "# 11. \n",
    "def split_values(df):\n",
    "    columna_a_expandir = df.columns[-3]\n",
    "    nuevas_columnas = df[columna_a_expandir].str.split(expand=True)\n",
    "    nuevas_columnas.columns = [f'{columna_a_expandir}_{i+1}' for i in range(nuevas_columnas.shape[1])]\n",
    "    posicion_insercion = len(df.columns) - 2\n",
    "    for col in reversed(nuevas_columnas.columns):\n",
    "        df.insert(posicion_insercion, col, nuevas_columnas[col])\n",
    "    df.drop(columns=[columna_a_expandir], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# 13.\n",
    "def roman_arabic(df):\n",
    "    primera_fila = df.iloc[0]\n",
    "    def convert_roman_number(numero):\n",
    "        try:\n",
    "            return str(roman.fromRoman(numero))\n",
    "        except roman.InvalidRomanNumeralError:\n",
    "            return numero\n",
    "\n",
    "    primera_fila_convertida = []\n",
    "    for valor in primera_fila:\n",
    "        if isinstance(valor, str) and not pd.isna(valor):\n",
    "            primera_fila_convertida.append(convert_roman_number(valor))\n",
    "        else:\n",
    "            primera_fila_convertida.append(valor)\n",
    "\n",
    "    df.iloc[0] = primera_fila_convertida\n",
    "    return df\n",
    "\n",
    "# 14.\n",
    "def fix_duplicates(df):\n",
    "    fila_segunda = df.iloc[0].copy()\n",
    "    prev_num = None\n",
    "    first_one_index = None\n",
    "\n",
    "    for i, num in enumerate(fila_segunda):\n",
    "        try:\n",
    "            num = int(num)\n",
    "            prev_num = int(prev_num) if prev_num is not None else None\n",
    "\n",
    "            if num == prev_num:\n",
    "                if num == 1:\n",
    "                    if first_one_index is None:\n",
    "                        first_one_index = i - 1\n",
    "                    next_num = int(fila_segunda[i - 1]) + 1\n",
    "                    for j in range(i, len(fila_segunda)):\n",
    "                        if fila_segunda.iloc[j].isdigit():\n",
    "                            fila_segunda.iloc[j] = str(next_num)\n",
    "                            next_num += 1\n",
    "                elif i - 1 >= 0:\n",
    "                    fila_segunda.iloc[i] = str(int(fila_segunda.iloc[i - 1]) + 1)\n",
    "\n",
    "            prev_num = num\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    df.iloc[0] = fila_segunda\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# 16. \n",
    "def get_quarters_sublist_list(df):\n",
    "    first_row = df.iloc[0]\n",
    "    # Initialize the list of sublists\n",
    "    quarters_sublist_list = []\n",
    "\n",
    "    # Initialize the current sublist\n",
    "    quarters_sublist = []\n",
    "\n",
    "    # Iterate over the elements of the first row\n",
    "    for item in first_row:\n",
    "        # Check if the item meets the requirements\n",
    "        if len(item) == 1:\n",
    "            quarters_sublist.append(item)\n",
    "        elif item == 'year':\n",
    "            quarters_sublist.append(item)\n",
    "            quarters_sublist_list.append(quarters_sublist)\n",
    "            quarters_sublist = []\n",
    "\n",
    "    # Add the last sublist if it's not empty\n",
    "    if quarters_sublist:\n",
    "        quarters_sublist_list.append(quarters_sublist)\n",
    "\n",
    "    new_elements = []\n",
    "\n",
    "    for i, year in enumerate(year_columns):\n",
    "        for element in quarters_sublist_list[i]:\n",
    "            new_elements.append(f\"{year}_{element}\")\n",
    "\n",
    "    two_first_elements = df.iloc[0][:2].tolist()\n",
    "\n",
    "    if two_first_elements[1] not in new_elements:\n",
    "        new_elements.insert(0, two_first_elements[1])\n",
    "\n",
    "    if two_first_elements[0] not in new_elements:\n",
    "        new_elements.insert(0, two_first_elements[0])\n",
    "\n",
    "    temp_df = pd.DataFrame([new_elements], columns=df.columns)\n",
    "    df.iloc[0] = temp_df.iloc[0]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88524d6",
   "metadata": {},
   "source": [
    "# Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4565452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataframe generado para el archivo pseudo_raw_pdf\\ns-01-2014.pdf es: ns_01_2014_1\n"
     ]
    }
   ],
   "source": [
    "import tabula\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import locale\n",
    "\n",
    "# Ruta de la carpeta que contiene los archivos PDF\n",
    "raw_pdf = 'pseudo_raw_pdf'\n",
    "if not os.path.exists(raw_pdf):\n",
    "    os.mkdir(raw_pdf)\n",
    "\n",
    "# Establecer la localización en español\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Palabras clave para buscar en el texto de la página\n",
    "keywords = [\"ECONOMIC SECTORS\"]\n",
    "\n",
    "# Diccionario para almacenar los DataFrames generados\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Función para corregir los nombres de los meses\n",
    "def corregir_nombre_mes(mes):\n",
    "    meses_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Agrega más mapeos si es necesario para otros nombres de meses\n",
    "    }\n",
    "    return meses_mapping.get(mes, mes)\n",
    "\n",
    "def procesar_pdf(pdf_path):\n",
    "    tables_dict = {}  # Diccionario local para cada PDF\n",
    "    table_counter = 1\n",
    "    keyword_count = 0 \n",
    "\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No se encontraron coincidencias para id_ns y year en el nombre del archivo:\", filename)\n",
    "        return None, None, None, None\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(pdf_path))[0].replace('-', '_')\n",
    "    date = None\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            text = page.extract_text()\n",
    "            if i == 1:\n",
    "                match = re.search(r'(\\d{1,2}\\s+de\\s+\\w+\\s+de\\s+\\d{4})', text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    fecha_str = match.group(0)\n",
    "                    # Corregir el nombre del mes\n",
    "                    partes_fecha = fecha_str.split()\n",
    "                    partes_fecha[2] = corregir_nombre_mes(partes_fecha[2].lower())\n",
    "                    fecha_str_corregida = ' '.join(partes_fecha)\n",
    "                    date = datetime.strptime(fecha_str_corregida, '%d de %B de %Y')\n",
    "\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                keyword_count += 1\n",
    "                if keyword_count == 1:  # Solo procesar la primera ocurrencia\n",
    "                    tables = tabula.read_pdf(pdf_path, pages=i, multiple_tables=False, stream=True) # change stream to another option if desired\n",
    "                    for j, table_df in enumerate(tables, start=1):\n",
    "                        nombre_dataframe = f\"{new_filename}_{keyword_count}\"\n",
    "                        tables_dict[nombre_dataframe] = table_df\n",
    "                        table_counter += 1\n",
    "\n",
    "                    break  # Salir del bucle después de encontrar la primera ocurrencia\n",
    "\n",
    "    return id_ns, year, date, tables_dict, keyword_count\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_folder = 'pseudo_raw_pdf'\n",
    "    pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "\n",
    "    tables_dict = {}  # Declarar tables_dict fuera del bucle principal\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        id_ns, year, date, tables_dict_temp, keyword_count = procesar_pdf(pdf_file)\n",
    "        table_counter = 1\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for nombre_df, df in tables_dict_temp.items():\n",
    "                # Aplicar las 20 líneas\n",
    "                \n",
    "                df = drop_rare_caracter_row(df)\n",
    "                df = drop_nan_rows(df)\n",
    "                df = drop_nan_columns(df)\n",
    "                df = relocate_last_columns(df)\n",
    "                df = replace_first_dot(df)\n",
    "                df = swap_first_second_row(df)\n",
    "                df = drop_nan_rows(df)\n",
    "                df = reset_index(df)\n",
    "                df = remove_digit_slash(df)\n",
    "                df = replace_var_perc_first_column(df)\n",
    "                df = replace_var_perc_last_columns(df)\n",
    "                df = replace_number_moving_average(df)\n",
    "                df = separate_text_digits(df)\n",
    "                df = intercambiar_valores(df)\n",
    "                df = relocate_last_column(df)\n",
    "                df = clean_first_row(df)\n",
    "                df = find_year_column(df)\n",
    "                year_columns = extract_years(df)\n",
    "                df = get_months_sublist_list(df)\n",
    "                df = first_row_columns(df)\n",
    "                df = clean_columns_values(df)\n",
    "                df = convertir_float(df)\n",
    "                \n",
    "                df.insert(0, 'date', date)\n",
    "                df.insert(1, 'id_ns', id_ns)\n",
    "                df.insert(2, 'year', year)\n",
    "\n",
    "                # Añadir las columnas 'year', 'id_ns', 'date' al DataFrame\n",
    "                df['year'] = year\n",
    "                df['id_ns'] = id_ns\n",
    "                df['date'] = date\n",
    "                \n",
    "                nombre_archivo = os.path.splitext(os.path.basename(pdf_file))[0].replace('-', '_')\n",
    "                nombre_df = f\"{nombre_archivo}_{keyword_count}\"\n",
    "                table_counter += 1\n",
    "\n",
    "                dataframes_dict[nombre_df] = df\n",
    "\n",
    "                print(f'El dataframe generado para el archivo {pdf_file} es: {nombre_df}')\n",
    "\n",
    "                #df.to_csv(f'{nombre_df}.csv', index=False)\n",
    "\n",
    "                # Agregar las claves al diccionario principal\n",
    "                tables_dict.update(tables_dict_temp)\n",
    "\n",
    "        else:\n",
    "            print(f\"No se encontraron dataframes generados para el archivo {pdf_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5108832",
   "metadata": {},
   "source": [
    "# Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b895a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import locale\n",
    "import roman\n",
    "import unicodedata\n",
    "\n",
    "# Ruta de la carpeta que contiene los archivos PDF\n",
    "raw_pdf = 'pseudo_raw_pdf'\n",
    "if not os.path.exists(raw_pdf):\n",
    "    os.mkdir(raw_pdf)\n",
    "\n",
    "# Establecer la localización en español\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Palabras clave para buscar en el texto de la página\n",
    "keywords = [\"ECONOMIC SECTORS\"]\n",
    "\n",
    "# Diccionario para almacenar los DataFrames generados\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Función para corregir los nombres de los meses\n",
    "def corregir_nombre_mes(mes):\n",
    "    meses_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Agrega más mapeos si es necesario para otros nombres de meses\n",
    "    }\n",
    "    return meses_mapping.get(mes, mes)\n",
    "\n",
    "def procesar_pdf(pdf_path):\n",
    "    tables_dict = {}  # Diccionario local para cada PDF\n",
    "    table_counter = 1\n",
    "    keyword_count = 0 \n",
    "\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No se encontraron coincidencias para id_ns y year en el nombre del archivo:\", filename)\n",
    "        return None, None, None, None\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(pdf_path))[0].replace('-', '_')\n",
    "    date = None\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            text = page.extract_text()\n",
    "            if i == 1:\n",
    "                match = re.search(r'(\\d{1,2}\\s+de\\s+\\w+\\s+de\\s+\\d{4})', text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    fecha_str = match.group(0)\n",
    "                    # Corregir el nombre del mes\n",
    "                    partes_fecha = fecha_str.split()\n",
    "                    partes_fecha[2] = corregir_nombre_mes(partes_fecha[2].lower())\n",
    "                    fecha_str_corregida = ' '.join(partes_fecha)\n",
    "                    date = datetime.strptime(fecha_str_corregida, '%d de %B de %Y')\n",
    "\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                keyword_count += 1\n",
    "                if keyword_count == 2:\n",
    "                    tables = tabula.read_pdf(pdf_path, pages=i, multiple_tables=False)\n",
    "                    for j, table_df in enumerate(tables, start=1):\n",
    "                        nombre_dataframe = f\"{new_filename}_{keyword_count}\"\n",
    "                        tables_dict[nombre_dataframe] = table_df\n",
    "                        table_counter += 1\n",
    "\n",
    "    return id_ns, year, date, tables_dict, keyword_count\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_folder = 'input_pdf'\n",
    "    pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "\n",
    "    tables_dict = {}  # Declarar tables_dict fuera del bucle principal\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        id_ns, year, date, tables_dict_temp, keyword_count = procesar_pdf(pdf_file)\n",
    "        table_counter = 1 \n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for nombre_df, df in tables_dict_temp.items():\n",
    "                if df.iloc[0, 0] is np.nan: # Verificar si la primera observación es NaN\n",
    "                    # Aplicar las 20 líneas\n",
    "                    df = drop_nan_columns(df)\n",
    "                    df = separate_years(df)\n",
    "                    df = relocate_roman_numerals(df)\n",
    "                    df = extract_mixed_values(df)\n",
    "                    df = replace_first_row_nan(df)\n",
    "                    df = first_row_columns(df)\n",
    "                    df = swap_first_second_row(df)\n",
    "                    df = reset_index(df)\n",
    "                    df = drop_nan_row(df)\n",
    "                    year_columns = extract_years(df)\n",
    "                    df = split_values(df)\n",
    "                    df = separate_text_digits(df)\n",
    "                    df = roman_arabic(df)\n",
    "                    df = fix_duplicates(df)\n",
    "                    df = relocate_last_column(df)\n",
    "                    df = clean_first_row(df)\n",
    "                    df = get_quarters_sublist_list(df)\n",
    "                    df = first_row_columns(df)\n",
    "                    df = clean_columns_values(df)\n",
    "                    df = reset_index(df)\n",
    "                    df = convertir_float(df)\n",
    "                    \n",
    "                    df.insert(0, 'date', date)\n",
    "                    df.insert(1, 'id_ns', id_ns)\n",
    "                    df.insert(2, 'year', year)\n",
    "                    \n",
    "                    # Añadir las columnas 'year', 'id_ns', 'date' al DataFrame\n",
    "                    df['year'] = year\n",
    "                    df['id_ns'] = id_ns\n",
    "                    df['date'] = date\n",
    "                    \n",
    "                else:\n",
    "                    # Aplicar las 15 líneas\n",
    "                    df = drop_nan_columns(df)\n",
    "                    df = remove_digit_slash(df)\n",
    "                    df = swap_first_second_row(df)\n",
    "                    df = reset_index(df)\n",
    "                    df = drop_nan_row(df)\n",
    "                    year_columns = extract_years(df)\n",
    "                    df = separate_text_digits(df)\n",
    "                    df = roman_arabic(df)\n",
    "                    df = fix_duplicates(df)\n",
    "                    df = relocate_last_column(df)\n",
    "                    df = clean_first_row(df)\n",
    "                    df = get_quarters_sublist_list(df)\n",
    "                    df = first_row_columns(df)\n",
    "                    df = clean_columns_values(df)\n",
    "                    df = reset_index(df)\n",
    "                    df = convertir_float(df)\n",
    "                    \n",
    "                    df.insert(0, 'date', date)\n",
    "                    df.insert(1, 'id_ns', id_ns)\n",
    "                    df.insert(2, 'year', year)\n",
    "                    \n",
    "                    # Añadir las columnas 'year', 'id_ns', 'date' al DataFrame\n",
    "                    df['year'] = year\n",
    "                    df['id_ns'] = id_ns\n",
    "                    df['date'] = date\n",
    "\n",
    "                nombre_archivo = os.path.splitext(os.path.basename(pdf_file))[0].replace('-', '_')\n",
    "                nombre_df = f\"{nombre_archivo}_{keyword_count}\"\n",
    "                table_counter += 1\n",
    "\n",
    "                dataframes_dict[nombre_df] = df\n",
    "\n",
    "                print(f'El dataframe generado para el archivo {pdf_file} es: {nombre_df}')\n",
    "\n",
    "                #df.to_csv(f'{nombre_df}.csv', index=False)\n",
    "\n",
    "                # Agregar las claves al diccionario principal\n",
    "                tables_dict.update(tables_dict_temp)\n",
    "\n",
    "        else:\n",
    "            print(f\"No se encontraron dataframes generados para el archivo {pdf_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726d387",
   "metadata": {},
   "source": [
    "# To sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ca68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre los DataFrames generados y guardarlos en PostgreSQL\n",
    "for nombre_df, df in all_dataframes.items():\n",
    "    # Crear una nueva tabla en PostgreSQL con el nombre del DataFrame\n",
    "    cur.execute(f\"CREATE TABLE IF NOT EXISTS {nombre_df} (index serial PRIMARY KEY);\")\n",
    "\n",
    "    # Iterar sobre las columnas del DataFrame y agregarlas a la tabla en PostgreSQL\n",
    "    for columna in df.columns:\n",
    "        cur.execute(f\"ALTER TABLE {nombre_df} ADD COLUMN IF NOT EXISTS {columna} TEXT;\")\n",
    "\n",
    "    # Insertar los datos del DataFrame en la tabla en PostgreSQL\n",
    "    for fila in df.itertuples(index=False):\n",
    "        cur.execute(f\"INSERT INTO {nombre_df} VALUES ({', '.join(['%s'] * len(fila))});\", fila)\n",
    "\n",
    "# Confirmar los cambios y cerrar la conexión a PostgreSQL\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f44ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "667d97b8",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217183e7",
   "metadata": {},
   "source": [
    "<div id=\"3\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8989d65",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.</span> <span style = \"color: dark; font-family: charter;\">SQL Tables</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a04abd",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Finally, after obtaining and cleaning all the necessary data, we can create the three most important datasets to store realeses, vintages, and revisions. These datasets will be stored as tables in SQL and can be loaded into any software or programming language.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6ca98",
   "metadata": {},
   "source": [
    "<div id=\"3-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b142e",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.1.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Annual Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb99fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the names of dataframes that meet the criterion of ending in '_3'\n",
    "dataframes_ending_with_3 = []\n",
    "\n",
    "# List to store the names of dataframes to be concatenated\n",
    "dataframes_to_concatenate = []\n",
    "\n",
    "# Iterate over the dataframe names in the all_dataframes dictionary\n",
    "for df_name in all_dataframes.keys():\n",
    "    # Check if the dataframe name ends with '_3' and add it to the corresponding list\n",
    "    if df_name.endswith('_3'):\n",
    "        dataframes_ending_with_3.append(df_name)\n",
    "        dataframes_to_concatenate.append(all_dataframes[df_name])\n",
    "\n",
    "# Print the names of dataframes that meet the criterion of ending in '_3'\n",
    "print(\"DataFrames ending with '_3' that will be concatenated:\")\n",
    "for df_name in dataframes_ending_with_3:\n",
    "    print(df_name)\n",
    "\n",
    "# Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "if dataframes_to_concatenate:\n",
    "    # Concatenate only rows that meet the specified conditions\n",
    "    gdp_annual_growth_rates = pd.concat([df[(df['sectores_economicos'] == 'pbi') | (df['economic_sectors'] == 'gdp')] \n",
    "                                for df in dataframes_to_concatenate \n",
    "                                if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                ignore_index=True)\n",
    "\n",
    "    # Keep only columns that start with 'year' and the 'id_ns', 'year', and 'date' columns\n",
    "    columns_to_keep = ['id_ns', 'year', 'date'] + [col for col in gdp_annual_growth_rates.columns if col.startswith('year')]\n",
    "\n",
    "    # Drop unwanted columns\n",
    "    gdp_annual_growth_rates = gdp_annual_growth_rates[columns_to_keep]\n",
    "    \n",
    "    # Remove duplicate columns if any\n",
    "    gdp_annual_growth_rates = gdp_annual_growth_rates.loc[:,~gdp_annual_growth_rates.columns.duplicated()]\n",
    "\n",
    "    # Print the number of rows in the concatenated dataframe\n",
    "    print(\"Number of rows in the concatenated dataframe:\", len(gdp_annual_growth_rates))\n",
    "else:\n",
    "    print(\"No dataframes were found to concatenate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_annual_growth_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f8d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61bbaac2",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d9bac",
   "metadata": {},
   "source": [
    "<div id=\"3-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50087e95",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.2.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Quarterly Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe25cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List to store the names of dataframes that meet the criterion of ending in '_3'\n",
    "dataframes_ending_with_3 = []\n",
    "\n",
    "# List to store the names of dataframes to be concatenated\n",
    "dataframes_to_concatenate = []\n",
    "\n",
    "# Iterate over the dataframe names in the all_dataframes dictionary\n",
    "for df_name in all_dataframes.keys():\n",
    "    # Check if the dataframe name ends with '_3' and add it to the corresponding list\n",
    "    if df_name.endswith('_3'):\n",
    "        dataframes_ending_with_3.append(df_name)\n",
    "        dataframes_to_concatenate.append(all_dataframes[df_name])\n",
    "\n",
    "# Print the names of dataframes that meet the criterion of ending in '_3'\n",
    "print(\"DataFrames ending with '_3' that will be concatenated:\")\n",
    "for df_name in dataframes_ending_with_3:\n",
    "    print(df_name)\n",
    "\n",
    "# Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "if dataframes_to_concatenate:\n",
    "    # Concatenate only rows that meet the specified conditions\n",
    "    gdp_quarterly_growth_rates = pd.concat([df[(df['sectores_economicos'] == 'pbi') | (df['economic_sectors'] == 'gdp')] \n",
    "                                for df in dataframes_to_concatenate \n",
    "                                if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                ignore_index=True)\n",
    "\n",
    "    # Keep all columns except those starting with 'year_', in addition to the 'id_ns', 'year', and 'date' columns\n",
    "    columns_to_keep = ['year', 'id_ns', 'date'] + [col for col in gdp_quarterly_growth_rates.columns if not col.startswith('year_')]\n",
    "\n",
    "    # Select unwanted columns\n",
    "    gdp_quarterly_growth_rates = gdp_quarterly_growth_rates[columns_to_keep]\n",
    "\n",
    "    # Drop the 'sectores_economicos' and 'economic_sectors' columns\n",
    "    gdp_quarterly_growth_rates.drop(columns=['sectores_economicos', 'economic_sectors'], inplace=True)\n",
    "\n",
    "    # Remove duplicate columns if any\n",
    "    gdp_quarterly_growth_rates = gdp_quarterly_growth_rates.loc[:,~gdp_quarterly_growth_rates.columns.duplicated()]\n",
    "\n",
    "    # Print the number of rows in the concatenated dataframe\n",
    "    print(\"Number of rows in the concatenated dataframe:\", len(gdp_quarterly_growth_rates))\n",
    "else:\n",
    "    print(\"No dataframes were found to concatenate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_quarterly_growth_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d99fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24dd0fc0",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca271f",
   "metadata": {},
   "source": [
    "<div id=\"3-3\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453bb965",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.3.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Monthly Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List to store the names of dataframes that meet the criterion of ending in '_1'\n",
    "dataframes_ending_with_1 = []\n",
    "\n",
    "# List to store the names of dataframes to be concatenated\n",
    "dataframes_to_concatenate = []\n",
    "\n",
    "# Iterate over the dataframe names in the all_dataframes dictionary\n",
    "for df_name in all_dataframes.keys():\n",
    "    # Check if the dataframe name ends with '_1' and add it to the corresponding list\n",
    "    if df_name.endswith('_1'):\n",
    "        dataframes_ending_with_1.append(df_name)\n",
    "        dataframes_to_concatenate.append(all_dataframes[df_name])\n",
    "\n",
    "# Print the names of dataframes that meet the criterion of ending with '_1'\n",
    "print(\"DataFrames ending with '_1' that will be concatenated:\")\n",
    "for df_name in dataframes_ending_with_1:\n",
    "    print(df_name)\n",
    "\n",
    "# Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "if dataframes_to_concatenate:\n",
    "    # Concatenate only rows that meet the specified conditions\n",
    "    gdp_monthly_growth_rates = pd.concat([df[(df['sectores_economicos'] == 'pbi') | (df['economic_sectors'] == 'gdp')] \n",
    "                                for df in dataframes_to_concatenate \n",
    "                                if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                ignore_index=True)\n",
    "\n",
    "    # Keep all columns except those starting with 'year_', in addition to the 'id_ns', 'year', and 'date' columns\n",
    "    columns_to_keep = ['year', 'id_ns', 'date'] + [col for col in gdp_monthly_growth_rates.columns if not col.startswith('year_')]\n",
    "\n",
    "    # Select unwanted columns\n",
    "    gdp_monthly_growth_rates = gdp_monthly_growth_rates[columns_to_keep]\n",
    "\n",
    "    # Drop the 'sectores_economicos' and 'economic_sectors' columns\n",
    "    gdp_monthly_growth_rates.drop(columns=['sectores_economicos', 'economic_sectors'], inplace=True)\n",
    "\n",
    "    # Remove duplicate columns if any\n",
    "    gdp_monthly_growth_rates = gdp_monthly_growth_rates.loc[:,~gdp_monthly_growth_rates.columns.duplicated()]\n",
    "    \n",
    "    # Drop columns with at least two underscores in their names\n",
    "    columns_to_drop = [col for col in gdp_monthly_growth_rates.columns if col.count('_') >= 2]\n",
    "    gdp_monthly_growth_rates.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    # Print the number of rows in the concatenated dataframe\n",
    "    print(\"Number of rows in the concatenated dataframe:\", len(gdp_monthly_growth_rates))\n",
    "else:\n",
    "    print(\"No dataframes were found to concatenate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e805081",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_monthly_growth_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_monthly_growth_rates['date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7880a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79df4c62",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda85bba",
   "metadata": {},
   "source": [
    "<div id=\"3-4\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d3554",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.4.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Loading SQL\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Get environment variables\n",
    "user = os.environ.get('CIUP_SQL_USER')\n",
    "password = os.environ.get('CIUP_SQL_PASS')\n",
    "host = os.environ.get('CIUP_SQL_HOST')\n",
    "port = 5432\n",
    "database = 'gdp_revisions_datasets'\n",
    "\n",
    "# Check if all environment variables are defined\n",
    "if not all([host, user, password]):\n",
    "    raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "# Create connection string\n",
    "connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# gdp_monthly_growth_rates is the DataFrame you want to save to the database\n",
    "gdp_annual_growth_rates.to_sql('gdp_annual_growth_rates', engine, index=False, if_exists='replace')\n",
    "gdp_quarterly_growth_rates.to_sql('gdp_quarterly_growth_rates', engine, index=False, if_exists='replace')\n",
    "gdp_monthly_growth_rates.to_sql('gdp_monthly_growth_rates', engine, index=False, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132cb43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c651835",
   "metadata": {},
   "source": [
    "### PENDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b4c405",
   "metadata": {},
   "source": [
    "1. Fix TYPOS (ns_05_2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd18b4",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc920fd",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eae11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf105ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdp_revisions",
   "language": "python",
   "name": "gdp_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
