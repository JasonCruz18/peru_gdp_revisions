{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd9a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a055b3b9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter'; color: rgb(0, 65, 75);\">\n",
    "    <h1>\n",
    "    GDP Revisions Datasets\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd88a4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter'; color: rgb(0, 65, 75);\">\n",
    "    <h4>\n",
    "        Documentation\n",
    "        <br>\n",
    "        ____________________\n",
    "            </br>\n",
    "    </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72f519",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color: dark;\">\n",
    "    This \n",
    "    <span style=\"color: rgb(61, 48, 162);\">jupyter notebook</span>\n",
    "    provides a step-by-step guide to <b>data building</b> regarding the project <b>'Revisiones y sesgos en las estimaciones preliminares del PBI en el Perú'</b>. The guide covers downloading PDF files containing tables with information on annual, quarterly, and monthly Peru's GDP growth rates (including sectoral GDP) and extracting this information into SQL tables. These data sets will be used for data analysis.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d22837",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter'; color: rgb(0, 65, 75);\">\n",
    "    Jason Cruz\n",
    "    <br>\n",
    "    <a href=\"mailto:jj.cruza@up.edu.pe\" style=\"color: rgb(0, 153, 123)\">\n",
    "        jj.cruza@up.edu.pe\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28540df2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Times New Roman; text-align: left; color: rgb(61, 48, 162)\">The provided outline is functional. Use the buttons to enhance the experience of this script.<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc04c9",
   "metadata": {},
   "source": [
    "<div id=\"outilne\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0fa13",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #141414; padding: 10px;\">\n",
    "<h2 style=\"text-align: left; font-family: 'charter'; color: #E0E0E0;\">\n",
    "    Outline\n",
    "    </h2>\n",
    "    <br>\n",
    "    <a href=\"#1\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        1. PDF Downloader</a>\n",
    "    <br>\n",
    "    <a href=\"#2\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        2. Extracting Tables (and data cleaning)</a>\n",
    "    <br>\n",
    "    <a href=\"#2-1\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        2.1. 'pdfplumber' demo.</a>\n",
    "    <br>\n",
    "    <a href=\"#2-1-1\" style=\"color: #E0E0E0; font-size: 12px;\">\n",
    "        2.1.1. What data would we get if we used the default settings?.</a>   \n",
    "    <br>\n",
    "    <a href=\"#2-1-2\" style=\"color: #E0E0E0; font-size: 12px;\">\n",
    "        2.1.2. Using custom '.extract_table' settings.</a>\n",
    "    <br> \n",
    "    <a href=\"#2-2\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        2.2. Extracting tables and generating dataframes (includes data cleanup).</a>\n",
    "    <br>\n",
    "    <a href=\"#3\" style=\"color: #687EFF; font-size: 18px;\">3. SQL Tables</a>\n",
    "    <br>\n",
    "    <a href=\"#3-1\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.1. Annual Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-2\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.2. Quarterly Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-3\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.3. Monthly Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-4\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.4. Loading SQL.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90323106",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    Any questions or issues regarding the coding, please <a href=\"mailto:jj.cruza@alum.up.edu.pe\" style=\"color: rgb(0, 153, 123)\">email Jason Cruz\n",
    "    </a>.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac40ed0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    If you don't have the libraries below, please use the following code (as example) to install the required libraries.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c73193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907046a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    <h2>\n",
    "    Libraries\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214f5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Downloader\n",
    "\n",
    "import os  # for file and directory manipulation\n",
    "import random  # to generate random numbers\n",
    "import time  # to manage time and take breaks in the script\n",
    "import requests  # to make HTTP requests to web servers\n",
    "from selenium import webdriver  # for automating web browsers\n",
    "from selenium.webdriver.common.by import By  # to locate elements on a webpage\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # to wait until certain conditions are met on a webpage.\n",
    "from selenium.webdriver.support import expected_conditions as EC  # to define expected conditions\n",
    "from selenium.common.exceptions import StaleElementReferenceException  # To handle exceptions related to elements on the webpage that are no longer available.\n",
    "\n",
    "\n",
    "# Extracting Tables (and data cleaning)\n",
    "\n",
    "import pdfplumber  # for extracting text and metadata from PDF files\n",
    "import pandas as pd  # for data manipulation and analysis\n",
    "import os  # for interacting with the operating system\n",
    "import unicodedata  # for manipulating Unicode data\n",
    "import re  # for regular expressions operations\n",
    "from datetime import datetime  # for working with dates and times\n",
    "import locale  # for locale-specific formatting of numbers, dates, and currencies\n",
    "\n",
    "\n",
    "# SQL tables\n",
    "\n",
    "import psycopg2  # for interacting with PostgreSQL databases\n",
    "from sqlalchemy import create_engine, text  # for creating and executing SQL queries using SQLAlchemy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ef8f8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    <h2>\n",
    "    Initial set-up\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc1c80",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\"> The next 3 code lines will create folders in your current path, call them to import and export your outputs. <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8899a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to download PDF files\n",
    "\n",
    "raw_pdf = 'raw_pdf' # to save raw data (.pdf).\n",
    "if not os.path.exists(raw_pdf):\n",
    "    os.mkdir(raw_pdf) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26b4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save text file with the names of already downloaded files\n",
    "\n",
    "download_record = 'download_record'\n",
    "if not os.path.exists(download_record):\n",
    "    os.mkdir(download_record) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147227ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to download the trimmed PDF files (these are PDF inputs for the extraction and cleanup code)\n",
    "\n",
    "input_pdf = 'input_pdf'\n",
    "if not os.path.exists(input_pdf):\n",
    "    os.makedirs(input_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6dc316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to download the trimmed PDF files (these are PDF inputs for the extraction and cleanup code)\n",
    "\n",
    "trimmed_record = 'trimmed_record'\n",
    "if not os.path.exists(trimmed_record):\n",
    "    os.makedirs(trimmed_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d69eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save dataframes generated record by year\n",
    "\n",
    "dataframes_record = 'dataframes_record'\n",
    "if not os.path.exists(dataframes_record):\n",
    "    os.makedirs(dataframes_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "821d5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to download the trimmed PDF files (these are PDF inputs for the extraction and cleanup code)\n",
    "\n",
    "pseudo_raw_pdf = 'pseudo_raw_pdf'\n",
    "if not os.path.exists(pseudo_raw_pdf):\n",
    "    os.makedirs(pseudo_raw_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f56ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d0038d2",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7cf410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba6e051c",
   "metadata": {},
   "source": [
    "<div id=\"1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f3192",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: charter;\">1.</span> <span style = \"color: dark; font-family: charter;\">PDF Downloader</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98250a64",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Our main source for data collection is the <a href=\"https://www.bcrp.gob.pe/\" style=\"color: rgb(0, 153, 123)\">BCRP's web page</a> (.../publicaciones/nota-semanal). The BCRP publishes \"Notas Semanales\", documents that contain, among other information, tables of GDP and sectoral GDP growth rate values for annual, quarterly and monthly frequencies.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3a388",
   "metadata": {},
   "source": [
    "-- (pending) Selenium tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcc4e3",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    The provided code will download all the 'Notas Semanales' files in PDF format from this web page.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the BCRP URL\n",
    "bcrp_url = \"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\"  # Never replace this URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269c03d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    The provided code will download all the 'Notas Semanales' files in PDF format from this web page.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68363e06",
   "metadata": {},
   "source": [
    "# Con ventana input al usuario y alarma por cada lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0199bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pygame\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Inicializar pygame\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Carpeta donde se almacenarán los archivos de sonido\n",
    "sound_folder = \"sound\"\n",
    "\n",
    "# Lista de archivos de sonido disponibles\n",
    "available_sounds = os.listdir(sound_folder)\n",
    "\n",
    "# Seleccionar un sonido aleatorio\n",
    "random_sound = random.choice(available_sounds)\n",
    "\n",
    "# Ruta completa del sonido aleatorio\n",
    "sound_path = os.path.join(sound_folder, random_sound)\n",
    "\n",
    "# Cargar el sonido seleccionado\n",
    "pygame.mixer.music.load(sound_path)\n",
    "\n",
    "# Función para reproducir el sonido\n",
    "def play_sound():\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "# List to keep track of successfully downloaded files\n",
    "downloaded_files = []\n",
    "\n",
    "# Folder where downloaded PDF files will be saved\n",
    "raw_pdf = \"raw_pdf\"  # Replace with the actual path\n",
    "\n",
    "# Folder where the download record file will be saved\n",
    "download_record = \"download_record\"  # Replace with the actual path\n",
    "\n",
    "# Load the list of previously downloaded files if it exists\n",
    "if os.path.exists(os.path.join(download_record, \"downloaded_files.txt\")):\n",
    "    with open(os.path.join(download_record, \"downloaded_files.txt\"), \"r\") as f:\n",
    "        downloaded_files = f.read().splitlines()\n",
    "\n",
    "# Web driver setup\n",
    "driver_path = os.environ.get('driver_path')\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "def random_wait(min_time, max_time):\n",
    "    wait_time = random.uniform(min_time, max_time)\n",
    "    print(f\"Waiting randomly for {wait_time:.2f} seconds\")\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "def download_pdf(pdf_link):\n",
    "    # Click the link using JavaScript\n",
    "    driver.execute_script(\"arguments[0].click();\", pdf_link)\n",
    "\n",
    "    # Wait for the new page to fully open (adjust timing as necessary)\n",
    "    wait.until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "    # Switch to the new window or tab\n",
    "    windows = driver.window_handles\n",
    "    driver.switch_to.window(windows[1])\n",
    "\n",
    "    # Get the current URL (may vary based on site-specific logic)\n",
    "    new_url = driver.current_url\n",
    "    print(f\"{download_counter}. New URL: {new_url}\")\n",
    "\n",
    "    # Get the file name from the URL\n",
    "    file_name = new_url.split(\"/\")[-1]\n",
    "\n",
    "    # Form the full destination path\n",
    "    destination_path = os.path.join(raw_pdf, file_name)\n",
    "\n",
    "    # Download the PDF\n",
    "    response = requests.get(new_url, stream=True)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Save the PDF content to the local file\n",
    "        with open(destination_path, 'wb') as pdf_file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                pdf_file.write(chunk)\n",
    "\n",
    "        print(f\"PDF downloaded successfully at: {destination_path}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error downloading the PDF. Response code: {response.status_code}\")\n",
    "\n",
    "    # Close the new window or tab\n",
    "    driver.close()\n",
    "\n",
    "    # Switch back to the main window\n",
    "    driver.switch_to.window(windows[0])\n",
    "\n",
    "# Number of downloads per batch\n",
    "downloads_per_batch = 5\n",
    "# Total number of downloads\n",
    "total_downloads = 25\n",
    "\n",
    "try:\n",
    "    # Open the test page\n",
    "    driver.get(bcrp_url)\n",
    "    print(\"Site opened successfully\")\n",
    "\n",
    "    # Wait for the container area to be present\n",
    "    wait = WebDriverWait(driver, 60)\n",
    "    container_area = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"rightside\"]')))\n",
    "\n",
    "    # Get all the links within the container area\n",
    "    pdf_links = container_area.find_elements(By.XPATH, './/a')\n",
    "\n",
    "    # Reverse the order of links\n",
    "    pdf_links = list(reversed(pdf_links))\n",
    "\n",
    "    # Initialize download counter\n",
    "    download_counter = 0\n",
    "\n",
    "    # Iterate over reversed links and download PDFs in batches\n",
    "    for pdf_link in pdf_links:\n",
    "        download_counter += 1\n",
    "\n",
    "        # Get the file name from the URL\n",
    "        new_url = pdf_link.get_attribute(\"href\")\n",
    "        file_name = new_url.split(\"/\")[-1]\n",
    "\n",
    "        # Check if the file has already been downloaded\n",
    "        if file_name in downloaded_files:\n",
    "            print(f\"{download_counter}. The file {file_name} has already been downloaded previously. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Try to download the file\n",
    "        try:\n",
    "            download_pdf(pdf_link)\n",
    "\n",
    "            # Update the list of downloaded files\n",
    "            downloaded_files.append(file_name)\n",
    "\n",
    "            # Save the file name in the record\n",
    "            with open(os.path.join(download_record, \"downloaded_files.txt\"), \"a\") as f:\n",
    "                f.write(file_name + \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading the file {file_name}: {str(e)}\")\n",
    "\n",
    "        # If the download count reaches a multiple of batch size, notify\n",
    "        if download_counter % downloads_per_batch == 0:\n",
    "            print(f\"Batch {download_counter // downloads_per_batch} of {downloads_per_batch} completed\")\n",
    "\n",
    "        # If the download count reaches a multiple of 25, ask the user if they want to continue\n",
    "        if download_counter % 25 == 0:\n",
    "            play_sound()\n",
    "            user_input = input(\"Do you want to continue downloading? (Enter 's' to continue, any other key to stop): \")\n",
    "            pygame.mixer.music.stop()\n",
    "            if user_input.lower() != 's':\n",
    "                break\n",
    "\n",
    "        # Random wait before the next iteration\n",
    "        random_wait(5, 10)\n",
    "\n",
    "        # If total downloads reached, break out of loop\n",
    "        if download_counter == total_downloads:\n",
    "            print(f\"All downloads completed ({total_downloads} in total)\")\n",
    "            break\n",
    "\n",
    "except StaleElementReferenceException:\n",
    "    print(\"StaleElementReferenceException occurred. Retrying...\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser when finished\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168995d",
   "metadata": {},
   "source": [
    "### Ordenando pdf por años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee016a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Obtener la lista de archivos en el directorio\n",
    "archivos = os.listdir(raw_pdf)\n",
    "\n",
    "# Iterar sobre cada archivo\n",
    "for archivo in archivos:\n",
    "    # Obtener el año del nombre del archivo\n",
    "    nombre, extension = os.path.splitext(archivo)\n",
    "    año = None\n",
    "    partes_nombre = nombre.split('-')\n",
    "    for parte in partes_nombre:\n",
    "        if parte.isdigit() and len(parte) == 4:\n",
    "            año = parte\n",
    "            break\n",
    "\n",
    "    # Si se encontró el año, mover el archivo a la carpeta correspondiente\n",
    "    if año:\n",
    "        carpeta_destino = os.path.join(raw_pdf, año)\n",
    "        # Crear la carpeta si no existe\n",
    "        if not os.path.exists(carpeta_destino):\n",
    "            os.makedirs(carpeta_destino)\n",
    "        # Mover el archivo a la carpeta destino\n",
    "        shutil.move(os.path.join(raw_pdf, archivo), carpeta_destino)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d24d2",
   "metadata": {},
   "source": [
    "# Recortando PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd2370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import tkinter as tk\n",
    "\n",
    "# Rutas de directorios\n",
    "trimmed_record_dir = 'trimmed_record'\n",
    "trimmed_record_file = 'trimmed_files.txt'\n",
    "\n",
    "class PopupWindow(tk.Toplevel):\n",
    "    def __init__(self, root, message):\n",
    "        super().__init__(root)\n",
    "        self.root = root\n",
    "        self.title(\"Atención!\")\n",
    "        self.message = message\n",
    "        self.result = None\n",
    "        self.configure_window()\n",
    "        self.create_widgets()\n",
    "\n",
    "    def configure_window(self):\n",
    "        self.resizable(False, False)  # Evita cambiar el tamaño de la ventana\n",
    "\n",
    "    def create_widgets(self):\n",
    "        self.label = tk.Label(self, text=self.message, wraplength=250)  # Ajusta el texto si es demasiado largo\n",
    "        self.label.pack(pady=10, padx=10)\n",
    "        self.btn_frame = tk.Frame(self)\n",
    "        self.btn_frame.pack(pady=5)\n",
    "        self.btn_yes = tk.Button(self.btn_frame, text=\"Sí\", command=self.yes)\n",
    "        self.btn_yes.pack(side=tk.LEFT, padx=5)\n",
    "        self.btn_no = tk.Button(self.btn_frame, text=\"No\", command=self.no)\n",
    "        self.btn_no.pack(side=tk.RIGHT, padx=5)\n",
    "\n",
    "        # Calcula el tamaño de la ventana en función del tamaño del texto\n",
    "        width = self.label.winfo_reqwidth() + 20\n",
    "        height = self.label.winfo_reqheight() + 100\n",
    "        self.geometry(f\"{width}x{height}\")\n",
    "\n",
    "    def yes(self):\n",
    "        self.result = True\n",
    "        self.destroy()\n",
    "\n",
    "    def no(self):\n",
    "        self.result = False\n",
    "        self.destroy()\n",
    "\n",
    "def search_keywords(pdf_file, keywords):\n",
    "    pages_with_keywords = []\n",
    "    with fitz.open(pdf_file) as doc:\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc.load_page(page_num)\n",
    "            text = page.get_text()\n",
    "            if any(keyword in text for keyword in keywords):\n",
    "                pages_with_keywords.append(page_num)\n",
    "    return pages_with_keywords\n",
    "\n",
    "def trim_pdf(pdf_file, pages):\n",
    "    if not pages:\n",
    "        print(f\"No se encontraron páginas con palabras clave en {pdf_file}\")\n",
    "        return 0\n",
    "    \n",
    "    new_pdf_file = os.path.join(input_pdf, os.path.basename(pdf_file))\n",
    "    \n",
    "    with fitz.open(pdf_file) as doc:\n",
    "        new_doc = fitz.open()\n",
    "        new_doc.insert_pdf(doc, from_page=0, to_page=0)\n",
    "        for page_num in pages:\n",
    "            new_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)\n",
    "        new_doc.save(new_pdf_file)\n",
    "    \n",
    "    num_pages_new_pdf = new_doc.page_count\n",
    "    print(f\"El PDF recortado '{new_pdf_file}' tiene {num_pages_new_pdf} páginas.\")\n",
    "\n",
    "    if num_pages_new_pdf == 5:\n",
    "        final_doc = fitz.open()\n",
    "        final_doc.insert_pdf(new_doc, from_page=0, to_page=0)\n",
    "        final_doc.insert_pdf(new_doc, from_page=1, to_page=1)\n",
    "        final_doc.insert_pdf(new_doc, from_page=3, to_page=3)\n",
    "        final_doc.save(new_pdf_file)\n",
    "\n",
    "        num_pages_new_pdf = final_doc.page_count\n",
    "        print(f\"Solo se conservaron la portada y las páginas con 2 tablas de interés en el PDF recortado '{new_pdf_file}'.\")\n",
    "    else:\n",
    "        print(f\"Se conservaron todas las páginas en el PDF recortado '{new_pdf_file}'.\")\n",
    "\n",
    "    return num_pages_new_pdf\n",
    "\n",
    "def read_trimmed_files():\n",
    "    trimmed_files_path = os.path.join(trimmed_record_dir, trimmed_record_file)\n",
    "    if not os.path.exists(trimmed_files_path):\n",
    "        return set()\n",
    "    \n",
    "    with open(trimmed_files_path, 'r') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "def write_trimmed_files(trimmed_files):\n",
    "    trimmed_files_path = os.path.join(trimmed_record_dir, trimmed_record_file)\n",
    "    sorted_filenames = sorted(trimmed_files)  # Sort the filenames\n",
    "    with open(trimmed_files_path, 'w') as file:\n",
    "        for filename in sorted_filenames:\n",
    "            file.write(filename + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = [\"ECONOMIC SECTORS\"]\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Oculta la ventana principal de Tkinter\n",
    "\n",
    "    trimmed_files = read_trimmed_files()\n",
    "    processing_counter = 1\n",
    "\n",
    "    for folder in os.listdir(raw_pdf):\n",
    "        folder_path = os.path.join(raw_pdf, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(\"Procesando carpeta:\", folder)\n",
    "            num_pdfs_trimmed = 0\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".pdf\"):\n",
    "                    pdf_file = os.path.join(folder_path, filename)\n",
    "                    if filename in trimmed_files:\n",
    "                        print(f\"{processing_counter}. El PDF '{filename}' ya ha sido recortado y guardado en '{input_pdf}'...\")\n",
    "                        processing_counter += 1\n",
    "                        continue\n",
    "                    print(f\"{processing_counter}. Procesando:\", pdf_file)\n",
    "                    \n",
    "                    pages_with_keywords = search_keywords(pdf_file, keywords)\n",
    "                    num_pages_new_pdf = trim_pdf(pdf_file, pages_with_keywords)\n",
    "                    if num_pages_new_pdf > 0:\n",
    "                        num_pdfs_trimmed += 1\n",
    "                        trimmed_files.add(filename)\n",
    "                        processing_counter += 1\n",
    "            \n",
    "            write_trimmed_files(trimmed_files)\n",
    "\n",
    "            message = f\"{num_pdfs_trimmed} PDFs han sido recortados en la carpeta {folder}. ¿Desea continuar?\"\n",
    "            popup = PopupWindow(root, message)\n",
    "            root.wait_window(popup)\n",
    "            if not popup.result:\n",
    "                break\n",
    "                \n",
    "    print(\"Proceso completado para todos los PDFs en el directorio:\", input_pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643079a",
   "metadata": {},
   "source": [
    "### Ordenando pdf por años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Obtener la lista de archivos en el directorio\n",
    "archivos = os.listdir(input_pdf)\n",
    "\n",
    "# Iterar sobre cada archivo\n",
    "for archivo in archivos:\n",
    "    # Obtener el año del nombre del archivo\n",
    "    nombre, extension = os.path.splitext(archivo)\n",
    "    año = None\n",
    "    partes_nombre = nombre.split('-')\n",
    "    for parte in partes_nombre:\n",
    "        if parte.isdigit() and len(parte) == 4:\n",
    "            año = parte\n",
    "            break\n",
    "\n",
    "    # Si se encontró el año, mover el archivo a la carpeta correspondiente\n",
    "    if año:\n",
    "        carpeta_destino = os.path.join(input_pdf, año)\n",
    "        # Crear la carpeta si no existe\n",
    "        if not os.path.exists(carpeta_destino):\n",
    "            os.makedirs(carpeta_destino)\n",
    "        # Mover el archivo a la carpeta destino\n",
    "        shutil.move(os.path.join(input_pdf, archivo), carpeta_destino)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045beb7b",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a384d2",
   "metadata": {},
   "source": [
    "<div id=\"2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e1e92",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.</span> <span style = \"color: dark; font-family: charter;\">Extracting Tables (and data cleaning)</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ed1db",
   "metadata": {},
   "source": [
    "<div id=\"2-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af999f",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.1.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        pdfplumber\n",
    "    </span> \n",
    "    demo\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebeb3a",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Import\n",
    "    <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        pdfplumber\n",
    "    </span>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "print(f'This library version is: {pdfplumber.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b481d4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Load the PDF\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(\".\\\\ns-10-2013.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad376ace",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Get the page 82\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_82 = pdf.pages[81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33facebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the page to a higher resolution image (e.g., 300 DPI).\n",
    "image = p_82.to_image(resolution=300)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629ee4d",
   "metadata": {},
   "source": [
    "<div id=\"2-1-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49929c",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.1.1.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    What data would we get if we used the default settings?\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ea8f5e",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    We can check by using <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        PageImage.debug_tablefinder()\n",
    "    </span>:\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd775601",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.reset().debug_tablefinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ae18d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    The default settings correctly identify the table's vertical demarcations, but don't capture the horizontal demarcations between each group of five states/territories. So:\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b70f0",
   "metadata": {},
   "source": [
    "<div id=\"2-1-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8947130a",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.1.2.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Using custom <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        <b>.extract_table\n",
    "            </b>\n",
    "    </span>'s settings\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c473c71b",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    <ul>\n",
    "        <li>Because the columns are separated by lines, we use <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        vertical_strategy=\"lines\"\n",
    "    </span>.\n",
    "            </li>\n",
    "        <li>Because the rows are, primarily, separated by gutters between the text, we use <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        horizontal_strategy=\"text\"\n",
    "    </span>.\n",
    "            <li>To snap together a handful of the gutters at the top which aren't fully flush with one another, we use <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        snap_y_tolerance\n",
    "    </span>which snaps horizontal lines within a certain distance to the same vertical alignment.\n",
    "                </li>\n",
    "        <li>And because the left and right-hand extremities of the text aren't quite flush with the vertical lines, we use <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        \"intersection_tolerance\": 15\n",
    "    </span>.\n",
    "            </li>\n",
    "        </ul>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_settings = {\n",
    "    \"vertical_strategy\": \"lines\", \n",
    "    \"horizontal_strategy\": \"lines\",\n",
    "    \"explicit_vertical_lines\": [],\n",
    "    \"explicit_horizontal_lines\": [],\n",
    "    \"snap_tolerance\": 3,\n",
    "    \"snap_x_tolerance\": 3,\n",
    "    \"snap_y_tolerance\": 3,\n",
    "    \"join_tolerance\": 3,\n",
    "    \"join_x_tolerance\": 3,\n",
    "    \"join_y_tolerance\": 3,\n",
    "    \"edge_min_length\": 3,\n",
    "    \"min_words_vertical\": 3,\n",
    "    \"min_words_horizontal\": 1,\n",
    "    \"text_keep_blank_chars\": False,\n",
    "    \"text_tolerance\": 3,\n",
    "    \"text_x_tolerance\": 3,\n",
    "    \"text_y_tolerance\": 3,\n",
    "    \"intersection_tolerance\": 3,\n",
    "    \"intersection_x_tolerance\": 3,\n",
    "    \"intersection_y_tolerance\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c25c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.reset().debug_tablefinder(table_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2093bf",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5a767",
   "metadata": {},
   "source": [
    "<div id=\"2-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357318e8",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.2.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Extracting tables and generating dataframes (includes data cleanup)\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100857d4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    We would like to get specific tables: information on GDP growth rates with annual, quarterly and monthly frequency. We don't need other tables also related to GDP that don't meet these requirements. Extraction will be easier if we use keywords.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77729e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to search in the page text\n",
    "keywords = [\"PRODUCTO BRUTO INTERNO\", \"SECTORES ECONÓMICOS\", \"PBI\", \"GDP\", \"Variaciones\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070fb47",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    The code iterates through each PDF and extracts the two required tables from each. The extracted information is then transformed into dataframes and the columns and values are cleaned up to conform to Python conventions (pythonic).\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af4d90",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df05629",
   "metadata": {},
   "source": [
    "### Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f4190e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_characters_first_row(texto):\n",
    "    texto = re.sub(r'\\s*-\\s*', '-', texto)  # Remueve espacios alrededor de guiones\n",
    "    texto = re.sub(r'[^a-zA-Z0-9\\s-]', '', texto)  # Remueve caracteres raros excepto letras, dígitos y guiones\n",
    "    return texto\n",
    "\n",
    "def remove_rare_characters(texto):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', texto)\n",
    "\n",
    "def remove_tildes(texto):\n",
    "    return ''.join((c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8eb2f",
   "metadata": {},
   "source": [
    "### Común"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25bf119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.\n",
    "def drop_nan_rows(df):\n",
    "    df = df.dropna(how='all')\n",
    "    return df\n",
    "\n",
    "# 1. \n",
    "def drop_nan_columns(df):\n",
    "    return df.dropna(axis=1, how='all')\n",
    "\n",
    "# 2.\n",
    "def swap_first_second_row(df):\n",
    "    temp = df.iloc[0, 0]\n",
    "    df.iloc[0, 0] = df.iloc[1, 0]\n",
    "    df.iloc[1, 0] = temp\n",
    "\n",
    "    temp = df.iloc[0, -1]\n",
    "    df.iloc[0, -1] = df.iloc[1, -1]\n",
    "    df.iloc[1, -1] = temp\n",
    "    return df\n",
    "\n",
    "# 8. \n",
    "def reset_index(df):\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "# 5.\n",
    "def remove_digit_slash(df):\n",
    "    # Aplica la función de reemplazo a la primera columna y a las dos últimas columnas\n",
    "    df.iloc[:, [0, -2, -1]] = df.iloc[:, [0, -2, -1]].apply(lambda x: x.str.replace(r'\\d+/', '', regex=True))\n",
    "    return df\n",
    "\n",
    "# 9. AUX (ROBUSTO)\n",
    "\n",
    "def separate_text_digits(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if any(char.isdigit() for char in str(row.iloc[-2])) and any(char.isalpha() for char in str(row.iloc[-2])):\n",
    "            if pd.isnull(row.iloc[-1]):\n",
    "                df.loc[index, df.columns[-1]] = ''.join(filter(lambda x: x.isalpha() or x == ' ', str(row.iloc[-2])))\n",
    "                df.loc[index, df.columns[-2]] = ''.join(filter(lambda x: not (x.isalpha() or x == ' '), str(row.iloc[-2])))\n",
    "            \n",
    "            # Check if comma or dot is used as decimal separator\n",
    "            if ',' in str(row.iloc[-2]):\n",
    "                split_values = str(row.iloc[-2]).split(',')\n",
    "            elif '.' in str(row.iloc[-2]):\n",
    "                split_values = str(row.iloc[-2]).split('.')\n",
    "            else:\n",
    "                # If neither comma nor dot found, assume no decimal part\n",
    "                split_values = [str(row.iloc[-2]), '']\n",
    "                \n",
    "            cleaned_integer = ''.join(filter(lambda x: x.isdigit() or x == '-', split_values[0]))\n",
    "            cleaned_decimal = ''.join(filter(lambda x: x.isdigit(), split_values[1]))\n",
    "            if cleaned_decimal:\n",
    "                # Use comma as decimal separator\n",
    "                cleaned_numeric = cleaned_integer + ',' + cleaned_decimal\n",
    "            else:\n",
    "                cleaned_numeric = cleaned_integer\n",
    "            df.loc[index, df.columns[-2]] = cleaned_numeric\n",
    "    return df\n",
    "\n",
    "\n",
    "# 4. \n",
    "def extract_years(df):\n",
    "    year_columns = [col for col in df.columns if re.match(r'\\b\\d{4}\\b', col)]\n",
    "    #print(\"Años (4 dígitos) extraídos:\")\n",
    "    #print(year_columns)\n",
    "    return year_columns\n",
    "\n",
    "# 6. \n",
    "def first_row_columns(df):\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.drop(df.index[0])\n",
    "    return df\n",
    "\n",
    "# 15.\n",
    "def clean_columns_values(df):\n",
    "    df.columns = df.columns.str.lower()\n",
    "    # Only normalize string column names\n",
    "    df.columns = [unicodedata.normalize('NFKD', col).encode('ASCII', 'ignore').decode('utf-8') if isinstance(col, str) else col for col in df.columns]\n",
    "    df.columns = df.columns.str.replace(' ', '_').str.replace('ano', 'year').str.replace('-', '_')\n",
    "    \n",
    "    text_columns = df.select_dtypes(include='object').columns\n",
    "    for col in df.columns:\n",
    "        df.loc[:, col] = df[col].apply(lambda x: remove_tildes(x) if isinstance(x, str) else x)\n",
    "        df.loc[:, col] = df[col].apply(lambda x: str(x).replace(',', '.') if isinstance(x, (int, float, str)) else x)\n",
    "    df.loc[:, 'sectores_economicos'] = df['sectores_economicos'].str.lower()\n",
    "    df.loc[:, 'economic_sectors'] = df['economic_sectors'].str.lower()\n",
    "    df.loc[:, 'sectores_economicos'] = df['sectores_economicos'].apply(remove_rare_characters)\n",
    "    df.loc[:, 'economic_sectors'] = df['economic_sectors'].apply(remove_rare_characters)\n",
    "    return df\n",
    "\n",
    "# 16.\n",
    "def convertir_float(df):\n",
    "    excluded_columns = ['sectores_economicos', 'economic_sectors']\n",
    "    columns_to_convert = [col for col in df.columns if col not in excluded_columns]\n",
    "    df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    return df\n",
    "\n",
    "# 15.\n",
    "def relocate_last_column(df):\n",
    "    last_column = df.pop(df.columns[-1])\n",
    "    df.insert(1, last_column.name, last_column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed8745",
   "metadata": {},
   "source": [
    "### Exclusiva Tabla 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6981f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATIPIC LAST COLUMNS\n",
    "def relocate_last_columns(df):\n",
    "    if not pd.isna(df.iloc[1, -1]):\n",
    "        # Create a new column with NaN\n",
    "        new_column = 'col_' + ''.join(map(str, np.random.randint(1, 5, size=1)))\n",
    "        df[new_column] = np.nan\n",
    "        \n",
    "        # Get 'ECONOMIC SECTORS' and relocate\n",
    "        insert_value_1 = df.iloc[0, -2]\n",
    "        # Convert the value to string before assignment\n",
    "        insert_value_1 = str(insert_value_1)\n",
    "        # Ensure the dtype of the last column is object (string) to accommodate string values\n",
    "        df.iloc[:, -1] = df.iloc[:, -1].astype('object')\n",
    "        df.iloc[0, -1] = insert_value_1\n",
    "        \n",
    "        # NaN first obs\n",
    "        df.iloc[0,-2] = np.nan\n",
    "    return df\n",
    "\n",
    "# Extraer meses\n",
    "\n",
    "def get_months_sublist_list(df, year_columns):\n",
    "    first_row = df.iloc[0]\n",
    "    # Initialize the list of sublists\n",
    "    months_sublist_list = []\n",
    "\n",
    "    # Initialize the current sublist\n",
    "    months_sublist = []\n",
    "\n",
    "    # Iterate over the elements of the first row\n",
    "    for item in first_row:\n",
    "        # Check if the item meets the requirements\n",
    "        if len(str(item)) == 3:\n",
    "            months_sublist.append(item)\n",
    "        elif '-' in item or str(item) == 'year':\n",
    "            months_sublist.append(item)\n",
    "            months_sublist_list.append(months_sublist)\n",
    "            months_sublist = []\n",
    "\n",
    "    # Add the last sublist if it's not empty\n",
    "    if months_sublist:\n",
    "        months_sublist_list.append(months_sublist)\n",
    "\n",
    "    new_elements = []\n",
    "\n",
    "    # Check if year_columns is not empty\n",
    "    if year_columns:\n",
    "        for i, year in enumerate(year_columns):\n",
    "            # Check if index i is valid for quarters_sublist_list\n",
    "            if i < len(months_sublist_list):\n",
    "                for element in months_sublist_list[i]:\n",
    "                    new_elements.append(f\"{year}_{element}\")\n",
    "                    \n",
    "    two_first_elements = df.iloc[0][:2].tolist()\n",
    "\n",
    "    # Ensure that the two_first_elements are added if they are not in new_elements\n",
    "    for index in range(len(two_first_elements) - 1, -1, -1):\n",
    "        if two_first_elements[index] not in new_elements:\n",
    "            new_elements.insert(0, two_first_elements[index])\n",
    "\n",
    "    # Ensure that the length of new_elements matches the number of columns in df\n",
    "    while len(new_elements) < len(df.columns):\n",
    "        new_elements.append(None)\n",
    "\n",
    "    temp_df = pd.DataFrame([new_elements], columns=df.columns)\n",
    "    df.iloc[0] = temp_df.iloc[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_year_column(df):\n",
    "    # List to store the found years\n",
    "    found_years = []\n",
    "\n",
    "    # Iterating over the column names of the DataFrame\n",
    "    for column in df.columns:\n",
    "        # Checking if the column name is a year (4 digits)\n",
    "        if column.isdigit() and len(column) == 4:\n",
    "            found_years.append(column)\n",
    "\n",
    "    # If more than one year is found, do nothing\n",
    "    if len(found_years) > 1:\n",
    "        pass\n",
    "    # If exactly one year is found, implement additional code\n",
    "    elif len(found_years) == 1:\n",
    "        # Getting the name of the found year\n",
    "        year_name = found_years[0]\n",
    "        print(\"The name of the column representing the year is:\", year_name)\n",
    "\n",
    "        # Getting the first row of the DataFrame\n",
    "        first_row = df.iloc[0]\n",
    "\n",
    "        # Searching for the first column containing the word \"year\" or some hyphen-separated expression\n",
    "        column_contains_year = first_row[first_row.astype(str).str.contains(r'\\byear\\b')]\n",
    "\n",
    "        if not column_contains_year.empty:\n",
    "            # Getting the name of the first column containing 'year' or some hyphen-separated expression in the first row\n",
    "            column_contains_year_name = column_contains_year.index[0]\n",
    "            print(\"The name of the first column containing 'year' or some hyphen-separated expression in the first row is:\", column_contains_year_name)\n",
    "\n",
    "            # Getting the indices of the columns\n",
    "            column_contains_year_index = df.columns.get_loc(column_contains_year_name)\n",
    "            year_name_index = df.columns.get_loc(year_name)\n",
    "            print(\"The index of the column containing 'year' is:\", column_contains_year_index)\n",
    "            print(\"The index of the column representing the year is:\", year_name_index)\n",
    "\n",
    "            # Checking if the column representing the year is to the right or to the left of column_contains_year\n",
    "            if column_contains_year_index < year_name_index:\n",
    "                print(\"The year column is to the right of the column containing 'year'.\")\n",
    "                # Adding one to the year\n",
    "                new_year = str(int(year_name) - 1)\n",
    "                # Renaming the column containing 'year' with the new year\n",
    "                df.rename(columns={column_contains_year_name: new_year}, inplace=True)\n",
    "                print(f\"The column containing 'year' is now named '{new_year}'.\")\n",
    "            elif column_contains_year_index > year_name_index:\n",
    "                print(\"The year column is to the left of the column containing 'year'.\")\n",
    "                # Subtracting one from the year\n",
    "                new_year = str(int(year_name) + 1)\n",
    "                # Renaming the year column with the new year\n",
    "                df.rename(columns={column_contains_year_name: new_year}, inplace=True)\n",
    "                print(f\"The column containing 'year' is now named '{new_year}'.\")\n",
    "            else:\n",
    "                print(\"The year column is in the same position as the column containing 'year'.\")\n",
    "        else:\n",
    "            print(\"No columns containing 'year' were found in the first row.\")\n",
    "    # If no year is found, print a message\n",
    "    else:\n",
    "        print(\"No years were found in the column names.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "def clean_first_row(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            if isinstance(df.at[0, col], str):\n",
    "                df.at[0, col] = df.at[0, col].lower()  # Convertir a minúsculas solo si es un objeto\n",
    "                df.at[0, col] = remove_tildes(df.at[0, col])\n",
    "                df.at[0, col] = remove_rare_characters_first_row(df.at[0, col])\n",
    "                # Reemplazar 'ano' por 'year'\n",
    "                df.at[0, col] = df.at[0, col].replace('ano', 'year')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def intercambiar_valores(df):\n",
    "    # Verificar si hay al menos dos columnas en el DataFrame\n",
    "    if len(df.columns) < 2:\n",
    "        print(\"El DataFrame tiene menos de dos columnas. No se pueden intercambiar valores.\")\n",
    "        return df\n",
    "\n",
    "    # Verificar si hay valores NaN en la última columna\n",
    "    if df.iloc[:, -1].isnull().any():\n",
    "        # Obtener índice de filas con NaN en la última columna\n",
    "        last_column_rows_nan = df[df.iloc[:, -1].isnull()].index\n",
    "\n",
    "        # Iterar sobre las filas con NaN en la última columna\n",
    "        for idx in last_column_rows_nan:\n",
    "            # Verificar si el índice está dentro del rango de las columnas\n",
    "            if -2 >= -len(df.columns):\n",
    "                # Intercambiar los valores de la última columna y la penúltima columna\n",
    "                df.iloc[idx, -1], df.iloc[idx, -2] = df.iloc[idx, -2], df.iloc[idx, -1]\n",
    "            else:\n",
    "                print(f\"Índice fuera de rango para la fila {idx}. No se pueden intercambiar valores.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def replace_var_perc_first_column(df):\n",
    "    # Regular expression to search for \"Var. %\" or \"Var.%\"\n",
    "    regex = re.compile(r'Var\\. ?%')\n",
    "\n",
    "    # Iterate over the rows of the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # Convert the value in the first column to a string\n",
    "        value = str(row.iloc[0])\n",
    "\n",
    "        # Check if the value matches the regular expression\n",
    "        if regex.search(value):\n",
    "            # Replace only the characters that match the regular expression\n",
    "            df.at[index, df.columns[0]] = regex.sub(\"variacion porcentual\", value)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# 8.\n",
    "number_moving_average = 'three' # Keep a space at the end\n",
    "\n",
    "def replace_number_moving_average(df):\n",
    "    for index, row in df.iterrows():\n",
    "        # Buscar la expresión regular en la penúltima o última columna\n",
    "        if pd.notnull(row.iloc[-1]) and re.search(r'(\\d\\s*-)', str(row.iloc[-1])):\n",
    "            df.at[index, df.columns[-1]] = re.sub(r'(\\d\\s*-)', f'{number_moving_average}-', str(row.iloc[-1]))\n",
    "        #elif pd.notnull(row.iloc[-2]) and re.search(r'(\\d\\s*-)', str(row.iloc[-2])):\n",
    "        #   df.at[index, df.columns[-2]] = re.sub(r'(\\d\\s*-)', f'{number_moving_average}-', str(row.iloc[-2]))\n",
    "    return df\n",
    "\n",
    "\n",
    "# 7.\n",
    "def replace_var_perc_last_columns(df):\n",
    "    # Expresión regular para buscar \"Var. %\" o \"Var.%\"\n",
    "    regex = re.compile(r'(Var\\. ?%)(.*)')\n",
    "\n",
    "    # Iterar sobre las filas del dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # Verificar si el valor en la penúltima columna es una cadena no nula\n",
    "        if isinstance(row.iloc[-2], str) and regex.search(row.iloc[-2]):\n",
    "            # Realizar el reemplazo al final del valor de la penúltima columna\n",
    "            replaced_text = regex.sub(r'\\2 percent change', row.iloc[-2])\n",
    "            df.at[index, df.columns[-2]] = replaced_text.strip()\n",
    "        \n",
    "        # Verificar si el valor en la última columna es una cadena no nula\n",
    "        if isinstance(row.iloc[-1], str) and regex.search(row.iloc[-1]):\n",
    "            # Realizar el reemplazo al final del valor de la última columna\n",
    "            replaced_text = regex.sub(r'\\2 percent change', row.iloc[-1])\n",
    "            df.at[index, df.columns[-1]] = replaced_text.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Función para buscar y reemplazar en la segunda fila del DataFrame\n",
    "def replace_first_dot(df):\n",
    "    second_row = df.iloc[1]  # Segunda fila del DataFrame\n",
    "    \n",
    "    # Verificar si al menos una observación cumple con el patrón\n",
    "    if any(isinstance(cell, str) and re.match(r'^\\w+\\.\\s?\\w+', cell) for cell in second_row):\n",
    "        for col in df.columns:\n",
    "            if isinstance(second_row[col], str):  # Verificar si el valor es una cadena\n",
    "                if re.match(r'^\\w+\\.\\s?\\w+', second_row[col]):  # Verificar si cumple con el patrón Xxx.Xxx o Xxx. Xxx.\n",
    "                    df.at[1, col] = re.sub(r'(\\w+)\\.(\\s?\\w+)', r'\\1-\\2', second_row[col], count=1)  # Reemplazar solo el primer punto\n",
    "    return df\n",
    "\n",
    "def drop_rare_caracter_row(df):\n",
    "    # Buscar el caracter solitario \"}\" en cada fila y obtener un booleano para cada fila\n",
    "    rare_caracter_row = df.apply(lambda row: '}' in row.values, axis=1)\n",
    "    \n",
    "    # Filtrar el DataFrame para eliminar las filas con el caracter solitario \"}\"\n",
    "    df = df[~rare_caracter_row]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def split_column_by_pattern(df):\n",
    "    # Iteramos sobre las columnas del dataframe\n",
    "    for col in df.columns:\n",
    "        # Verificamos si la segunda fila de la columna contiene el patrón\n",
    "        if re.match(r'^[A-Z][a-z]+\\.\\s[A-Z][a-z]+\\.$', str(df.iloc[1][col])):\n",
    "            # Realizamos el split de la columna usando como criterio el espacio\n",
    "            split_values = df[col].str.split(expand=True)\n",
    "            # Guardamos los primeros valores en la columna original\n",
    "            df[col] = split_values[0]\n",
    "            # Guardamos los segundos valores en una nueva columna con el sufijo \"_split\"\n",
    "            new_col_name = col + '_split'\n",
    "            df.insert(df.columns.get_loc(col) + 1, new_col_name, split_values[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c629c56",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2014\\_07}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728de5fd",
   "metadata": {},
   "source": [
    "se: sectores económicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb2111c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_nan_se(df):\n",
    "    # Check if the first observation of the first column is NaN\n",
    "    if pd.isna(df.iloc[0, 0]) and df.iloc[0, 1] == \"SECTORES ECONÓMICOS\":\n",
    "        # Create a temporary copy of the values\n",
    "        column_1_value = df.iloc[0, 1]\n",
    "        # Swap values in the original row\n",
    "        df.iloc[0, 0] = column_1_value\n",
    "        df.iloc[0, 1] = np.nan\n",
    "    # Drop the second column\n",
    "    df = df.drop(df.columns[1], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12022c55",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2014\\_08}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb9254dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_first_row_with_columns(df):\n",
    "    # Check if the first row contains at least one year\n",
    "    if any(isinstance(element, str) and element.isdigit() and len(element) == 4 for element in df.iloc[0]):\n",
    "        # Replace NaN values in the first row with random column names\n",
    "        for col_index, value in enumerate(df.iloc[0]):\n",
    "            if pd.isna(value):\n",
    "                df.iloc[0, col_index] = f\"column_{col_index + 1}\"\n",
    "        # Replace column names with the values of the first row\n",
    "        df.columns = df.iloc[0]\n",
    "        # Drop the first row after setting it as column names\n",
    "        df = df.drop(df.index[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17072860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_column(df):\n",
    "    columna_a_expandir = df.columns[-2]\n",
    "    \n",
    "    def reemplazar_guiones(match_obj):\n",
    "        return match_obj.group(1) + ' ' + match_obj.group(2)    \n",
    "\n",
    "    if df[columna_a_expandir].str.contains(r'\\d').any() and df[columna_a_expandir].str.contains(r'[a-zA-Z]').any():\n",
    "        df[columna_a_expandir] = df[columna_a_expandir].apply(lambda x: re.sub(r'([a-zA-Z]+)\\s*-\\s*([a-zA-Z]+)', reemplazar_guiones, str(x)) if pd.notnull(x) else x)\n",
    "\n",
    "        \n",
    "        # Expresión regular para extraer palabras\n",
    "        pattern = re.compile(r'[a-zA-Z\\s]+$')\n",
    "\n",
    "        # Función para aplicar la lógica de extracción y reemplazo a cada fila\n",
    "        def extract_replace(row):\n",
    "            if pd.notnull(row[columna_a_expandir]) and isinstance(row[columna_a_expandir], str):  # Verifica que el valor no sea NaN y sea de tipo string\n",
    "                if row.name != 0:  # Para que empiece desde la segunda fila\n",
    "                    value_to_replace = pattern.search(row[columna_a_expandir])\n",
    "                    if value_to_replace:\n",
    "                        value_to_replace = value_to_replace.group().strip()\n",
    "                        row[df.columns[-1]] = value_to_replace\n",
    "                        row[columna_a_expandir] = re.sub(pattern, '', row[columna_a_expandir]).strip()\n",
    "            return row\n",
    "\n",
    "        # Aplicar la función a cada fila del DataFrame\n",
    "        df = df.apply(extract_replace, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "045c5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_values_1(df):\n",
    "    columna_a_expandir = df.columns[-2]\n",
    "    nuevas_columnas = df[columna_a_expandir].str.split(expand=True)\n",
    "    nuevas_columnas.columns = [f'{columna_a_expandir}_{i+1}' for i in range(nuevas_columnas.shape[1])]\n",
    "    posicion_insercion = len(df.columns) - 1\n",
    "    for col in reversed(nuevas_columnas.columns):\n",
    "        df.insert(posicion_insercion, col, nuevas_columnas[col])\n",
    "    df.drop(columns=[columna_a_expandir], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8245ec",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2015\\_11}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c8d0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revisar_primera_fila(df):\n",
    "    primera_fila = df.iloc[0]  # Obtenemos la primera fila del DataFrame\n",
    "    \n",
    "    for i, (col, valor) in enumerate(primera_fila.items()):  # Iteramos sobre los índices y valores de la primera fila\n",
    "        # Comprobamos si algún valor de la primera fila tiene dos años juntos\n",
    "        if re.search(r'\\b\\d{4}\\s\\d{4}\\b', str(valor)):\n",
    "            # Si es así, extraemos los dos años\n",
    "            anios = valor.split()\n",
    "            primer_anio = anios[0]\n",
    "            segundo_anio = anios[1]\n",
    "            \n",
    "            # Nombre de la columna original\n",
    "            nombre_columna_original = f'col_{i}'\n",
    "            df.at[0, col] = nombre_columna_original\n",
    "            \n",
    "            # Actualizamos el valor de la primera columna si es NaN con el primer año\n",
    "            if pd.isna(df.iloc[0, 0]):\n",
    "                df.iloc[0, 0] = primer_anio\n",
    "            \n",
    "            # Actualizamos el valor de la segunda columna si es NaN con el segundo año\n",
    "            if pd.isna(df.iloc[0, 1]):\n",
    "                df.iloc[0, 1] = segundo_anio\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc7df157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_with_previous_column(df):\n",
    "    columns = df.columns\n",
    "    \n",
    "    for i in range(len(columns) - 1):\n",
    "        if columns[i].endswith('_year'):\n",
    "            nan_indices = df[columns[i+1]].isnull()\n",
    "            df.loc[nan_indices, [columns[i], columns[i+1]]] = df.loc[nan_indices, [columns[i+1], columns[i]]].values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6fc072",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2016\\_15}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21c2508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revisar_primera_fila_1(df):\n",
    "    # Comprobar si el valor en la primera fila y primera columna es NaN\n",
    "    if pd.isnull(df.iloc[0, 0]):\n",
    "        # Comprobar si el valor en la penúltima columna y primera fila es un año (4 dígitos)\n",
    "        penultimate_column = df.iloc[0, -2]\n",
    "        if isinstance(penultimate_column, str) and len(penultimate_column) == 4 and penultimate_column.isdigit():\n",
    "            # Intercambiar los valores\n",
    "            df.iloc[0, 0] = penultimate_column\n",
    "            df.iloc[0, -2] = np.nan\n",
    "    \n",
    "    # Comprobar si el valor en la segunda columna y primera fila es NaN\n",
    "    if pd.isnull(df.iloc[0, 1]):\n",
    "        # Comprobar si el valor en la última columna y primera fila es un año (4 dígitos)\n",
    "        last_column = df.iloc[0, -1]\n",
    "        if isinstance(last_column, str) and len(last_column) == 4 and last_column.isdigit():\n",
    "            # Intercambiar los valores\n",
    "            df.iloc[0, 1] = last_column\n",
    "            df.iloc[0, -1] = np.nan\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe304de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_values_2(df):\n",
    "    columna_a_expandir = df.columns[-4]\n",
    "    nuevas_columnas = df[columna_a_expandir].str.split(expand=True)\n",
    "    nuevas_columnas.columns = [f'{columna_a_expandir}_{i+1}' for i in range(nuevas_columnas.shape[1])]\n",
    "    posicion_insercion = len(df.columns) - 3\n",
    "    for col in reversed(nuevas_columnas.columns):\n",
    "        df.insert(posicion_insercion, col, nuevas_columnas[col])\n",
    "    df.drop(columns=[columna_a_expandir], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b3bf9",
   "metadata": {},
   "source": [
    "### Exclusiva Tabla 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a68cc096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_first_row(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            if isinstance(df.at[0, col], str):\n",
    "                df.at[0, col] = df.at[0, col].lower()  # Convertir a minúsculas solo si es un objeto\n",
    "                df.at[0, col] = remove_tildes(df.at[0, col])\n",
    "                df.at[0, col] = remove_rare_characters_first_row(df.at[0, col])\n",
    "                # Reemplazar 'ano' por 'year'\n",
    "                df.at[0, col] = df.at[0, col].replace('ano', 'year')\n",
    "\n",
    "    return df\n",
    "\n",
    "# 2.\n",
    "def separate_years(df):\n",
    "    df = df.copy()  # Se crea una copia del DataFrame para evitar SettingWithCopyWarning\n",
    "    if isinstance(df.iloc[0, -2], str) and len(df.iloc[0, -2].split()) == 2:\n",
    "        years = df.iloc[0, -2].split()\n",
    "        if all(len(year) == 4 for year in years):\n",
    "            segundo_anio = years[1]\n",
    "            df.iloc[0, -2] = years[0]\n",
    "            df.insert(len(df.columns) - 1, 'new_column', [segundo_anio] + [None] * (len(df) - 1))\n",
    "    return df\n",
    "\n",
    "# 3.\n",
    "def find_roman_numerals(text):\n",
    "    pattern = r'\\b(?:I{1,3}|IV|V|VI{0,3}|IX|X)\\b'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "def relocate_roman_numerals(df):\n",
    "    numeros_romanos = find_roman_numerals(df.iloc[2, -1])\n",
    "    if numeros_romanos:\n",
    "        original_text = df.iloc[2, -1]\n",
    "        for roman_numeral in numeros_romanos:\n",
    "            original_text = original_text.replace(roman_numeral, '').strip()\n",
    "        df.iloc[2, -1] = original_text\n",
    "        df.at[2, 'new_column'] = ', '.join(numeros_romanos)\n",
    "        df.iloc[2, -1] = np.nan\n",
    "    return df\n",
    "\n",
    "# 4.\n",
    "def extract_mixed_values(df):\n",
    "    df = df.copy()  # Se crea una copia del DataFrame para evitar SettingWithCopyWarning\n",
    "    regex_pattern = r'(-?\\d+,\\d [a-zA-Z\\s]+)'\n",
    "    for index, row in df.iterrows():\n",
    "        antepenultima_obs = row.iloc[-3]\n",
    "        penultima_obs = row.iloc[-2]\n",
    "\n",
    "        if isinstance(antepenultima_obs, str) and pd.notnull(antepenultima_obs):\n",
    "            match = re.search(regex_pattern, antepenultima_obs)\n",
    "            if match:\n",
    "                parte_extraida = match.group(0)\n",
    "                if pd.isna(penultima_obs) or pd.isnull(penultima_obs):\n",
    "                    df.iloc[index, -2] = parte_extraida\n",
    "                    antepenultima_obs = re.sub(regex_pattern, '', antepenultima_obs).strip()\n",
    "                    df.iloc[index, -3] = antepenultima_obs\n",
    "    return df\n",
    "\n",
    "# 5.\n",
    "def replace_first_row_nan(df):\n",
    "    for col in df.columns:\n",
    "        if pd.isna(df.iloc[0][col]):\n",
    "            df.iloc[0, df.columns.get_loc(col)] = col\n",
    "    return df\n",
    "\n",
    "# 11. \n",
    "def split_values(df):\n",
    "    columna_a_expandir = df.columns[-3]\n",
    "    nuevas_columnas = df[columna_a_expandir].str.split(expand=True)\n",
    "    nuevas_columnas.columns = [f'{columna_a_expandir}_{i+1}' for i in range(nuevas_columnas.shape[1])]\n",
    "    posicion_insercion = len(df.columns) - 2\n",
    "    for col in reversed(nuevas_columnas.columns):\n",
    "        df.insert(posicion_insercion, col, nuevas_columnas[col])\n",
    "    df.drop(columns=[columna_a_expandir], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# 13.\n",
    "def roman_arabic(df):\n",
    "    primera_fila = df.iloc[0]\n",
    "    def convert_roman_number(numero):\n",
    "        try:\n",
    "            return str(roman.fromRoman(numero))\n",
    "        except roman.InvalidRomanNumeralError:\n",
    "            return numero\n",
    "\n",
    "    primera_fila_convertida = []\n",
    "    for valor in primera_fila:\n",
    "        if isinstance(valor, str) and not pd.isna(valor):\n",
    "            primera_fila_convertida.append(convert_roman_number(valor))\n",
    "        else:\n",
    "            primera_fila_convertida.append(valor)\n",
    "\n",
    "    df.iloc[0] = primera_fila_convertida\n",
    "    return df\n",
    "\n",
    "# 14.\n",
    "def fix_duplicates(df):\n",
    "    fila_segunda = df.iloc[0].copy()\n",
    "    prev_num = None\n",
    "    first_one_index = None\n",
    "\n",
    "    for i, num in enumerate(fila_segunda):\n",
    "        try:\n",
    "            num = int(num)\n",
    "            prev_num = int(prev_num) if prev_num is not None else None\n",
    "\n",
    "            if num == prev_num:\n",
    "                if num == 1:\n",
    "                    if first_one_index is None:\n",
    "                        first_one_index = i - 1\n",
    "                    next_num = int(fila_segunda[i - 1]) + 1\n",
    "                    for j in range(i, len(fila_segunda)):\n",
    "                        if fila_segunda.iloc[j].isdigit():\n",
    "                            fila_segunda.iloc[j] = str(next_num)\n",
    "                            next_num += 1\n",
    "                elif i - 1 >= 0:\n",
    "                    fila_segunda.iloc[i] = str(int(fila_segunda.iloc[i - 1]) + 1)\n",
    "\n",
    "            prev_num = num\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    df.iloc[0] = fila_segunda\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e03cbc",
   "metadata": {},
   "source": [
    "## More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba6224f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarters_sublist_list(df, year_columns):\n",
    "    first_row = df.iloc[0]\n",
    "    # Initialize the list of sublists\n",
    "    quarters_sublist_list = []\n",
    "\n",
    "    # Initialize the current sublist\n",
    "    quarters_sublist = []\n",
    "\n",
    "    # Iterate over the elements of the first row\n",
    "    for item in first_row:\n",
    "        # Check if the item meets the requirements\n",
    "        if len(str(item)) == 1:\n",
    "            quarters_sublist.append(item)\n",
    "        elif str(item) == 'year':\n",
    "            quarters_sublist.append(item)\n",
    "            quarters_sublist_list.append(quarters_sublist)\n",
    "            quarters_sublist = []\n",
    "\n",
    "    # Add the last sublist if it's not empty\n",
    "    if quarters_sublist:\n",
    "        quarters_sublist_list.append(quarters_sublist)\n",
    "\n",
    "    new_elements = []\n",
    "\n",
    "    # Check if year_columns is not empty\n",
    "    if year_columns:\n",
    "        for i, year in enumerate(year_columns):\n",
    "            # Check if index i is valid for quarters_sublist_list\n",
    "            if i < len(quarters_sublist_list):\n",
    "                for element in quarters_sublist_list[i]:\n",
    "                    new_elements.append(f\"{year}_{element}\")\n",
    "\n",
    "    two_first_elements = df.iloc[0][:2].tolist()\n",
    "\n",
    "    # Ensure that the two_first_elements are added if they are not in new_elements\n",
    "    for index in range(len(two_first_elements) - 1, -1, -1):\n",
    "        if two_first_elements[index] not in new_elements:\n",
    "            new_elements.insert(0, two_first_elements[index])\n",
    "\n",
    "    # Ensure that the length of new_elements matches the number of columns in df\n",
    "    while len(new_elements) < len(df.columns):\n",
    "        new_elements.append(None)\n",
    "\n",
    "    temp_df = pd.DataFrame([new_elements], columns=df.columns)\n",
    "    df.iloc[0] = temp_df.iloc[0]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bb4b7",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2016\\_20}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "440ab6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan_row(df):\n",
    "    if df.iloc[0].isnull().all():\n",
    "        df = df.drop(index=0)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c56742",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2019\\_17}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b32f58fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_column_es(df): # similar than relocate last columns\n",
    "    # Check if the DataFrame has at least two columns and the last column is a 4-digit year\n",
    "    if len(df.columns) >= 2 and df.columns[-1].isdigit() and len(df[df.columns[-1]].iloc[:2]) >= 2:\n",
    "        # Check if the first observation of the last column is 'ECONOMIC SECTORS'\n",
    "        if df[df.columns[-1]].iloc[0] == 'ECONOMIC SECTORS':\n",
    "            # Check if the second observation of the last column is not empty\n",
    "            if pd.notnull(df[df.columns[-1]].iloc[1]):\n",
    "                # Create a new column with NaN values\n",
    "                new_column_name = f\"col_{len(df.columns)}\"\n",
    "                df[new_column_name] = np.nan\n",
    "                \n",
    "                # Get 'ECONOMIC SECTORS' and relocate\n",
    "                insert_value_1 = df.iloc[0, -2]\n",
    "                # Convert the value to string before assignment\n",
    "                insert_value_1 = str(insert_value_1)\n",
    "                # Ensure the dtype of the last column is object (string) to accommodate string values\n",
    "                df.iloc[:, -1] = df.iloc[:, -1].astype('object')\n",
    "                df.iloc[0, -1] = insert_value_1\n",
    "\n",
    "                # NaN first obs\n",
    "                df.iloc[0,-2] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028154b1",
   "metadata": {},
   "source": [
    "$\\Large{\\color{blue}{ns\\_2019\\_26}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68af7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intercambiar_columnas(df):\n",
    "    # Buscar una columna con todos los valores NaN\n",
    "    columna_nan = None\n",
    "    for columna in df.columns:\n",
    "        if df[columna].isnull().all() and len(columna) == 4 and columna.isdigit():\n",
    "            columna_nan = columna\n",
    "            break\n",
    "    \n",
    "    if columna_nan:\n",
    "        # Revisar la columna de la izquierda\n",
    "        indice_columna = df.columns.get_loc(columna_nan)\n",
    "        if indice_columna > 0:\n",
    "            columna_izquierda = df.columns[indice_columna - 1]\n",
    "            # Verificar si no es un año (no tiene 4 dígitos)\n",
    "            if not (len(columna_izquierda) == 4 and columna_izquierda.isdigit()):\n",
    "                # Intercambiar nombres de columnas\n",
    "                df.rename(columns={columna_nan: columna_izquierda, columna_izquierda: columna_nan}, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88524d6",
   "metadata": {},
   "source": [
    "# Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ce026",
   "metadata": {},
   "source": [
    "# Con registro de carpetas procesdas 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ab844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import locale\n",
    "from tkinter import Tk, messagebox, TOP, YES, NO\n",
    "import os\n",
    "\n",
    "# Establecer la localización en español\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Palabras clave para buscar en el texto de la página\n",
    "keywords = [\"ECONOMIC SECTORS\"]\n",
    "\n",
    "# Diccionario para almacenar los DataFrames generados\n",
    "dataframes_dict_1 = {}\n",
    "\n",
    "# Ruta del archivo de registro de carpetas procesadas\n",
    "registro_path = 'dataframes_record/carpetas_procesadas_1.txt'\n",
    "\n",
    "# Función para corregir los nombres de los meses\n",
    "def corregir_nombre_mes(mes):\n",
    "    meses_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Agrega más mapeos si es necesario para otros nombres de meses\n",
    "    }\n",
    "    return meses_mapping.get(mes, mes)\n",
    "\n",
    "def registrar_carpeta_procesada(carpeta, num_archivos_procesados):\n",
    "    with open(registro_path, 'a') as file:\n",
    "        file.write(f\"{carpeta}:{num_archivos_procesados}\\n\")\n",
    "\n",
    "def carpeta_procesada(carpeta):\n",
    "    if not os.path.exists(registro_path):\n",
    "        return False\n",
    "    with open(registro_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(carpeta):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def procesar_pdf(pdf_path):\n",
    "    tables_dict_1 = {}  # Diccionario local para cada PDF\n",
    "    table_counter = 1\n",
    "    keyword_count = 0 \n",
    "\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No se encontraron coincidencias para id_ns y year en el nombre del archivo:\", filename)\n",
    "        return None, None, None, None, None  # Return None for tables_dict_1 as well\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(pdf_path))[0].replace('-', '_')\n",
    "    date = None\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            text = page.extract_text()\n",
    "            if i == 1:\n",
    "                match = re.search(r'(\\d{1,2}\\s+de\\s+\\w+\\s+de\\s+\\d{4})', text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    fecha_str = match.group(0)\n",
    "                    # Corregir el nombre del mes\n",
    "                    partes_fecha = fecha_str.split()\n",
    "                    partes_fecha[2] = corregir_nombre_mes(partes_fecha[2].lower())\n",
    "                    fecha_str_corregida = ' '.join(partes_fecha)\n",
    "                    date = datetime.strptime(fecha_str_corregida, '%d de %B de %Y')\n",
    "\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                keyword_count += 1\n",
    "                if keyword_count == 1:  # Solo procesar la primera ocurrencia\n",
    "                    tables = tabula.read_pdf(pdf_path, pages=i, multiple_tables=False, stream=True) # change stream to another option if desired\n",
    "                    for j, table_df in enumerate(tables, start=1):\n",
    "                        nombre_dataframe = f\"{new_filename}_{keyword_count}\"\n",
    "                        tables_dict_1[nombre_dataframe] = table_df\n",
    "                        table_counter += 1\n",
    "\n",
    "                    break  # Salir del bucle después de encontrar la primera ocurrencia\n",
    "\n",
    "    return id_ns, year, date, tables_dict_1, keyword_count  # Return tables_dict_1 here\n",
    "\n",
    "\n",
    "def procesar_carpeta(carpeta):\n",
    "    print(f\"Procesando la carpeta {os.path.basename(carpeta)}\")\n",
    "    pdf_files = [os.path.join(carpeta, f) for f in os.listdir(carpeta) if f.endswith('.pdf')]\n",
    "\n",
    "    num_pdfs_procesados = 0\n",
    "    num_dataframes_generados = 0\n",
    "\n",
    "    table_counter = 1  # Inicializar el contador de tabla aquí\n",
    "    tables_dict_1 = {}  # Declarar tables_dict_1 fuera del bucle principal\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        id_ns, year, date, tables_dict_temp, keyword_count = procesar_pdf(pdf_file)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for nombre_df, df in tables_dict_temp.items():\n",
    "                nombre_archivo = os.path.splitext(os.path.basename(pdf_file))[0].replace('-', '_')\n",
    "                nombre_df = f\"{nombre_archivo}_{keyword_count}\"\n",
    "                \n",
    "                # Almacenar DataFrame sin procesar en tables_dict_1\n",
    "                tables_dict_1[nombre_df] = df.copy()\n",
    "                \n",
    "                # Aplicar las 20 líneas de funciones de limpieza a una copia del DataFrame\n",
    "                df_clean = df.copy()\n",
    "\n",
    "                if any(col.isdigit() and len(col) == 4 for col in df_clean.columns):\n",
    "                    # Si hay al menos una columna que representa un año\n",
    "                    df_clean = swap_nan_se(df_clean)\n",
    "                    df_clean = split_column_by_pattern(df_clean)\n",
    "                    df_clean = drop_rare_caracter_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = relocate_last_columns(df_clean)\n",
    "                    df_clean = replace_first_dot(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = replace_var_perc_first_column(df_clean)\n",
    "                    df_clean = replace_var_perc_last_columns(df_clean)\n",
    "                    df_clean = replace_number_moving_average(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = intercambiar_valores(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = find_year_column(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = convertir_float(df_clean)\n",
    "                else: # 2014 ns 08\n",
    "                    # Si no hay columnas que representen años\n",
    "                    df_clean = revisar_primera_fila(df_clean)\n",
    "                    df_clean = revisar_primera_fila_1(df_clean)\n",
    "                    df_clean = replace_first_row_with_columns(df_clean)\n",
    "                    df_clean = swap_nan_se(df_clean)\n",
    "                    df_clean = split_column_by_pattern(df_clean)\n",
    "                    df_clean = drop_rare_caracter_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = relocate_last_columns(df_clean)\n",
    "                    #df_clean = replace_first_dot(df_clean) # comment for 2014 ns 08\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = replace_var_perc_first_column(df_clean)\n",
    "                    df_clean = replace_var_perc_last_columns(df_clean)\n",
    "                    df_clean = replace_number_moving_average(df_clean)\n",
    "                    df_clean = expand_column(df_clean) # 2014 ns 08\n",
    "                    df_clean = split_values_1(df_clean) # 2014 ns 08\n",
    "                    df_clean = split_values_2(df_clean) # 2016 ns 15\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = intercambiar_valores(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = find_year_column(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = convertir_float(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column(df_clean)\n",
    "                \n",
    "                # Añadir las columnas 'year', 'id_ns', 'date' al DataFrame limpio\n",
    "                df_clean.insert(0, 'date', date)\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                df_clean.insert(2, 'year', year)\n",
    "                \n",
    "                # Almacenar DataFrame limpio en dataframes_dict_1\n",
    "                dataframes_dict_1[nombre_df] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. El dataframe generado para el archivo {pdf_file} es: {nombre_df}')\n",
    "                num_dataframes_generados += 1\n",
    "                table_counter += 1  # Incrementar el contador de tabla aquí\n",
    "        \n",
    "        num_pdfs_procesados += 1  # Incrementar el número de PDFs procesados por cada PDF en la carpeta\n",
    "\n",
    "    return num_pdfs_procesados, num_dataframes_generados, tables_dict_1\n",
    "\n",
    "def procesar_carpetas():\n",
    "    pdf_folder = 'pseudo_raw_pdf'\n",
    "    carpetas = [os.path.join(pdf_folder, d) for d in os.listdir(pdf_folder) if os.path.isdir(os.path.join(pdf_folder, d))]\n",
    "    \n",
    "    tables_dict_1 = {}  # Inicializar tables_dict_1 aquí\n",
    "    \n",
    "    for carpeta in carpetas:\n",
    "        if carpeta_procesada(carpeta):\n",
    "            print(f\"La carpeta {carpeta} ya ha sido procesada.\")\n",
    "            continue\n",
    "        \n",
    "        num_pdfs_procesados, num_dataframes_generados, tables_dict_temp = procesar_carpeta(carpeta)\n",
    "        \n",
    "        # Actualizar tables_dict_1 con los valores devueltos de procesar_carpeta()\n",
    "        tables_dict_1.update(tables_dict_temp)\n",
    "        \n",
    "        registrar_carpeta_procesada(carpeta, num_pdfs_procesados)\n",
    "\n",
    "        # Preguntar al usuario si desea continuar con la siguiente carpeta\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Para asegurar que la ventana esté en primer plano\n",
    "        \n",
    "        mensaje = f\"Se han generado {num_dataframes_generados} dataframes en la carpeta {carpeta}. ¿Deseas continuar con la siguiente carpeta?\"\n",
    "        continuar = messagebox.askyesno(\"Continuar\", mensaje)\n",
    "        root.destroy()\n",
    "\n",
    "        if not continuar:\n",
    "            print(\"Procesamiento detenido por el usuario.\")\n",
    "            break  # Romper el bucle for si el usuario decide no continuar\n",
    "\n",
    "    print(\"Procesamiento completado para todas las carpetas.\")  # Add a message to indicate completion\n",
    "\n",
    "    return tables_dict_1  # Devolver tables_dict_1 al final de la función\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tables_dict_1 = procesar_carpetas()  # Capturar el valor devuelto de procesar_carpetas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6cc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_1['ns_08_2014_1'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ecc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict_1['ns_08_2014_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5108832",
   "metadata": {},
   "source": [
    "# Table 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28075a94",
   "metadata": {},
   "source": [
    "# Con registro de carpetas procesdas 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19908f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carpeta pseudo_raw_pdf\\2013 ya ha sido procesada.\n",
      "La carpeta pseudo_raw_pdf\\2014 ya ha sido procesada.\n",
      "La carpeta pseudo_raw_pdf\\2015 ya ha sido procesada.\n",
      "La carpeta pseudo_raw_pdf\\2016 ya ha sido procesada.\n",
      "La carpeta pseudo_raw_pdf\\2017 ya ha sido procesada.\n",
      "La carpeta pseudo_raw_pdf\\2018 ya ha sido procesada.\n",
      "Procesando la carpeta 2019\n",
      "  1. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-01-2019.pdf es: ns_01_2019_2\n",
      "  2. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-02-2019.pdf es: ns_02_2019_2\n",
      "  3. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-03-2019.pdf es: ns_03_2019_2\n",
      "  4. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-04-2019.pdf es: ns_04_2019_2\n",
      "  5. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-05-2019.pdf es: ns_05_2019_2\n",
      "  6. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-06-2019.pdf es: ns_06_2019_2\n",
      "  7. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-07-2019.pdf es: ns_07_2019_2\n",
      "  8. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-08-2019.pdf es: ns_08_2019_2\n",
      "  9. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-09-2019.pdf es: ns_09_2019_2\n",
      "  10. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-10-2019.pdf es: ns_10_2019_2\n",
      "  11. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-11-2019.pdf es: ns_11_2019_2\n",
      "  12. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-12-2019.pdf es: ns_12_2019_2\n",
      "  13. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-13-2019.pdf es: ns_13_2019_2\n",
      "  14. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-14-2019.pdf es: ns_14_2019_2\n",
      "  15. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-15-2019.pdf es: ns_15_2019_2\n",
      "  16. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-16-2019.pdf es: ns_16_2019_2\n",
      "  17. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-17-2019.pdf es: ns_17_2019_2\n",
      "  18. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-18-2019.pdf es: ns_18_2019_2\n",
      "  19. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-19-2019.pdf es: ns_19_2019_2\n",
      "  20. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-20-2019.pdf es: ns_20_2019_2\n",
      "  21. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-21-2019.pdf es: ns_21_2019_2\n",
      "  22. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-22-2019.pdf es: ns_22_2019_2\n",
      "  23. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-23-2019.pdf es: ns_23_2019_2\n",
      "  24. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-24-2019.pdf es: ns_24_2019_2\n",
      "  25. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-25-2019.pdf es: ns_25_2019_2\n",
      "  26. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-26-2019.pdf es: ns_26_2019_2\n",
      "  27. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-27-2019.pdf es: ns_27_2019_2\n",
      "  28. El dataframe generado para el archivo pseudo_raw_pdf\\2019\\ns-28-2019.pdf es: ns_28_2019_2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Setting with non-unique columns is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 208\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tables_dict_2  \u001b[38;5;66;03m# Devolver tables_dict al final de la función\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 208\u001b[0m     tables_dict_2 \u001b[38;5;241m=\u001b[39m \u001b[43mprocesar_carpetas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 183\u001b[0m, in \u001b[0;36mprocesar_carpetas\u001b[1;34m()\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLa carpeta \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcarpeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ya ha sido procesada.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m num_pdfs_procesados, num_dataframes_generados, tables_dict_temp \u001b[38;5;241m=\u001b[39m \u001b[43mprocesar_carpeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarpeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Actualizar tables_dict con los valores devueltos de procesar_carpeta()\u001b[39;00m\n\u001b[0;32m    186\u001b[0m tables_dict_2\u001b[38;5;241m.\u001b[39mupdate(tables_dict_temp)\n",
      "Cell \u001b[1;32mIn[24], line 151\u001b[0m, in \u001b[0;36mprocesar_carpeta\u001b[1;34m(carpeta)\u001b[0m\n\u001b[0;32m    149\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m get_quarters_sublist_list(df_clean, year_columns)\n\u001b[0;32m    150\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m first_row_columns(df_clean)\n\u001b[1;32m--> 151\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m \u001b[43mclean_columns_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_clean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m reset_index(df_clean)\n\u001b[0;32m    153\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m convertir_float(df_clean)\n",
      "Cell \u001b[1;32mIn[9], line 83\u001b[0m, in \u001b[0;36mclean_columns_values\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     81\u001b[0m text_columns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 83\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: remove_tildes(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x)\n\u001b[0;32m     84\u001b[0m     df\u001b[38;5;241m.\u001b[39mloc[:, col] \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m x)\n\u001b[0;32m     85\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msectores_economicos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msectores_economicos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32m~\\.conda\\envs\\gdp_revisions\\Lib\\site-packages\\pandas\\core\\indexing.py:885\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    884\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 885\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gdp_revisions\\Lib\\site-packages\\pandas\\core\\indexing.py:1893\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1892\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1893\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32m~\\.conda\\envs\\gdp_revisions\\Lib\\site-packages\\pandas\\core\\indexing.py:1928\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like_indexer(value) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ABCDataFrame):\n\u001b[1;32m-> 1928\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_frame_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1930\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1931\u001b[0m         \u001b[38;5;66;03m# TODO: avoid np.ndim call in case it isn't an ndarray, since\u001b[39;00m\n\u001b[0;32m   1932\u001b[0m         \u001b[38;5;66;03m#  that will construct an ndarray, which will be wasteful\u001b[39;00m\n\u001b[0;32m   1933\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_2d_value(indexer, value)\n",
      "File \u001b[1;32m~\\.conda\\envs\\gdp_revisions\\Lib\\site-packages\\pandas\\core\\indexing.py:2044\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_frame_value\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   2041\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_column(loc, val, pi)\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unique_cols:\n\u001b[1;32m-> 2044\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting with non-unique columns is not allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2046\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n",
      "\u001b[1;31mValueError\u001b[0m: Setting with non-unique columns is not allowed."
     ]
    }
   ],
   "source": [
    "import tabula\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import roman\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import locale\n",
    "from tkinter import Tk, messagebox, TOP, YES, NO\n",
    "import os\n",
    "\n",
    "\n",
    "# Establecer la localización en español\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Palabras clave para buscar en el texto de la página\n",
    "keywords = [\"ECONOMIC SECTORS\"]\n",
    "\n",
    "# Diccionario para almacenar los DataFrames generados\n",
    "dataframes_dict_2 = {}\n",
    "\n",
    "# Ruta del archivo de registro de carpetas procesadas\n",
    "registro_path = 'dataframes_record/carpetas_procesadas_2.txt'\n",
    "\n",
    "# Función para corregir los nombres de los meses\n",
    "def corregir_nombre_mes(mes):\n",
    "    meses_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Agrega más mapeos si es necesario para otros nombres de meses\n",
    "    }\n",
    "    return meses_mapping.get(mes, mes)\n",
    "\n",
    "def registrar_carpeta_procesada(carpeta, num_archivos_procesados):\n",
    "    with open(registro_path, 'a') as file:\n",
    "        file.write(f\"{carpeta}:{num_archivos_procesados}\\n\")\n",
    "\n",
    "def carpeta_procesada(carpeta):\n",
    "    if not os.path.exists(registro_path):\n",
    "        return False\n",
    "    with open(registro_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(carpeta):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def procesar_pdf(pdf_path):\n",
    "    tables_dict_2 = {}  # Diccionario local para cada PDF\n",
    "    table_counter = 1\n",
    "    keyword_count = 0 \n",
    "\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No se encontraron coincidencias para id_ns y year en el nombre del archivo:\", filename)\n",
    "        return None, None, None, None\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(pdf_path))[0].replace('-', '_')\n",
    "    date = None\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            text = page.extract_text()\n",
    "            if i == 1:\n",
    "                match = re.search(r'(\\d{1,2}\\s+de\\s+\\w+\\s+de\\s+\\d{4})', text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    fecha_str = match.group(0)\n",
    "                    # Corregir el nombre del mes\n",
    "                    partes_fecha = fecha_str.split()\n",
    "                    partes_fecha[2] = corregir_nombre_mes(partes_fecha[2].lower())\n",
    "                    fecha_str_corregida = ' '.join(partes_fecha)\n",
    "                    date = datetime.strptime(fecha_str_corregida, '%d de %B de %Y')\n",
    "\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                keyword_count += 1\n",
    "                if keyword_count == 2:\n",
    "                    tables = tabula.read_pdf(pdf_path, pages=i, multiple_tables=False)\n",
    "                    for j, table_df in enumerate(tables, start=1):\n",
    "                        nombre_dataframe = f\"{new_filename}_{keyword_count}\"\n",
    "                        tables_dict_2[nombre_dataframe] = table_df\n",
    "                        table_counter += 1\n",
    "\n",
    "    return id_ns, year, date, tables_dict_2, keyword_count\n",
    "\n",
    "\n",
    "def procesar_carpeta(carpeta):\n",
    "    print(f\"Procesando la carpeta {os.path.basename(carpeta)}\")\n",
    "    pdf_files = [os.path.join(carpeta, f) for f in os.listdir(carpeta) if f.endswith('.pdf')]\n",
    "\n",
    "    num_pdfs_procesados = 0\n",
    "    num_dataframes_generados = 0\n",
    "\n",
    "    table_counter = 1  # Inicializar el contador de tabla aquí\n",
    "    tables_dict_2 = {}  # Declarar tables_dict fuera del bucle principal\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        id_ns, year, date, tables_dict_temp, keyword_count = procesar_pdf(pdf_file)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for nombre_df, df in tables_dict_temp.items():\n",
    "                nombre_archivo = os.path.splitext(os.path.basename(pdf_file))[0].replace('-', '_')\n",
    "                nombre_df = f\"{nombre_archivo}_{keyword_count}\"\n",
    "\n",
    "                # Almacenar DataFrame sin procesar en tables_dict\n",
    "                tables_dict_2[nombre_df] = df.copy()\n",
    "\n",
    "                # Aplicar las 20 líneas de funciones de limpieza a una copia del DataFrame\n",
    "                df_clean = df.copy()\n",
    "                if df_clean.iloc[0, 0] is np.nan:\n",
    "                    # Aplicar las 20 líneas de limpieza\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = separate_years(df_clean)\n",
    "                    df_clean = relocate_roman_numerals(df_clean)\n",
    "                    df_clean = extract_mixed_values(df_clean)\n",
    "                    df_clean = replace_first_row_nan(df_clean)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = drop_nan_row(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = split_values(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = roman_arabic(df_clean)\n",
    "                    df_clean = fix_duplicates(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = convertir_float(df_clean)\n",
    "                else:\n",
    "                    # Aplicar las 15 líneas de limpieza\n",
    "                    df_clean = intercambiar_columnas(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = last_column_es(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = roman_arabic(df_clean)\n",
    "                    df_clean = fix_duplicates(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = convertir_float(df_clean)\n",
    "\n",
    "                # Añadir las columnas 'year', 'id_ns', 'date' al DataFrame limpio\n",
    "                df_clean.insert(0, 'date', date)\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                df_clean.insert(2, 'year', year)\n",
    "\n",
    "                # Almacenar DataFrame limpio en dataframes_dict\n",
    "                dataframes_dict_2[nombre_df] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. El dataframe generado para el archivo {pdf_file} es: {nombre_df}')\n",
    "                num_dataframes_generados += 1\n",
    "                table_counter += 1  # Incrementar el contador de tabla aquí\n",
    "                    \n",
    "        num_pdfs_procesados += 1  # Incrementar el número de PDFs procesados por cada PDF en la carpeta\n",
    "\n",
    "    return num_pdfs_procesados, num_dataframes_generados, tables_dict_2\n",
    "\n",
    "\n",
    "def procesar_carpetas():\n",
    "    pdf_folder = 'pseudo_raw_pdf'\n",
    "    carpetas = [os.path.join(pdf_folder, d) for d in os.listdir(pdf_folder) if os.path.isdir(os.path.join(pdf_folder, d))]\n",
    "\n",
    "    tables_dict_2 = {}  # Inicializar tables_dict aquí\n",
    "    \n",
    "    for carpeta in carpetas:\n",
    "        if carpeta_procesada(carpeta):\n",
    "            print(f\"La carpeta {carpeta} ya ha sido procesada.\")\n",
    "            continue\n",
    "        \n",
    "        num_pdfs_procesados, num_dataframes_generados, tables_dict_temp = procesar_carpeta(carpeta)\n",
    "        \n",
    "        # Actualizar tables_dict con los valores devueltos de procesar_carpeta()\n",
    "        tables_dict_2.update(tables_dict_temp)\n",
    "        \n",
    "        registrar_carpeta_procesada(carpeta, num_pdfs_procesados)\n",
    "\n",
    "        # Preguntar al usuario si desea continuar con la siguiente carpeta\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Para asegurar que la ventana esté en primer plano\n",
    "        \n",
    "        mensaje = f\"Se han generado {num_dataframes_generados} dataframes en la carpeta {carpeta}. ¿Deseas continuar con la siguiente carpeta?\"\n",
    "        continuar = messagebox.askyesno(\"Continuar\", mensaje)\n",
    "        root.destroy()\n",
    "\n",
    "        if not continuar:\n",
    "            print(\"Procesamiento detenido por el usuario.\")\n",
    "            break  # Romper el bucle for si el usuario decide no continuar\n",
    "\n",
    "    print(\"Procesamiento completado para todas las carpetas.\")  # Add a message to indicate completion\n",
    "\n",
    "    return tables_dict_2  # Devolver tables_dict al final de la función\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tables_dict_2 = procesar_carpetas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d96bcf0",
   "metadata": {},
   "source": [
    "PENDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212ced2",
   "metadata": {},
   "source": [
    "* 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a46bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_2['ns_08_2014_2'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict_2['ns_08_2014_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d654ba",
   "metadata": {},
   "source": [
    "# Se recomienda procesar por carpetas. Asegúrese de que cuando una carpeta ya ha sido procesada con todos los dataframes generados correctamente, se corra el siguiente código que concatena por sectores las revisiones de todas las NS de un mismo año (para todas las frecuencias). Los datasets de growth rates deben ser cargados a SQL por años. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1939c",
   "metadata": {},
   "source": [
    "**Nota sobre $\\color{blue}{NS-08-2017}$: La NS-08-2017 no tiene la tabla 1, pero resulta que la tabla 1 no cambia desde la NS 07 a la NS 09 del mismo año. Además la tabla 2 sí existe en la NS 08 y es la misma que en la NS 09. Por ello se puede reconstruir la NS (con las 3 pa´ginas de interés) a partir de la NS 09, se duplicarían las dos últimas páginas de la NS 09 y estas reemplazarían las dos últimas de la NS 08 actual, la primera no porque la primera es la portada y nos da información sobre la fecha en que ocurrió la NS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e0f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709d3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "667d97b8",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217183e7",
   "metadata": {},
   "source": [
    "<div id=\"3\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8989d65",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.</span> <span style = \"color: dark; font-family: charter;\">SQL Tables</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a04abd",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Finally, after obtaining and cleaning all the necessary data, we can create the three most important datasets to store realeses, vintages, and revisions. These datasets will be stored as tables in SQL and can be loaded into any software or programming language.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6ca98",
   "metadata": {},
   "source": [
    "<div id=\"3-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b142e",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.1.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Annual Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_annual_df(dataframes_dict):\n",
    "    # List to store the names of dataframes that meet the criterion of ending in '_2'\n",
    "    dataframes_ending_with_2 = []\n",
    "\n",
    "    # List to store the names of dataframes to be concatenated\n",
    "    dataframes_to_concatenate = []\n",
    "\n",
    "    # Iterate over the dataframe names in the all_dataframes dictionary\n",
    "    for df_name in dataframes_dict.keys():\n",
    "        # Check if the dataframe name ends with '_2' and add it to the corresponding list\n",
    "        if df_name.endswith('_2'):\n",
    "            dataframes_ending_with_2.append(df_name)\n",
    "            dataframes_to_concatenate.append(dataframes_dict[df_name])\n",
    "\n",
    "    # Print the names of dataframes that meet the criterion of ending in '_2'\n",
    "    print(\"DataFrames ending with '_2' that will be concatenated:\")\n",
    "    for df_name in dataframes_ending_with_2:\n",
    "        print(df_name)\n",
    "\n",
    "    # Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "    if dataframes_to_concatenate:\n",
    "        # Concatenate only rows that meet the specified conditions\n",
    "        gdp_annual_growth_rates = pd.concat([df[(df['sectores_economicos'] == 'pbi') | (df['economic_sectors'] == 'gdp')] \n",
    "                                    for df in dataframes_to_concatenate \n",
    "                                    if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Keep only columns that start with 'year' and the 'id_ns', 'year', and 'date' columns\n",
    "        columns_to_keep = ['id_ns', 'year', 'date'] + [col for col in gdp_annual_growth_rates.columns if col.endswith('_year')]\n",
    "\n",
    "        # Drop unwanted columns\n",
    "        gdp_annual_growth_rates = gdp_annual_growth_rates[columns_to_keep]\n",
    "        \n",
    "        # Remove duplicate columns if any\n",
    "        gdp_annual_growth_rates = gdp_annual_growth_rates.loc[:,~gdp_annual_growth_rates.columns.duplicated()]\n",
    "\n",
    "        # Print the number of rows in the concatenated dataframe\n",
    "        print(\"Number of rows in the concatenated dataframe:\", len(gdp_annual_growth_rates))\n",
    "        \n",
    "        return gdp_annual_growth_rates\n",
    "    else:\n",
    "        print(\"No dataframes were found to concatenate.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_annual_growth_rates = concatenate_annual_df(dataframes_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_annual_growth_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f8d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61bbaac2",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d9bac",
   "metadata": {},
   "source": [
    "<div id=\"3-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50087e95",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.2.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Quarterly Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6467ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concatenate_quarterly_df(dataframes_dict):\n",
    "    # List to store the names of dataframes that meet the criterion of ending in '_2'\n",
    "    dataframes_ending_with_2 = []\n",
    "\n",
    "    # List to store the names of dataframes to be concatenated\n",
    "    dataframes_to_concatenate = []\n",
    "\n",
    "    # Iterate over the dataframe names in the all_dataframes dictionary\n",
    "    for df_name in dataframes_dict.keys():\n",
    "        # Check if the dataframe name ends with '_2' and add it to the corresponding list\n",
    "        if df_name.endswith('_2'):\n",
    "            dataframes_ending_with_2.append(df_name)\n",
    "            dataframes_to_concatenate.append(dataframes_dict[df_name])\n",
    "\n",
    "    # Print the names of dataframes that meet the criterion of ending in '_2'\n",
    "    print(\"DataFrames ending with '_2' that will be concatenated:\")\n",
    "    for df_name in dataframes_ending_with_2:\n",
    "        print(df_name)\n",
    "\n",
    "    # Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "    if dataframes_to_concatenate:\n",
    "        # Concatenate only rows that meet the specified conditions\n",
    "        gdp_quarterly_growth_rates = pd.concat([df[(df['sectores_economicos'] == 'pbi') | (df['economic_sectors'] == 'gdp')] \n",
    "                                    for df in dataframes_to_concatenate \n",
    "                                    if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Keep all columns except those starting with 'year_', in addition to the 'id_ns', 'year', and 'date' columns\n",
    "        columns_to_keep = ['year', 'id_ns', 'date'] + [col for col in gdp_quarterly_growth_rates.columns if not col.endswith('_year')]\n",
    "\n",
    "        # Select unwanted columns\n",
    "        gdp_quarterly_growth_rates = gdp_quarterly_growth_rates[columns_to_keep]\n",
    "\n",
    "        # Drop the 'sectores_economicos' and 'economic_sectors' columns\n",
    "        gdp_quarterly_growth_rates.drop(columns=['sectores_economicos', 'economic_sectors'], inplace=True)\n",
    "\n",
    "        # Remove duplicate columns if any\n",
    "        gdp_quarterly_growth_rates = gdp_quarterly_growth_rates.loc[:,~gdp_quarterly_growth_rates.columns.duplicated()]\n",
    "\n",
    "        # Print the number of rows in the concatenated dataframe\n",
    "        print(\"Number of rows in the concatenated dataframe:\", len(gdp_quarterly_growth_rates))\n",
    "        \n",
    "        return gdp_quarterly_growth_rates\n",
    "    else:\n",
    "        print(\"No dataframes were found to concatenate.\")\n",
    "        return None\n",
    "\n",
    "# Uso de la función con el diccionario como argumento\n",
    "# gdp_quarterly_growth_rates = concatenate_and_filter_dataframes(dataframes_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b797aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_quarterly_growth_rates = concatenate_quarterly_df(dataframes_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_quarterly_growth_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d99fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24dd0fc0",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca271f",
   "metadata": {},
   "source": [
    "<div id=\"3-3\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453bb965",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.3.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Monthly Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a86c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concatenate_monthly_df(dataframes_dict):\n",
    "    # List to store the names of dataframes that meet the criterion of ending in '_1'\n",
    "    dataframes_ending_with_1 = []\n",
    "\n",
    "    # List to store the names of dataframes to be concatenated\n",
    "    dataframes_to_concatenate = []\n",
    "\n",
    "    # Iterate over the dataframe names in the all_dataframes dictionary\n",
    "    for df_name in dataframes_dict.keys():\n",
    "        # Check if the dataframe name ends with '_1' and add it to the corresponding list\n",
    "        if df_name.endswith('_1'):\n",
    "            dataframes_ending_with_1.append(df_name)\n",
    "            dataframes_to_concatenate.append(dataframes_dict[df_name])\n",
    "\n",
    "    # Print the names of dataframes that meet the criterion of ending with '_1'\n",
    "    print(\"DataFrames ending with '_1' that will be concatenated:\")\n",
    "    for df_name in dataframes_ending_with_1:\n",
    "        print(df_name)\n",
    "\n",
    "    # Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "    if dataframes_to_concatenate:\n",
    "        # Concatenate only rows that meet the specified conditions\n",
    "        gdp_monthly_growth_rates = pd.concat([df[(df['sectores_economicos'] == 'pbi') | (df['economic_sectors'] == 'gdp')] \n",
    "                                    for df in dataframes_to_concatenate \n",
    "                                    if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Keep all columns except those starting with 'year_', in addition to the 'id_ns', 'year', and 'date' columns\n",
    "        columns_to_keep = ['year', 'id_ns', 'date'] + [col for col in gdp_monthly_growth_rates.columns if not col.endswith('_year')]\n",
    "\n",
    "        # Select unwanted columns\n",
    "        gdp_monthly_growth_rates = gdp_monthly_growth_rates[columns_to_keep]\n",
    "\n",
    "        # Drop the 'sectores_economicos' and 'economic_sectors' columns\n",
    "        gdp_monthly_growth_rates.drop(columns=['sectores_economicos', 'economic_sectors'], inplace=True)\n",
    "\n",
    "        # Remove duplicate columns if any\n",
    "        gdp_monthly_growth_rates = gdp_monthly_growth_rates.loc[:,~gdp_monthly_growth_rates.columns.duplicated()]\n",
    "        \n",
    "        # Drop columns with at least two underscores in their names\n",
    "        columns_to_drop = [col for col in gdp_monthly_growth_rates.columns if col.count('_') >= 2]\n",
    "        gdp_monthly_growth_rates.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "        # Print the number of rows in the concatenated dataframe\n",
    "        print(\"Number of rows in the concatenated dataframe:\", len(gdp_monthly_growth_rates))\n",
    "        \n",
    "        return gdp_monthly_growth_rates\n",
    "    else:\n",
    "        print(\"No dataframes were found to concatenate.\")\n",
    "        return None\n",
    "\n",
    "# Uso de la función con el diccionario como argumento\n",
    "# gdp_monthly_growth_rates = concatenate_and_filter_dataframes(dataframes_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671223c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_monthly_growth_rates = concatenate_monthly_df(dataframes_dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e805081",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_monthly_growth_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que todas las fechas estén en el mismo formato\n",
    "#gdp_monthly_growth_rates['date'] = pd.to_datetime(gdp_monthly_growth_rates['date'])\n",
    "\n",
    "# Ahora, convierte la columna 'date' a solo fecha\n",
    "#gdp_monthly_growth_rates['date'] = pd.to_datetime(gdp_monthly_growth_rates['date'].dt.date)\n",
    "\n",
    "#print(gdp_monthly_growth_rates['date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdp_monthly_growth_rates['date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec57e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79df4c62",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda85bba",
   "metadata": {},
   "source": [
    "<div id=\"3-4\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d3554",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.4.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Loading SQL\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Get environment variables\n",
    "user = os.environ.get('CIUP_SQL_USER')\n",
    "password = os.environ.get('CIUP_SQL_PASS')\n",
    "host = os.environ.get('CIUP_SQL_HOST')\n",
    "port = 5432\n",
    "database = 'gdp_revisions_datasets'\n",
    "\n",
    "# Check if all environment variables are defined\n",
    "if not all([host, user, password]):\n",
    "    raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "# Create connection string\n",
    "connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# gdp_monthly_growth_rates is the DataFrame you want to save to the database\n",
    "#gdp_annual_growth_rates.to_sql('gdp_annual_growth_rates_2013', engine, index=False, if_exists='replace')\n",
    "#gdp_quarterly_growth_rates.to_sql('gdp_quarterly_growth_rates', engine, index=False, if_exists='replace')\n",
    "gdp_monthly_growth_rates.to_sql('gdp_monthly_growth_rates_2014', engine, index=False, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132cb43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c651835",
   "metadata": {},
   "source": [
    "### PENDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b4c405",
   "metadata": {},
   "source": [
    "1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fda3a8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: red; color: white;\">\n",
    "    Incluir variables globales para que tan solo cambiar el nombre del sector, el código de concatenacipon funcione. Lo mismo para el año al final de \"gdp_annual_growth_rates_\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd18b4",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc920fd",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eae11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf105ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdp_revisions",
   "language": "python",
   "name": "gdp_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
