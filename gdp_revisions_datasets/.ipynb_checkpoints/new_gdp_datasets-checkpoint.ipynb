{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd9a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a055b3b9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter bt pro roman'; color: rgb(0, 65, 75);\">\n",
    "<h1>\n",
    "New GDP Revisions Datasets\n",
    "</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd88a4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter bt pro roman'; color: rgb(0, 65, 75);\">\n",
    "<h3>\n",
    "Documentation\n",
    "<br>\n",
    "____________________\n",
    "<br>\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d22837",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'PT Serif Pro Book'; color: rgb(0, 65, 75); font-size: 16px;\">\n",
    "    Jason Cruz\n",
    "    <br>\n",
    "    <a href=\"mailto:jj.cruza@up.edu.pe\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">\n",
    "        jj.cruza@up.edu.pe\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72f519",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "This <span style=\"color: rgb(0, 65, 75);\">jupyter notebook</span> documents step-by-step the <b>construction of datasets</b> for the project <b>'Revisions and Biases in Preliminary GDP Estimates in Peru'</b>.\n",
    "\n",
    "This jupyter notebook starts with downloading the Weekly Reports (WR) of the Central Reserve Bank of Peru (BCRP), stored on its <a href=\"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\" style=\"color: rgb(0, 153, 123)\">website</a> as PDF files, in section 1. We clean the raw PDFs to keep only the pages with the key information in section 2. We extract and clean each table with Peru GDP growth rates from each PDF in section 3, in the same section we generate the dictionaries with the dataframes of Peru GDP growth from 2013-2024. In section 4 we create the vintages of Peru's GDP growth rates. Then, we create the final revisions dataset of Peru's GDP in section 5. Finally, we upload all the datasets generated in this jupyter notebook to SQL in section 6.\n",
    "\n",
    "Please note that, the WR contain the information of annual, quarterly and monthly GDP growth rates by economic sectors of Peru, while the main datasets that will be used for the data analysis of this project are generated in this jupyter notebook using big data and machine learning techniques.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca165838",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;line-height: 1.5;\">\n",
    "<span style=\"font-size: 24px;\">&#128196;</span> The Weekly Report/<i>Nota Semanal</i> (WR/<i>NS</i>) of the Central Bank.\n",
    "    <br>\n",
    "    <span style=\"font-size: 24px;\">&#8987;</span> Available since <b>2013-</b>. \n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28540df2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Amaya; text-align: left; color: rgb(0, 65, 75); font-size:16px\">The following <b>outline is functional</b>. By utilising the provided buttons, users are able to enhance their experience by browsing this script.<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc04c9",
   "metadata": {},
   "source": [
    "<div id=\"outilne\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0fa13",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #292929; padding: 10px; line-height: 1.5; font-family: 'PT Serif Pro Book';\">\n",
    "    <h2 style=\"text-align: left; color: #E0E0E0;\">\n",
    "        Outline\n",
    "    </h2>\n",
    "    <br>\n",
    "    <a href=\"#libraries\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        Libraries</a>\n",
    "    <br>\n",
    "    <a href=\"#setup\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        Initial set-up</a>\n",
    "    <br>\n",
    "    <a href=\"#1\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        1. PDF Downloader</a>\n",
    "    <br>\n",
    "    <a href=\"#2\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        2. Generate PDF input with key tables</a>\n",
    "    <br>\n",
    "    <a href=\"#3\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">\n",
    "        3. Data cleaning</a>\n",
    "    <br>\n",
    "    <a href=\"#3-1\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        3.1. A brief documentation on issues in the table information of the PDFs.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-2\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        3.2. Extracting tables and data cleanup.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-2-1\" style=\"color: #94FFD8; font-size: 14px; margin-left: 40px;\">\n",
    "        3.2.1. Table 1. Extraction and cleaning of data from tables on monthly real GDP growth rates.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-2-2\" style=\"color: #94FFD8; font-size: 14px; margin-left: 40px;\">\n",
    "        3.2.2. Table 2. Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.</a>\n",
    "    <br>\n",
    "    <a href=\"#4\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">4. Real-time data of Peru's GDP growth rates</a>\n",
    "    <br>\n",
    "    <a href=\"#4-1\" style=\"color: #94FFD8; font-size: 16px; margin-left: 20px;\">\n",
    "        4.1. Growth rates datasets concatenation for all frequencies.</a>\n",
    "    <br>\n",
    "    <a href=\"#5\" style=\"color: #E0E0E0; font-size: 18px; margin-left: 0px;\">5. Uploading data to SQL</a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90323106",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    Any questions or issues regarding the coding, please email Jason Cruz <a href=\"mailto:jj.cruza@alum.up.edu.pe\" style=\"color: rgb(0, 153, 123); text-decoration: none;\"><span style=\"font-size: 24px;\">&#x2709;</span>\n",
    "    </a>.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac40ed0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    If you don't have the libraries below, please use the following code (as example) to install the required libraries.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c73193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87292fcf",
   "metadata": {},
   "source": [
    "<div id=\"libraries\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907046a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark;\">\n",
    "    <h2>\n",
    "    Libraries\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214f5982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# 1. PDF downloader\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os  # For file and directory manipulation, for interacting with the operating system\n",
    "import random  # To generate random numbers\n",
    "from selenium import webdriver  # For automating web browsers\n",
    "from selenium.webdriver.common.by import By  # To locate elements on a webpage\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # To wait until certain conditions are met on a webpage.\n",
    "from selenium.webdriver.support import expected_conditions as EC  # To define expected conditions\n",
    "from selenium.common.exceptions import StaleElementReferenceException  # To handle exceptions related to elements on the webpage that are no longer available.\n",
    "import pygame # Allows you to handle graphics, sounds and input events.\n",
    "from webdriver_manager.chrome import ChromeDriverManager # To avoid compatibility issues with the ChromeDrive version of ChromeDrive\n",
    "\n",
    "import shutil # Used for high-level file operations, such as copying, moving, renaming, and deleting files and directories.\n",
    "\n",
    "\n",
    "# 2. Generate PDF input with key tables\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import fitz  # This library is used for working with PDF documents, including reading, writing, and modifying PDFs (PyMuPDF).\n",
    "import tkinter as tk  # This library is used for creating graphical user interfaces (GUIs) in Python.\n",
    "\n",
    "\n",
    "# 3. Data cleaning\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 3.1. A brief documentation on issus in the table information of the PDFs\n",
    "\n",
    "from PIL import Image  # Used for opening, manipulating, and saving image files.\n",
    "import matplotlib.pyplot as plt  # Used for creating static, animated, and interactive visualizations.\n",
    "\n",
    "# 3.2. Extracting tables and data cleanup\n",
    "\n",
    "import pdfplumber  # For extracting text and metadata from PDF files\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import unicodedata  # For manipulating Unicode data\n",
    "import re  # For regular expressions operations\n",
    "from datetime import datetime  # For working with dates and times\n",
    "import locale  # For locale-specific formatting of numbers, dates, and currencies\n",
    "\n",
    "# 3.2.1. Table 1. Extraction and cleaning of data from tables on monthly real GDP growth rates.\n",
    "\n",
    "import tabula  # Used to extract tables from PDF files into pandas DataFrames\n",
    "from tkinter import Tk, messagebox, TOP, YES, NO  # Used for creating graphical user interfaces\n",
    "from sqlalchemy import create_engine  # Used for connecting to and interacting with SQL databases\n",
    "\n",
    "# 3.2.2. Table 2. Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.\n",
    "\n",
    "import roman\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# 4. Real-time data of Peru's GDP growth rates\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import psycopg2  # For interacting with PostgreSQL databases\n",
    "from sqlalchemy import create_engine, text  # For creating and executing SQL queries using SQLAlchemy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e776e8d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db739b",
   "metadata": {},
   "source": [
    "<div id=\"setup\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ef8f8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark;\">\n",
    "    <h2>\n",
    "    Initial set-up\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc1c80",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\"> The following code lines will create folders in your current path, call them to import and export your outputs. <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8899a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save downloaded PDF files\n",
    "\n",
    "raw_pdf = 'raw_pdf' # to save raw data (.pdf).\n",
    "if not os.path.exists(raw_pdf):\n",
    "    os.mkdir(raw_pdf) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26b4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save text file with the names of already downloaded files\n",
    "\n",
    "download_record = 'download_record'\n",
    "if not os.path.exists(download_record):\n",
    "    os.mkdir(download_record) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147227ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to download the trimmed PDF files (these are PDF inputs for the extraction and cleanup code)\n",
    "\n",
    "input_pdf = 'input_pdf'\n",
    "if not os.path.exists(input_pdf):\n",
    "    os.makedirs(input_pdf) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6dc316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save PDF files containing only the pages of interest (where the GDP growth rate tables are located)\n",
    "\n",
    "input_pdf_record = 'input_pdf_record'\n",
    "if not os.path.exists(input_pdf_record):\n",
    "    os.makedirs(input_pdf_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d69eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save dataframes generated record by year\n",
    "\n",
    "dataframes_record = 'dataframes_record'\n",
    "if not os.path.exists(dataframes_record):\n",
    "    os.makedirs(dataframes_record) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21e9ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save sound files\n",
    "\n",
    "sound_folder = 'sound'\n",
    "if not os.path.exists(sound_folder):\n",
    "    os.makedirs(sound_folder) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb4b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save screenshots about issues in the table information\n",
    "\n",
    "ns_issues_folder = 'ns_issues_folder'\n",
    "if not os.path.exists(ns_issues_folder):\n",
    "    os.makedirs(ns_issues_folder) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdd5dc",
   "metadata": {},
   "source": [
    "<p style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\"> The following function will establish a connection to the <code>gdp_revisions_datasets</code> database in <code>PostgreSQL</code>. The <b>input data</b> used in this jupyter notebook will be loaded from this <code>PostgreSQL</code> database, and similarly, all <b>output data</b> generated by this jupyter notebook will be stored in that database. Ensure that you set the necessary parameters to access the server once you have obtained the required permissions.<p/>\n",
    "    \n",
    "<p style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "To request permissions, please email Jason Cruz <a href=\"mailto:jj.cruza@alum.up.edu.pe\" style=\"color: rgb(0, 153, 123); text-decoration: none;\"> <span style=\"font-size: 24px;\">&#x2709;</span>\n",
    "    </a>.\n",
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e982f83",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    <span style=\"font-size: 24px; color: #FFA823; font-weight: bold;\">&#9888;</span>\n",
    "    Enter your user credentials to acces to SQL.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b60634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sqlalchemy_engine():\n",
    "    \"\"\"\n",
    "    Function to create an SQLAlchemy engine using environment variables.\n",
    "    \n",
    "    Returns:\n",
    "        engine: SQLAlchemy engine object.\n",
    "    \"\"\"\n",
    "    # Get environment variables\n",
    "    user = os.environ.get('CIUP_SQL_USER')  # Get the SQL user from environment variables\n",
    "    password = os.environ.get('CIUP_SQL_PASS')  # Get the SQL password from environment variables\n",
    "    host = os.environ.get('CIUP_SQL_HOST')  # Get the SQL host from environment variables\n",
    "    port = 5432  # Set the SQL port to 5432\n",
    "    database = 'gdp_revisions_datasets'  # Set the database name 'gdp_revisions_datasets' from SQL\n",
    "\n",
    "    # Check if all environment variables are defined\n",
    "    if not all([host, user, password]):\n",
    "        raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "    # Create connection string\n",
    "    connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    return engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df5bdf",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"font-size: 24px; color: rgb(255, 32, 78); font-weight: bold;\">&#9888;</span>\n",
    "    <span style=\"font-family: PT Serif Pro Book; color: black; font-size: 16px;\">\n",
    "        Import all other functions required by this jupyter notebook.\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a836f",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\"> Please, check the script <code>new_gdp_datasets_functions.py</code> which contains all the functions required by this jupyter notebook. The functions there are ordered according to the <a href=\"#outilne\" style=\"color: #3d30a2;\">sections</a> of this jupyter notebok.<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83d1e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_gdp_datasets_functions import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0038d2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e051c",
   "metadata": {},
   "source": [
    "<div id=\"1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f3192",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: 'PT Serif Pro Book'; color: dark;\">1.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">PDF Downloader</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98250a64",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Our main source for data collection is the <a href=\"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\" style=\"color: rgb(0, 153, 123)\">BCRP's web page</a>. The weekly report is a periodic (weekly) publication of the BCRP in compliance with article 84 of the Peruvian Constitution and articles 2 and 74 of the BCRP's organic law, which include, among its functions, the periodic publication of the main national macroeconomic statistics.\n",
    "    \n",
    "Our project requires the publication of two tables: the table of monthly growth rates of real GDP (12-month percentage changes), and the table of quarterly (annual) growth rates of real GDP. These tables are referred to as <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 1</a> and <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 2</a>, respectively, throughout this jupyter notebook.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcc4e3",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    The following bot runs the following steps:\n",
    "    <ol>\n",
    "        <li>Download the PDF <span style=\"font-size: 24px;\">&#128462;</span> files (WR) from the BCRP web page, starting with the oldest and continuing to the most recent.</li>\n",
    "        <li>Notify you with a fabulous song each time a certain number of downloads is reached.</li>\n",
    "        <li>Display a window asking if you want to continue with the downloads. You can stop them at any time.</li>\n",
    "        <li>Report in detail about the downloaded files. If a file has already been downloaded, you will also be notified.</li>\n",
    "        <li>Save the raw PDFs <span style=\"font-size: 24px;\">&#128462;</span> to the paths set in the preamble of this Jupyter Notebook.</li>\n",
    "    </ol>\n",
    "    Try the bot, it's an adventure!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e812cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the BCRP URL\n",
    "bcrp_url = \"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\"  # Never replace this URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d95c04",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #F5F5F5; padding: 10px; line-height: 1.5; font-family: 'PT Serif Pro Book';\">\n",
    "    <h2 style=\"text-align: left; color: #292929;\">\n",
    "        Download bot\n",
    "    </h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6be0c",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    Execute the code that allows the bot to perform the download tasks (steps detailed above).\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71ff21",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"font-size: 24px; color: rgb(255, 32, 78); font-weight: bold;\">&#9888;</span>\n",
    "    <span style=\"font-family: PT Serif Pro Book; color: black; font-size: 16px;\">\n",
    "        If the code has any errors, please re-run it. At some point the BCRP web page from chrome may ask you to solve a captcha, please do so and re-execute the code. \n",
    "    </span>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0199bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize pygame\n",
    "pygame.mixer.init()\n",
    "\n",
    "# List of available sound files\n",
    "available_sounds = os.listdir(sound_folder)\n",
    "\n",
    "# Select a random sound\n",
    "random_sound = random.choice(available_sounds)\n",
    "\n",
    "# Full path of the random sound\n",
    "sound_path = os.path.join(sound_folder, random_sound)\n",
    "\n",
    "# Load the selected sound\n",
    "pygame.mixer.music.load(sound_path)\n",
    "\n",
    "# List to keep track of successfully downloaded files\n",
    "downloaded_files = []\n",
    "\n",
    "# Load the list of previously downloaded files if it exists\n",
    "if os.path.exists(os.path.join(download_record, \"downloaded_files.txt\")):\n",
    "    with open(os.path.join(download_record, \"downloaded_files.txt\"), \"r\") as f:\n",
    "        downloaded_files = f.read().splitlines()\n",
    "\n",
    "# Web driver setup\n",
    "\n",
    "'''\n",
    "Nota: Download chrome.exe from 'https://googlechromelabs.github.io/chrome-for-testing/#stable'\n",
    "and call in (1) the folder where you saved this application.\n",
    "'''\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())  # This automatically handles the correct ChromeDriver version\n",
    "\n",
    "# Number of downloads per batch\n",
    "downloads_per_batch = 5\n",
    "# Total number of downloads\n",
    "total_downloads = 5\n",
    "\n",
    "try:\n",
    "    # Open the test page\n",
    "    driver.get(bcrp_url)\n",
    "    print(\"Site opened successfully\")\n",
    "\n",
    "    # Wait for the container area to be present\n",
    "    wait = WebDriverWait(driver, 60)\n",
    "    container_area = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"rightside\"]')))\n",
    "\n",
    "    # Get all the links within the container area\n",
    "    pdf_links = container_area.find_elements(By.XPATH, './/a')\n",
    "\n",
    "    # Reverse the order of links\n",
    "    pdf_links = list(reversed(pdf_links))\n",
    "\n",
    "    # Initialize download counter\n",
    "    download_counter = 0\n",
    "\n",
    "    # Iterate over reversed links and download PDFs in batches\n",
    "    for pdf_link in pdf_links:\n",
    "        download_counter += 1\n",
    "\n",
    "        # Get the file name from the URL\n",
    "        new_url = pdf_link.get_attribute(\"href\")\n",
    "        file_name = new_url.split(\"/\")[-1]\n",
    "\n",
    "        # Check if the file has already been downloaded\n",
    "        if file_name in downloaded_files:\n",
    "            print(f\"{download_counter}. The file {file_name} has already been downloaded previously. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Try to download the file\n",
    "        try:\n",
    "            download_pdf(driver, pdf_link, wait, download_counter, raw_pdf, download_record)\n",
    "\n",
    "            # Update the list of downloaded files\n",
    "            downloaded_files.append(file_name)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading the file {file_name}: {str(e)}\")\n",
    "\n",
    "        # If the download count reaches a multiple of batch size, notify\n",
    "        if download_counter % downloads_per_batch == 0:\n",
    "            print(f\"Batch {download_counter // downloads_per_batch} of {downloads_per_batch} completed\")\n",
    "\n",
    "        # If the download count reaches a multiple of 25, ask the user if they want to continue\n",
    "        if download_counter % 5 == 0: # after the fifth PDF downloaded, you'll listen a beautiful song\n",
    "            play_sound()\n",
    "            user_input = input(\"Do you want to continue downloading? (Enter 'y' to continue, any other key to stop): \")\n",
    "            pygame.mixer.music.stop()\n",
    "            if user_input.lower() != 'y':\n",
    "                break\n",
    "\n",
    "        # Random wait before the next iteration\n",
    "        random_wait(5, 10)\n",
    "\n",
    "        # If total downloads reached, break out of loop\n",
    "        if download_counter == total_downloads:\n",
    "            print(f\"All downloads completed ({total_downloads} in total)\")\n",
    "            break\n",
    "\n",
    "except StaleElementReferenceException:\n",
    "    print(\"StaleElementReferenceException occurred. Retrying...\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser when finished\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168995d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "Probably the WR (PDF files <span style=\"font-size: 24px;\">&#128462;</span>) were downloaded in a single folder (<code>raw_pdf</code>), but we would like the WR to be sorted by years. The following code sorts the PDFs <span style=\"font-size: 24px;\">&#128462;</span> into subfolders (years) for us by placing each WR according to the year of its publication. This happens in the <b>\"blink of an eye\"</b>. \n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee016a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the directory\n",
    "files = os.listdir(raw_pdf)\n",
    "\n",
    "# Call the function to organize files\n",
    "organize_files_by_year(raw_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9113a",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28a023",
   "metadata": {},
   "source": [
    "<div id=\"2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d24d2",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;; color: dark;\">2.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Generate PDF input with key tables</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666f310",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>Now that we have downloaded the WR from the Central Bank, we should know that each of these files has more than 100 pages, but not all of them contain the information required for this project.\n",
    "   </p> \n",
    "   <p>\n",
    "    All we really want is a couple of pages from each WR (PDF <span style=\"font-size: 24px;\">&#128462;</span>), one for <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 1</a> (monthly real GDP growth) and one for <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 2</a> (annual and quarterly real GDP growth). The code below is executed to maintain the 2 key pages with both tables of each PDF <span style=\"font-size: 24px;\">&#128462;</span> plus the cover page that contains the information that helps us identify one WR from another such as its date of publication and serial number.\n",
    "       </p>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code will reduce the number of pages of each PDF saved in the folder 'raw_pdf'.\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = [\"ECONOMIC SECTORS\"] # Keywords to identify the 2 key tables or pages within the 100+ page PDF.\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main Tkinter window\n",
    "\n",
    "    input_pdf_files = read_input_pdf_files()\n",
    "    processing_counter = 1\n",
    "\n",
    "    for folder in os.listdir(raw_pdf):\n",
    "        folder_path = os.path.join(raw_pdf, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(\"Processing folder:\", folder)\n",
    "            num_pdfs_trimmed = 0\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".pdf\"):\n",
    "                    pdf_file = os.path.join(folder_path, filename)\n",
    "                    if filename in input_pdf_files:\n",
    "                        print(f\"{processing_counter}. The PDF '{filename}' has already been trimmed and saved in '{input_pdf}'...\")\n",
    "                        processing_counter += 1\n",
    "                        continue\n",
    "                    print(f\"{processing_counter}. Processing:\", pdf_file)\n",
    "                    \n",
    "                    pages_with_keywords = search_keywords(pdf_file, keywords)\n",
    "                    num_pages_new_pdf = trim_pdf(pdf_file, pages_with_keywords)\n",
    "                    if num_pages_new_pdf > 0:\n",
    "                        num_pdfs_trimmed += 1\n",
    "                        input_pdf_files.add(filename)\n",
    "                        processing_counter += 1\n",
    "            \n",
    "            write_input_pdf_files(input_pdf_files)\n",
    "\n",
    "            message = f\"{num_pdfs_trimmed} PDFs have been trimmed in folder {folder}. Do you want to continue?\"\n",
    "            popup = PopupWindow(root, message)\n",
    "            root.wait_window(popup)\n",
    "            if not popup.result:\n",
    "                break\n",
    "                \n",
    "    print(\"Process completed for all PDFs in directory:\", input_pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643079a",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "Again, probably the WR (PDF files <span style=\"font-size: 24px;\">&#128462;</span>, now of few pages) were stored in disorder in the <code>input_pdf</code> folder. The following code sorts the PDFs <span style=\"font-size: 24px;\">&#128462;</span> into subfolders (years) by placing each WR (which now includes only the key tables) according to the year of its publication. This happens in the <b>\"blink of an eye\"</b>.  \n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the directory\n",
    "files = os.listdir(input_pdf)\n",
    "\n",
    "# Call the function to organize files\n",
    "organize_files_by_year(input_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045beb7b",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a384d2",
   "metadata": {},
   "source": [
    "<div id=\"3\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e1e92",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;; color: dark;\">3.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Data cleaning</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100857d4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "Since we already have the PDFs <span style=\"font-size: 24px;\">&#128462;</span> with just the tables required for this project, we can start extracting them. Then we can proceed with data cleaning.\n",
    "</p>  \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570beb4",
   "metadata": {},
   "source": [
    "<div id=\"3-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88eae51",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    A brief documentation on issus in the table information of the PDFs\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaeb25c",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "Note that, the table information within the PDFs <span style=\"font-size: 24px;\">&#128462;</span> are available as editable text (including numeric values), but sometimes they can have various encoded formats that can make them difficult to extract and clean up. Undoubtedly, this is the most challenging stage of this jupyter notebook because there is no single pattern in which the information in the PDFs <span style=\"font-size: 24px;\">&#128462;</span> is arranged, each PDF <span style=\"font-size: 24px;\">&#128462;</span> adds a difficulty to extract the information. To understand more about this last point, we will start this section by documenting the most common problems we may face when trying to extract and clean tables from PDFs <span style=\"font-size: 24px;\">&#128462;</span>.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d7c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e464cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd75758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7a5a767",
   "metadata": {},
   "source": [
    "<div id=\"3-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357318e8",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.2.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Extracting tables and data cleanup\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea58b1c",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "The main library used for extracting tables from PDFs <span style=\"font-size: 24px;\">&#128462;</span> is <code>pdfplumber</code>. You can review the official documentation by clicking <a href=\"https://github.com/jsvine/pdfplumber\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">here</a>.\n",
    "</p>\n",
    "    \n",
    "<p>     \n",
    "    The functions in <b>Section 3</b> of the <code>\"new_gdp_datasets_functions.py\"</code> script were built to deal with each of these issues. An interesting exercise is to compare the original tables (the ones in the PDF <span style=\"font-size: 24px;\">&#128462;</span>) and the cleaned tables (by the cleanup codes below). Thus, the cleanup codes for <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 1</a> and <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 2</a> generates two dictionaries, the first one stores the raw tables; that is, the original tables from the PDF <span style=\"font-size: 24px;\">&#128462;</span> extracted by the <code>pdfplumber</code> library, while the second dictionary stores the fully cleaned tables.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070fb47",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    The code iterates through each PDF <span style=\"font-size: 24px;\">&#128462;</span> and extracts the two required tables from each. The extracted information is then transformed into dataframes and the columns and values are cleaned up to conform to Python conventions (pythonic).\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3334671",
   "metadata": {},
   "source": [
    "<div id=\"3-2-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8436c2d1",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.2.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    <span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">Table 1.</span> Extraction and cleaning of data from tables on monthly real GDP growth rates.\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65139cc2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "The basic criterion to start extracting tables is to use keywords (sufficient condition). I mean, tables containing the following keywords meet the requirements to be extracted.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e77729e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to search in the page text\n",
    "keywords = [\"ECONOMIC SECTORS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4ac96",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"font-size: 24px; color: rgb(255, 32, 78); font-weight: bold;\">&#9888;</span>\n",
    "    <span style=\"font-family: PT Serif Pro Book; color: black; font-size: 16px;\">\n",
    "        Please check that the flat file <b>\"ns_dates.csv\"</b> is updated with the dates, years and ids for the newly downloaded PDF <span style=\"font-size: 24px;\">&#128462;</span> (WR). That file is located in the <b>\"ns_dates\"</b> folder and is uploaded to SQL from the jupyeter notebook <code>aux_files_to_sql.ipynb</code>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f028da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder 2013\n",
      "  1. DataFrame generated for file input_pdf\\2013\\ns-01-2013.pdf: ns_01_2013_1\n",
      "  2. DataFrame generated for file input_pdf\\2013\\ns-02-2013.pdf: ns_02_2013_1\n",
      "  3. DataFrame generated for file input_pdf\\2013\\ns-03-2013.pdf: ns_03_2013_1\n",
      "  4. DataFrame generated for file input_pdf\\2013\\ns-04-2013.pdf: ns_04_2013_1\n",
      "  5. DataFrame generated for file input_pdf\\2013\\ns-05-2013.pdf: ns_05_2013_1\n",
      "  6. DataFrame generated for file input_pdf\\2013\\ns-06-2013.pdf: ns_06_2013_1\n",
      "  7. DataFrame generated for file input_pdf\\2013\\ns-07-2013.pdf: ns_07_2013_1\n",
      "  8. DataFrame generated for file input_pdf\\2013\\ns-08-2013.pdf: ns_08_2013_1\n",
      "  9. DataFrame generated for file input_pdf\\2013\\ns-09-2013.pdf: ns_09_2013_1\n",
      "  10. DataFrame generated for file input_pdf\\2013\\ns-10-2013.pdf: ns_10_2013_1\n",
      "  11. DataFrame generated for file input_pdf\\2013\\ns-11-2013.pdf: ns_11_2013_1\n",
      "  12. DataFrame generated for file input_pdf\\2013\\ns-12-2013.pdf: ns_12_2013_1\n",
      "  13. DataFrame generated for file input_pdf\\2013\\ns-13-2013.pdf: ns_13_2013_1\n",
      "  14. DataFrame generated for file input_pdf\\2013\\ns-14-2013.pdf: ns_14_2013_1\n",
      "  15. DataFrame generated for file input_pdf\\2013\\ns-15-2013.pdf: ns_15_2013_1\n",
      "  16. DataFrame generated for file input_pdf\\2013\\ns-16-2013.pdf: ns_16_2013_1\n",
      "  17. DataFrame generated for file input_pdf\\2013\\ns-17-2013.pdf: ns_17_2013_1\n",
      "  18. DataFrame generated for file input_pdf\\2013\\ns-18-2013.pdf: ns_18_2013_1\n",
      "  19. DataFrame generated for file input_pdf\\2013\\ns-19-2013.pdf: ns_19_2013_1\n",
      "  20. DataFrame generated for file input_pdf\\2013\\ns-20-2013.pdf: ns_20_2013_1\n",
      "  21. DataFrame generated for file input_pdf\\2013\\ns-21-2013.pdf: ns_21_2013_1\n",
      "  22. DataFrame generated for file input_pdf\\2013\\ns-22-2013.pdf: ns_22_2013_1\n",
      "  23. DataFrame generated for file input_pdf\\2013\\ns-23-2013.pdf: ns_23_2013_1\n",
      "  24. DataFrame generated for file input_pdf\\2013\\ns-24-2013.pdf: ns_24_2013_1\n",
      "  25. DataFrame generated for file input_pdf\\2013\\ns-25-2013.pdf: ns_25_2013_1\n",
      "  26. DataFrame generated for file input_pdf\\2013\\ns-26-2013.pdf: ns_26_2013_1\n",
      "  27. DataFrame generated for file input_pdf\\2013\\ns-27-2013.pdf: ns_27_2013_1\n",
      "  28. DataFrame generated for file input_pdf\\2013\\ns-28-2013.pdf: ns_28_2013_1\n",
      "  29. DataFrame generated for file input_pdf\\2013\\ns-29-2013.pdf: ns_29_2013_1\n",
      "  30. DataFrame generated for file input_pdf\\2013\\ns-30-2013.pdf: ns_30_2013_1\n",
      "  31. DataFrame generated for file input_pdf\\2013\\ns-31-2013.pdf: ns_31_2013_1\n",
      "  32. DataFrame generated for file input_pdf\\2013\\ns-32-2013.pdf: ns_32_2013_1\n",
      "  33. DataFrame generated for file input_pdf\\2013\\ns-33-2013.pdf: ns_33_2013_1\n",
      "  34. DataFrame generated for file input_pdf\\2013\\ns-34-2013.pdf: ns_34_2013_1\n",
      "  35. DataFrame generated for file input_pdf\\2013\\ns-35-2013.pdf: ns_35_2013_1\n",
      "  36. DataFrame generated for file input_pdf\\2013\\ns-36-2013.pdf: ns_36_2013_1\n",
      "  37. DataFrame generated for file input_pdf\\2013\\ns-37-2013.pdf: ns_37_2013_1\n",
      "  38. DataFrame generated for file input_pdf\\2013\\ns-38-2013.pdf: ns_38_2013_1\n",
      "  39. DataFrame generated for file input_pdf\\2013\\ns-39-2013.pdf: ns_39_2013_1\n",
      "  40. DataFrame generated for file input_pdf\\2013\\ns-40-2013.pdf: ns_40_2013_1\n",
      "  41. DataFrame generated for file input_pdf\\2013\\ns-41-2013.pdf: ns_41_2013_1\n",
      "  42. DataFrame generated for file input_pdf\\2013\\ns-42-2013.pdf: ns_42_2013_1\n",
      "  43. DataFrame generated for file input_pdf\\2013\\ns-43-2013.pdf: ns_43_2013_1\n",
      "  44. DataFrame generated for file input_pdf\\2013\\ns-44-2013.pdf: ns_44_2013_1\n",
      "  45. DataFrame generated for file input_pdf\\2013\\ns-45-2013.pdf: ns_45_2013_1\n",
      "  46. DataFrame generated for file input_pdf\\2013\\ns-46-2013.pdf: ns_46_2013_1\n",
      "  47. DataFrame generated for file input_pdf\\2013\\ns-47-2013.pdf: ns_47_2013_1\n",
      "  48. DataFrame generated for file input_pdf\\2013\\ns-48-2013.pdf: ns_48_2013_1\n",
      "  49. DataFrame generated for file input_pdf\\2013\\ns-49-2013.pdf: ns_49_2013_1\n",
      "  50. DataFrame generated for file input_pdf\\2013\\ns-50-2013.pdf: ns_50_2013_1\n",
      "Processing folder 2014\n",
      "  1. DataFrame generated for file input_pdf\\2014\\ns-01-2014.pdf: ns_01_2014_1\n",
      "  2. DataFrame generated for file input_pdf\\2014\\ns-02-2014.pdf: ns_02_2014_1\n",
      "  3. DataFrame generated for file input_pdf\\2014\\ns-03-2014.pdf: ns_03_2014_1\n",
      "  4. DataFrame generated for file input_pdf\\2014\\ns-04-2014.pdf: ns_04_2014_1\n",
      "  5. DataFrame generated for file input_pdf\\2014\\ns-05-2014.pdf: ns_05_2014_1\n",
      "  6. DataFrame generated for file input_pdf\\2014\\ns-06-2014.pdf: ns_06_2014_1\n",
      "  7. DataFrame generated for file input_pdf\\2014\\ns-07-2014.pdf: ns_07_2014_1\n",
      "  8. DataFrame generated for file input_pdf\\2014\\ns-08-2014.pdf: ns_08_2014_1\n",
      "  9. DataFrame generated for file input_pdf\\2014\\ns-09-2014.pdf: ns_09_2014_1\n",
      "  10. DataFrame generated for file input_pdf\\2014\\ns-10-2014.pdf: ns_10_2014_1\n",
      "  11. DataFrame generated for file input_pdf\\2014\\ns-11-2014.pdf: ns_11_2014_1\n",
      "  12. DataFrame generated for file input_pdf\\2014\\ns-12-2014.pdf: ns_12_2014_1\n",
      "  13. DataFrame generated for file input_pdf\\2014\\ns-13-2014.pdf: ns_13_2014_1\n",
      "  14. DataFrame generated for file input_pdf\\2014\\ns-14-2014.pdf: ns_14_2014_1\n",
      "  15. DataFrame generated for file input_pdf\\2014\\ns-15-2014.pdf: ns_15_2014_1\n",
      "  16. DataFrame generated for file input_pdf\\2014\\ns-16-2014.pdf: ns_16_2014_1\n",
      "  17. DataFrame generated for file input_pdf\\2014\\ns-17-2014.pdf: ns_17_2014_1\n",
      "  18. DataFrame generated for file input_pdf\\2014\\ns-18-2014.pdf: ns_18_2014_1\n",
      "  19. DataFrame generated for file input_pdf\\2014\\ns-19-2014.pdf: ns_19_2014_1\n",
      "  20. DataFrame generated for file input_pdf\\2014\\ns-20-2014.pdf: ns_20_2014_1\n",
      "  21. DataFrame generated for file input_pdf\\2014\\ns-21-2014.pdf: ns_21_2014_1\n",
      "  22. DataFrame generated for file input_pdf\\2014\\ns-22-2014.pdf: ns_22_2014_1\n",
      "  23. DataFrame generated for file input_pdf\\2014\\ns-23-2014.pdf: ns_23_2014_1\n",
      "  24. DataFrame generated for file input_pdf\\2014\\ns-24-2014.pdf: ns_24_2014_1\n",
      "  25. DataFrame generated for file input_pdf\\2014\\ns-25-2014.pdf: ns_25_2014_1\n",
      "  26. DataFrame generated for file input_pdf\\2014\\ns-26-2014.pdf: ns_26_2014_1\n",
      "  27. DataFrame generated for file input_pdf\\2014\\ns-27-2014.pdf: ns_27_2014_1\n",
      "  28. DataFrame generated for file input_pdf\\2014\\ns-28-2014.pdf: ns_28_2014_1\n",
      "  29. DataFrame generated for file input_pdf\\2014\\ns-29-2014.pdf: ns_29_2014_1\n",
      "  30. DataFrame generated for file input_pdf\\2014\\ns-30-2014.pdf: ns_30_2014_1\n",
      "  31. DataFrame generated for file input_pdf\\2014\\ns-31-2014.pdf: ns_31_2014_1\n",
      "  32. DataFrame generated for file input_pdf\\2014\\ns-32-2014.pdf: ns_32_2014_1\n",
      "  33. DataFrame generated for file input_pdf\\2014\\ns-33-2014.pdf: ns_33_2014_1\n",
      "  34. DataFrame generated for file input_pdf\\2014\\ns-34-2014.pdf: ns_34_2014_1\n",
      "  35. DataFrame generated for file input_pdf\\2014\\ns-35-2014.pdf: ns_35_2014_1\n",
      "  36. DataFrame generated for file input_pdf\\2014\\ns-36-2014.pdf: ns_36_2014_1\n",
      "  37. DataFrame generated for file input_pdf\\2014\\ns-37-2014.pdf: ns_37_2014_1\n",
      "  38. DataFrame generated for file input_pdf\\2014\\ns-38-2014.pdf: ns_38_2014_1\n",
      "  39. DataFrame generated for file input_pdf\\2014\\ns-39-2014.pdf: ns_39_2014_1\n",
      "  40. DataFrame generated for file input_pdf\\2014\\ns-40-2014.pdf: ns_40_2014_1\n",
      "  41. DataFrame generated for file input_pdf\\2014\\ns-41-2014.pdf: ns_41_2014_1\n",
      "  42. DataFrame generated for file input_pdf\\2014\\ns-42-2014.pdf: ns_42_2014_1\n",
      "  43. DataFrame generated for file input_pdf\\2014\\ns-43-2014.pdf: ns_43_2014_1\n",
      "  44. DataFrame generated for file input_pdf\\2014\\ns-44-2014.pdf: ns_44_2014_1\n",
      "  45. DataFrame generated for file input_pdf\\2014\\ns-45-2014.pdf: ns_45_2014_1\n",
      "  46. DataFrame generated for file input_pdf\\2014\\ns-46-2014.pdf: ns_46_2014_1\n",
      "  47. DataFrame generated for file input_pdf\\2014\\ns-47-2014.pdf: ns_47_2014_1\n",
      "  48. DataFrame generated for file input_pdf\\2014\\ns-48-2014.pdf: ns_48_2014_1\n",
      "  49. DataFrame generated for file input_pdf\\2014\\ns-49-2014.pdf: ns_49_2014_1\n",
      "Processing folder 2015\n",
      "  1. DataFrame generated for file input_pdf\\2015\\ns-01-2015.pdf: ns_01_2015_1\n",
      "  2. DataFrame generated for file input_pdf\\2015\\ns-02-2015.pdf: ns_02_2015_1\n",
      "  3. DataFrame generated for file input_pdf\\2015\\ns-03-2015.pdf: ns_03_2015_1\n",
      "  4. DataFrame generated for file input_pdf\\2015\\ns-04-2015.pdf: ns_04_2015_1\n",
      "  5. DataFrame generated for file input_pdf\\2015\\ns-05-2015.pdf: ns_05_2015_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6. DataFrame generated for file input_pdf\\2015\\ns-06-2015.pdf: ns_06_2015_1\n",
      "  7. DataFrame generated for file input_pdf\\2015\\ns-07-2015.pdf: ns_07_2015_1\n",
      "  8. DataFrame generated for file input_pdf\\2015\\ns-08-2015.pdf: ns_08_2015_1\n",
      "  9. DataFrame generated for file input_pdf\\2015\\ns-09-2015.pdf: ns_09_2015_1\n",
      "  10. DataFrame generated for file input_pdf\\2015\\ns-10-2015.pdf: ns_10_2015_1\n",
      "  11. DataFrame generated for file input_pdf\\2015\\ns-11-2015.pdf: ns_11_2015_1\n",
      "  12. DataFrame generated for file input_pdf\\2015\\ns-12-2015.pdf: ns_12_2015_1\n",
      "  13. DataFrame generated for file input_pdf\\2015\\ns-13-2015.pdf: ns_13_2015_1\n",
      "  14. DataFrame generated for file input_pdf\\2015\\ns-14-2015.pdf: ns_14_2015_1\n",
      "  15. DataFrame generated for file input_pdf\\2015\\ns-15-2015.pdf: ns_15_2015_1\n",
      "  16. DataFrame generated for file input_pdf\\2015\\ns-16-2015.pdf: ns_16_2015_1\n",
      "  17. DataFrame generated for file input_pdf\\2015\\ns-17-2015.pdf: ns_17_2015_1\n",
      "  18. DataFrame generated for file input_pdf\\2015\\ns-18-2015.pdf: ns_18_2015_1\n",
      "  19. DataFrame generated for file input_pdf\\2015\\ns-19-2015.pdf: ns_19_2015_1\n",
      "  20. DataFrame generated for file input_pdf\\2015\\ns-20-2015.pdf: ns_20_2015_1\n",
      "  21. DataFrame generated for file input_pdf\\2015\\ns-21-2015.pdf: ns_21_2015_1\n",
      "  22. DataFrame generated for file input_pdf\\2015\\ns-22-2015.pdf: ns_22_2015_1\n",
      "  23. DataFrame generated for file input_pdf\\2015\\ns-23-2015.pdf: ns_23_2015_1\n",
      "  24. DataFrame generated for file input_pdf\\2015\\ns-24-2015.pdf: ns_24_2015_1\n",
      "  25. DataFrame generated for file input_pdf\\2015\\ns-25-2015.pdf: ns_25_2015_1\n",
      "  26. DataFrame generated for file input_pdf\\2015\\ns-26-2015.pdf: ns_26_2015_1\n",
      "  27. DataFrame generated for file input_pdf\\2015\\ns-27-2015.pdf: ns_27_2015_1\n",
      "  28. DataFrame generated for file input_pdf\\2015\\ns-28-2015.pdf: ns_28_2015_1\n",
      "  29. DataFrame generated for file input_pdf\\2015\\ns-29-2015.pdf: ns_29_2015_1\n",
      "  30. DataFrame generated for file input_pdf\\2015\\ns-30-2015.pdf: ns_30_2015_1\n",
      "  31. DataFrame generated for file input_pdf\\2015\\ns-31-2015.pdf: ns_31_2015_1\n",
      "  32. DataFrame generated for file input_pdf\\2015\\ns-32-2015.pdf: ns_32_2015_1\n",
      "  33. DataFrame generated for file input_pdf\\2015\\ns-33-2015.pdf: ns_33_2015_1\n",
      "  34. DataFrame generated for file input_pdf\\2015\\ns-34-2015.pdf: ns_34_2015_1\n",
      "  35. DataFrame generated for file input_pdf\\2015\\ns-35-2015.pdf: ns_35_2015_1\n",
      "  36. DataFrame generated for file input_pdf\\2015\\ns-36-2015.pdf: ns_36_2015_1\n",
      "  37. DataFrame generated for file input_pdf\\2015\\ns-37-2015.pdf: ns_37_2015_1\n",
      "  38. DataFrame generated for file input_pdf\\2015\\ns-38-2015.pdf: ns_38_2015_1\n",
      "  39. DataFrame generated for file input_pdf\\2015\\ns-39-2015.pdf: ns_39_2015_1\n",
      "  40. DataFrame generated for file input_pdf\\2015\\ns-40-2015.pdf: ns_40_2015_1\n",
      "  41. DataFrame generated for file input_pdf\\2015\\ns-41-2015.pdf: ns_41_2015_1\n",
      "  42. DataFrame generated for file input_pdf\\2015\\ns-42-2015.pdf: ns_42_2015_1\n",
      "  43. DataFrame generated for file input_pdf\\2015\\ns-43-2015.pdf: ns_43_2015_1\n",
      "  44. DataFrame generated for file input_pdf\\2015\\ns-44-2015.pdf: ns_44_2015_1\n",
      "  45. DataFrame generated for file input_pdf\\2015\\ns-45-2015.pdf: ns_45_2015_1\n",
      "  46. DataFrame generated for file input_pdf\\2015\\ns-46-2015.pdf: ns_46_2015_1\n",
      "  47. DataFrame generated for file input_pdf\\2015\\ns-47-2015.pdf: ns_47_2015_1\n",
      "  48. DataFrame generated for file input_pdf\\2015\\ns-48-2015.pdf: ns_48_2015_1\n",
      "Processing folder 2016\n",
      "  1. DataFrame generated for file input_pdf\\2016\\ns-01-2016.pdf: ns_01_2016_1\n",
      "  2. DataFrame generated for file input_pdf\\2016\\ns-02-2016.pdf: ns_02_2016_1\n",
      "  3. DataFrame generated for file input_pdf\\2016\\ns-03-2016.pdf: ns_03_2016_1\n",
      "  4. DataFrame generated for file input_pdf\\2016\\ns-04-2016.pdf: ns_04_2016_1\n",
      "  5. DataFrame generated for file input_pdf\\2016\\ns-05-2016.pdf: ns_05_2016_1\n",
      "  6. DataFrame generated for file input_pdf\\2016\\ns-06-2016.pdf: ns_06_2016_1\n",
      "  7. DataFrame generated for file input_pdf\\2016\\ns-07-2016.pdf: ns_07_2016_1\n",
      "  8. DataFrame generated for file input_pdf\\2016\\ns-08-2016.pdf: ns_08_2016_1\n",
      "  9. DataFrame generated for file input_pdf\\2016\\ns-09-2016.pdf: ns_09_2016_1\n",
      "  10. DataFrame generated for file input_pdf\\2016\\ns-10-2016.pdf: ns_10_2016_1\n",
      "  11. DataFrame generated for file input_pdf\\2016\\ns-11-2016.pdf: ns_11_2016_1\n",
      "  12. DataFrame generated for file input_pdf\\2016\\ns-12-2016.pdf: ns_12_2016_1\n",
      "  13. DataFrame generated for file input_pdf\\2016\\ns-13-2016.pdf: ns_13_2016_1\n",
      "  14. DataFrame generated for file input_pdf\\2016\\ns-14-2016.pdf: ns_14_2016_1\n",
      "  15. DataFrame generated for file input_pdf\\2016\\ns-15-2016.pdf: ns_15_2016_1\n",
      "  16. DataFrame generated for file input_pdf\\2016\\ns-16-2016.pdf: ns_16_2016_1\n",
      "  17. DataFrame generated for file input_pdf\\2016\\ns-17-2016.pdf: ns_17_2016_1\n",
      "  18. DataFrame generated for file input_pdf\\2016\\ns-18-2016.pdf: ns_18_2016_1\n",
      "  19. DataFrame generated for file input_pdf\\2016\\ns-19-2016.pdf: ns_19_2016_1\n",
      "  20. DataFrame generated for file input_pdf\\2016\\ns-20-2016.pdf: ns_20_2016_1\n",
      "  21. DataFrame generated for file input_pdf\\2016\\ns-21-2016.pdf: ns_21_2016_1\n",
      "  22. DataFrame generated for file input_pdf\\2016\\ns-22-2016.pdf: ns_22_2016_1\n",
      "  23. DataFrame generated for file input_pdf\\2016\\ns-23-2016.pdf: ns_23_2016_1\n",
      "  24. DataFrame generated for file input_pdf\\2016\\ns-24-2016.pdf: ns_24_2016_1\n",
      "  25. DataFrame generated for file input_pdf\\2016\\ns-25-2016.pdf: ns_25_2016_1\n",
      "  26. DataFrame generated for file input_pdf\\2016\\ns-26-2016.pdf: ns_26_2016_1\n",
      "  27. DataFrame generated for file input_pdf\\2016\\ns-27-2016.pdf: ns_27_2016_1\n",
      "  28. DataFrame generated for file input_pdf\\2016\\ns-28-2016.pdf: ns_28_2016_1\n",
      "  29. DataFrame generated for file input_pdf\\2016\\ns-29-2016.pdf: ns_29_2016_1\n",
      "  30. DataFrame generated for file input_pdf\\2016\\ns-30-2016.pdf: ns_30_2016_1\n",
      "  31. DataFrame generated for file input_pdf\\2016\\ns-31-2016.pdf: ns_31_2016_1\n",
      "  32. DataFrame generated for file input_pdf\\2016\\ns-32-2016.pdf: ns_32_2016_1\n",
      "  33. DataFrame generated for file input_pdf\\2016\\ns-33-2016.pdf: ns_33_2016_1\n",
      "  34. DataFrame generated for file input_pdf\\2016\\ns-34-2016.pdf: ns_34_2016_1\n",
      "  35. DataFrame generated for file input_pdf\\2016\\ns-35-2016.pdf: ns_35_2016_1\n",
      "  36. DataFrame generated for file input_pdf\\2016\\ns-36-2016.pdf: ns_36_2016_1\n",
      "  37. DataFrame generated for file input_pdf\\2016\\ns-37-2016.pdf: ns_37_2016_1\n",
      "  38. DataFrame generated for file input_pdf\\2016\\ns-38-2016.pdf: ns_38_2016_1\n",
      "  39. DataFrame generated for file input_pdf\\2016\\ns-39-2016.pdf: ns_39_2016_1\n",
      "  40. DataFrame generated for file input_pdf\\2016\\ns-40-2016.pdf: ns_40_2016_1\n",
      "  41. DataFrame generated for file input_pdf\\2016\\ns-41-2016.pdf: ns_41_2016_1\n",
      "  42. DataFrame generated for file input_pdf\\2016\\ns-42-2016.pdf: ns_42_2016_1\n",
      "  43. DataFrame generated for file input_pdf\\2016\\ns-43-2016.pdf: ns_43_2016_1\n",
      "  44. DataFrame generated for file input_pdf\\2016\\ns-44-2016.pdf: ns_44_2016_1\n",
      "  45. DataFrame generated for file input_pdf\\2016\\ns-45-2016.pdf: ns_45_2016_1\n",
      "  46. DataFrame generated for file input_pdf\\2016\\ns-46-2016.pdf: ns_46_2016_1\n",
      "  47. DataFrame generated for file input_pdf\\2016\\ns-47-2016.pdf: ns_47_2016_1\n",
      "  48. DataFrame generated for file input_pdf\\2016\\ns-48-2016.pdf: ns_48_2016_1\n",
      "Processing folder 2017\n",
      "  1. DataFrame generated for file input_pdf\\2017\\ns-01-2017.pdf: ns_01_2017_1\n",
      "  2. DataFrame generated for file input_pdf\\2017\\ns-02-2017.pdf: ns_02_2017_1\n",
      "  3. DataFrame generated for file input_pdf\\2017\\ns-03-2017.pdf: ns_03_2017_1\n",
      "  4. DataFrame generated for file input_pdf\\2017\\ns-04-2017.pdf: ns_04_2017_1\n",
      "  5. DataFrame generated for file input_pdf\\2017\\ns-05-2017.pdf: ns_05_2017_1\n",
      "  6. DataFrame generated for file input_pdf\\2017\\ns-06-2017.pdf: ns_06_2017_1\n",
      "  7. DataFrame generated for file input_pdf\\2017\\ns-07-2017.pdf: ns_07_2017_1\n",
      "  8. DataFrame generated for file input_pdf\\2017\\ns-08-2017.pdf: ns_08_2017_1\n",
      "  9. DataFrame generated for file input_pdf\\2017\\ns-09-2017.pdf: ns_09_2017_1\n",
      "  10. DataFrame generated for file input_pdf\\2017\\ns-10-2017.pdf: ns_10_2017_1\n",
      "  11. DataFrame generated for file input_pdf\\2017\\ns-11-2017.pdf: ns_11_2017_1\n",
      "  12. DataFrame generated for file input_pdf\\2017\\ns-12-2017.pdf: ns_12_2017_1\n",
      "  13. DataFrame generated for file input_pdf\\2017\\ns-13-2017.pdf: ns_13_2017_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14. DataFrame generated for file input_pdf\\2017\\ns-14-2017.pdf: ns_14_2017_1\n",
      "  15. DataFrame generated for file input_pdf\\2017\\ns-15-2017.pdf: ns_15_2017_1\n",
      "  16. DataFrame generated for file input_pdf\\2017\\ns-16-2017.pdf: ns_16_2017_1\n",
      "  17. DataFrame generated for file input_pdf\\2017\\ns-17-2017.pdf: ns_17_2017_1\n",
      "  18. DataFrame generated for file input_pdf\\2017\\ns-18-2017.pdf: ns_18_2017_1\n",
      "  19. DataFrame generated for file input_pdf\\2017\\ns-19-2017.pdf: ns_19_2017_1\n",
      "  20. DataFrame generated for file input_pdf\\2017\\ns-20-2017.pdf: ns_20_2017_1\n",
      "  21. DataFrame generated for file input_pdf\\2017\\ns-21-2017.pdf: ns_21_2017_1\n",
      "  22. DataFrame generated for file input_pdf\\2017\\ns-22-2017.pdf: ns_22_2017_1\n",
      "  23. DataFrame generated for file input_pdf\\2017\\ns-23-2017.pdf: ns_23_2017_1\n",
      "  24. DataFrame generated for file input_pdf\\2017\\ns-24-2017.pdf: ns_24_2017_1\n",
      "  25. DataFrame generated for file input_pdf\\2017\\ns-25-2017.pdf: ns_25_2017_1\n",
      "  26. DataFrame generated for file input_pdf\\2017\\ns-26-2017.pdf: ns_26_2017_1\n",
      "  27. DataFrame generated for file input_pdf\\2017\\ns-27-2017.pdf: ns_27_2017_1\n",
      "  28. DataFrame generated for file input_pdf\\2017\\ns-28-2017.pdf: ns_28_2017_1\n",
      "  29. DataFrame generated for file input_pdf\\2017\\ns-29-2017.pdf: ns_29_2017_1\n",
      "  30. DataFrame generated for file input_pdf\\2017\\ns-30-2017.pdf: ns_30_2017_1\n",
      "  31. DataFrame generated for file input_pdf\\2017\\ns-31-2017.pdf: ns_31_2017_1\n",
      "  32. DataFrame generated for file input_pdf\\2017\\ns-32-2017.pdf: ns_32_2017_1\n",
      "  33. DataFrame generated for file input_pdf\\2017\\ns-33-2017.pdf: ns_33_2017_1\n",
      "  34. DataFrame generated for file input_pdf\\2017\\ns-34-2017.pdf: ns_34_2017_1\n",
      "  35. DataFrame generated for file input_pdf\\2017\\ns-35-2017.pdf: ns_35_2017_1\n",
      "  36. DataFrame generated for file input_pdf\\2017\\ns-36-2017.pdf: ns_36_2017_1\n",
      "  37. DataFrame generated for file input_pdf\\2017\\ns-37-2017.pdf: ns_37_2017_1\n",
      "  38. DataFrame generated for file input_pdf\\2017\\ns-38-2017.pdf: ns_38_2017_1\n",
      "  39. DataFrame generated for file input_pdf\\2017\\ns-39-2017.pdf: ns_39_2017_1\n",
      "  40. DataFrame generated for file input_pdf\\2017\\ns-40-2017.pdf: ns_40_2017_1\n",
      "  41. DataFrame generated for file input_pdf\\2017\\ns-41-2017.pdf: ns_41_2017_1\n",
      "  42. DataFrame generated for file input_pdf\\2017\\ns-42-2017.pdf: ns_42_2017_1\n",
      "  43. DataFrame generated for file input_pdf\\2017\\ns-43-2017.pdf: ns_43_2017_1\n",
      "  44. DataFrame generated for file input_pdf\\2017\\ns-44-2017.pdf: ns_44_2017_1\n",
      "  45. DataFrame generated for file input_pdf\\2017\\ns-45-2017.pdf: ns_45_2017_1\n",
      "  46. DataFrame generated for file input_pdf\\2017\\ns-46-2017.pdf: ns_46_2017_1\n",
      "  47. DataFrame generated for file input_pdf\\2017\\ns-47-2017.pdf: ns_47_2017_1\n",
      "  48. DataFrame generated for file input_pdf\\2017\\ns-48-2017.pdf: ns_48_2017_1\n",
      "  49. DataFrame generated for file input_pdf\\2017\\ns-49-2017.pdf: ns_49_2017_1\n",
      "Processing folder 2018\n",
      "  1. DataFrame generated for file input_pdf\\2018\\ns-01-2018.pdf: ns_01_2018_1\n",
      "  2. DataFrame generated for file input_pdf\\2018\\ns-02-2018.pdf: ns_02_2018_1\n",
      "  3. DataFrame generated for file input_pdf\\2018\\ns-03-2018.pdf: ns_03_2018_1\n",
      "  4. DataFrame generated for file input_pdf\\2018\\ns-04-2018.pdf: ns_04_2018_1\n",
      "  5. DataFrame generated for file input_pdf\\2018\\ns-05-2018.pdf: ns_05_2018_1\n",
      "  6. DataFrame generated for file input_pdf\\2018\\ns-06-2018.pdf: ns_06_2018_1\n",
      "  7. DataFrame generated for file input_pdf\\2018\\ns-07-2018.pdf: ns_07_2018_1\n",
      "  8. DataFrame generated for file input_pdf\\2018\\ns-08-2018.pdf: ns_08_2018_1\n",
      "  9. DataFrame generated for file input_pdf\\2018\\ns-09-2018.pdf: ns_09_2018_1\n",
      "  10. DataFrame generated for file input_pdf\\2018\\ns-10-2018.pdf: ns_10_2018_1\n",
      "  11. DataFrame generated for file input_pdf\\2018\\ns-11-2018.pdf: ns_11_2018_1\n",
      "  12. DataFrame generated for file input_pdf\\2018\\ns-12-2018.pdf: ns_12_2018_1\n",
      "  13. DataFrame generated for file input_pdf\\2018\\ns-13-2018.pdf: ns_13_2018_1\n",
      "  14. DataFrame generated for file input_pdf\\2018\\ns-14-2018.pdf: ns_14_2018_1\n",
      "  15. DataFrame generated for file input_pdf\\2018\\ns-15-2018.pdf: ns_15_2018_1\n",
      "  16. DataFrame generated for file input_pdf\\2018\\ns-16-2018.pdf: ns_16_2018_1\n",
      "  17. DataFrame generated for file input_pdf\\2018\\ns-17-2018.pdf: ns_17_2018_1\n",
      "  18. DataFrame generated for file input_pdf\\2018\\ns-18-2018.pdf: ns_18_2018_1\n",
      "  19. DataFrame generated for file input_pdf\\2018\\ns-19-2018.pdf: ns_19_2018_1\n",
      "  20. DataFrame generated for file input_pdf\\2018\\ns-20-2018.pdf: ns_20_2018_1\n",
      "  21. DataFrame generated for file input_pdf\\2018\\ns-21-2018.pdf: ns_21_2018_1\n",
      "  22. DataFrame generated for file input_pdf\\2018\\ns-22-2018.pdf: ns_22_2018_1\n",
      "  23. DataFrame generated for file input_pdf\\2018\\ns-23-2018.pdf: ns_23_2018_1\n",
      "  24. DataFrame generated for file input_pdf\\2018\\ns-24-2018.pdf: ns_24_2018_1\n",
      "  25. DataFrame generated for file input_pdf\\2018\\ns-25-2018.pdf: ns_25_2018_1\n",
      "  26. DataFrame generated for file input_pdf\\2018\\ns-26-2018.pdf: ns_26_2018_1\n",
      "  27. DataFrame generated for file input_pdf\\2018\\ns-27-2018.pdf: ns_27_2018_1\n",
      "  28. DataFrame generated for file input_pdf\\2018\\ns-28-2018.pdf: ns_28_2018_1\n",
      "  29. DataFrame generated for file input_pdf\\2018\\ns-29-2018.pdf: ns_29_2018_1\n",
      "  30. DataFrame generated for file input_pdf\\2018\\ns-30-2018.pdf: ns_30_2018_1\n",
      "  31. DataFrame generated for file input_pdf\\2018\\ns-31-2018.pdf: ns_31_2018_1\n",
      "  32. DataFrame generated for file input_pdf\\2018\\ns-32-2018.pdf: ns_32_2018_1\n",
      "  33. DataFrame generated for file input_pdf\\2018\\ns-33-2018.pdf: ns_33_2018_1\n",
      "  34. DataFrame generated for file input_pdf\\2018\\ns-34-2018.pdf: ns_34_2018_1\n",
      "  35. DataFrame generated for file input_pdf\\2018\\ns-35-2018.pdf: ns_35_2018_1\n",
      "  36. DataFrame generated for file input_pdf\\2018\\ns-36-2018.pdf: ns_36_2018_1\n",
      "  37. DataFrame generated for file input_pdf\\2018\\ns-37-2018.pdf: ns_37_2018_1\n",
      "  38. DataFrame generated for file input_pdf\\2018\\ns-38-2018.pdf: ns_38_2018_1\n",
      "  39. DataFrame generated for file input_pdf\\2018\\ns-39-2018.pdf: ns_39_2018_1\n",
      "  40. DataFrame generated for file input_pdf\\2018\\ns-40-2018.pdf: ns_40_2018_1\n",
      "  41. DataFrame generated for file input_pdf\\2018\\ns-41-2018.pdf: ns_41_2018_1\n",
      "  42. DataFrame generated for file input_pdf\\2018\\ns-42-2018.pdf: ns_42_2018_1\n",
      "  43. DataFrame generated for file input_pdf\\2018\\ns-43-2018.pdf: ns_43_2018_1\n",
      "  44. DataFrame generated for file input_pdf\\2018\\ns-44-2018.pdf: ns_44_2018_1\n",
      "  45. DataFrame generated for file input_pdf\\2018\\ns-45-2018.pdf: ns_45_2018_1\n",
      "  46. DataFrame generated for file input_pdf\\2018\\ns-46-2018.pdf: ns_46_2018_1\n",
      "  47. DataFrame generated for file input_pdf\\2018\\ns-47-2018.pdf: ns_47_2018_1\n",
      "  48. DataFrame generated for file input_pdf\\2018\\ns-48-2018.pdf: ns_48_2018_1\n",
      "  49. DataFrame generated for file input_pdf\\2018\\ns-49-2018.pdf: ns_49_2018_1\n",
      "Processing folder 2019\n",
      "  1. DataFrame generated for file input_pdf\\2019\\ns-01-2019.pdf: ns_01_2019_1\n",
      "  2. DataFrame generated for file input_pdf\\2019\\ns-02-2019.pdf: ns_02_2019_1\n",
      "  3. DataFrame generated for file input_pdf\\2019\\ns-03-2019.pdf: ns_03_2019_1\n",
      "  4. DataFrame generated for file input_pdf\\2019\\ns-04-2019.pdf: ns_04_2019_1\n",
      "  5. DataFrame generated for file input_pdf\\2019\\ns-05-2019.pdf: ns_05_2019_1\n",
      "  6. DataFrame generated for file input_pdf\\2019\\ns-06-2019.pdf: ns_06_2019_1\n",
      "  7. DataFrame generated for file input_pdf\\2019\\ns-07-2019.pdf: ns_07_2019_1\n",
      "  8. DataFrame generated for file input_pdf\\2019\\ns-08-2019.pdf: ns_08_2019_1\n",
      "  9. DataFrame generated for file input_pdf\\2019\\ns-09-2019.pdf: ns_09_2019_1\n",
      "  10. DataFrame generated for file input_pdf\\2019\\ns-10-2019.pdf: ns_10_2019_1\n",
      "  11. DataFrame generated for file input_pdf\\2019\\ns-11-2019.pdf: ns_11_2019_1\n",
      "  12. DataFrame generated for file input_pdf\\2019\\ns-12-2019.pdf: ns_12_2019_1\n",
      "  13. DataFrame generated for file input_pdf\\2019\\ns-13-2019.pdf: ns_13_2019_1\n",
      "  14. DataFrame generated for file input_pdf\\2019\\ns-14-2019.pdf: ns_14_2019_1\n",
      "  15. DataFrame generated for file input_pdf\\2019\\ns-15-2019.pdf: ns_15_2019_1\n",
      "  16. DataFrame generated for file input_pdf\\2019\\ns-16-2019.pdf: ns_16_2019_1\n",
      "  17. DataFrame generated for file input_pdf\\2019\\ns-17-2019.pdf: ns_17_2019_1\n",
      "  18. DataFrame generated for file input_pdf\\2019\\ns-18-2019.pdf: ns_18_2019_1\n",
      "  19. DataFrame generated for file input_pdf\\2019\\ns-19-2019.pdf: ns_19_2019_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20. DataFrame generated for file input_pdf\\2019\\ns-20-2019.pdf: ns_20_2019_1\n",
      "  21. DataFrame generated for file input_pdf\\2019\\ns-21-2019.pdf: ns_21_2019_1\n",
      "  22. DataFrame generated for file input_pdf\\2019\\ns-22-2019.pdf: ns_22_2019_1\n",
      "  23. DataFrame generated for file input_pdf\\2019\\ns-23-2019.pdf: ns_23_2019_1\n",
      "  24. DataFrame generated for file input_pdf\\2019\\ns-24-2019.pdf: ns_24_2019_1\n",
      "  25. DataFrame generated for file input_pdf\\2019\\ns-25-2019.pdf: ns_25_2019_1\n",
      "  26. DataFrame generated for file input_pdf\\2019\\ns-26-2019.pdf: ns_26_2019_1\n",
      "  27. DataFrame generated for file input_pdf\\2019\\ns-27-2019.pdf: ns_27_2019_1\n",
      "  28. DataFrame generated for file input_pdf\\2019\\ns-28-2019.pdf: ns_28_2019_1\n",
      "  29. DataFrame generated for file input_pdf\\2019\\ns-29-2019.pdf: ns_29_2019_1\n",
      "  30. DataFrame generated for file input_pdf\\2019\\ns-30-2019.pdf: ns_30_2019_1\n",
      "  31. DataFrame generated for file input_pdf\\2019\\ns-31-2019.pdf: ns_31_2019_1\n",
      "  32. DataFrame generated for file input_pdf\\2019\\ns-32-2019.pdf: ns_32_2019_1\n",
      "  33. DataFrame generated for file input_pdf\\2019\\ns-33-2019.pdf: ns_33_2019_1\n",
      "  34. DataFrame generated for file input_pdf\\2019\\ns-34-2019.pdf: ns_34_2019_1\n",
      "  35. DataFrame generated for file input_pdf\\2019\\ns-35-2019.pdf: ns_35_2019_1\n",
      "  36. DataFrame generated for file input_pdf\\2019\\ns-36-2019.pdf: ns_36_2019_1\n",
      "  37. DataFrame generated for file input_pdf\\2019\\ns-37-2019.pdf: ns_37_2019_1\n",
      "  38. DataFrame generated for file input_pdf\\2019\\ns-38-2019.pdf: ns_38_2019_1\n",
      "  39. DataFrame generated for file input_pdf\\2019\\ns-39-2019.pdf: ns_39_2019_1\n",
      "Processing folder 2020\n",
      "  1. DataFrame generated for file input_pdf\\2020\\ns-01-2020.pdf: ns_01_2020_1\n",
      "  2. DataFrame generated for file input_pdf\\2020\\ns-02-2020.pdf: ns_02_2020_1\n",
      "  3. DataFrame generated for file input_pdf\\2020\\ns-03-2020.pdf: ns_03_2020_1\n",
      "  4. DataFrame generated for file input_pdf\\2020\\ns-04-2020.pdf: ns_04_2020_1\n",
      "  5. DataFrame generated for file input_pdf\\2020\\ns-05-2020.pdf: ns_05_2020_1\n",
      "  6. DataFrame generated for file input_pdf\\2020\\ns-06-2020.pdf: ns_06_2020_1\n",
      "  7. DataFrame generated for file input_pdf\\2020\\ns-07-2020.pdf: ns_07_2020_1\n",
      "  8. DataFrame generated for file input_pdf\\2020\\ns-08-2020.pdf: ns_08_2020_1\n",
      "  9. DataFrame generated for file input_pdf\\2020\\ns-09-2020.pdf: ns_09_2020_1\n",
      "  10. DataFrame generated for file input_pdf\\2020\\ns-10-2020.pdf: ns_10_2020_1\n",
      "  11. DataFrame generated for file input_pdf\\2020\\ns-11-2020.pdf: ns_11_2020_1\n",
      "  12. DataFrame generated for file input_pdf\\2020\\ns-12-2020.pdf: ns_12_2020_1\n",
      "  13. DataFrame generated for file input_pdf\\2020\\ns-13-2020.pdf: ns_13_2020_1\n",
      "  14. DataFrame generated for file input_pdf\\2020\\ns-14-2020.pdf: ns_14_2020_1\n",
      "  15. DataFrame generated for file input_pdf\\2020\\ns-15-2020.pdf: ns_15_2020_1\n",
      "  16. DataFrame generated for file input_pdf\\2020\\ns-16-2020.pdf: ns_16_2020_1\n",
      "  17. DataFrame generated for file input_pdf\\2020\\ns-17-2020.pdf: ns_17_2020_1\n",
      "  18. DataFrame generated for file input_pdf\\2020\\ns-18-2020.pdf: ns_18_2020_1\n",
      "  19. DataFrame generated for file input_pdf\\2020\\ns-19-2020.pdf: ns_19_2020_1\n",
      "  20. DataFrame generated for file input_pdf\\2020\\ns-20-2020.pdf: ns_20_2020_1\n",
      "  21. DataFrame generated for file input_pdf\\2020\\ns-21-2020.pdf: ns_21_2020_1\n",
      "  22. DataFrame generated for file input_pdf\\2020\\ns-22-2020.pdf: ns_22_2020_1\n",
      "  23. DataFrame generated for file input_pdf\\2020\\ns-23-2020.pdf: ns_23_2020_1\n",
      "  24. DataFrame generated for file input_pdf\\2020\\ns-24-2020.pdf: ns_24_2020_1\n",
      "  25. DataFrame generated for file input_pdf\\2020\\ns-25-2020.pdf: ns_25_2020_1\n",
      "  26. DataFrame generated for file input_pdf\\2020\\ns-26-2020.pdf: ns_26_2020_1\n",
      "  27. DataFrame generated for file input_pdf\\2020\\ns-27-2020.pdf: ns_27_2020_1\n",
      "  28. DataFrame generated for file input_pdf\\2020\\ns-28-2020.pdf: ns_28_2020_1\n",
      "  29. DataFrame generated for file input_pdf\\2020\\ns-29-2020.pdf: ns_29_2020_1\n",
      "  30. DataFrame generated for file input_pdf\\2020\\ns-30-2020.pdf: ns_30_2020_1\n",
      "  31. DataFrame generated for file input_pdf\\2020\\ns-31-2020.pdf: ns_31_2020_1\n",
      "  32. DataFrame generated for file input_pdf\\2020\\ns-32-2020.pdf: ns_32_2020_1\n",
      "  33. DataFrame generated for file input_pdf\\2020\\ns-33-2020.pdf: ns_33_2020_1\n",
      "  34. DataFrame generated for file input_pdf\\2020\\ns-34-2020.pdf: ns_34_2020_1\n",
      "  35. DataFrame generated for file input_pdf\\2020\\ns-35-2020.pdf: ns_35_2020_1\n",
      "  36. DataFrame generated for file input_pdf\\2020\\ns-36-2020.pdf: ns_36_2020_1\n",
      "  37. DataFrame generated for file input_pdf\\2020\\ns-37-2020.pdf: ns_37_2020_1\n",
      "  38. DataFrame generated for file input_pdf\\2020\\ns-38-2020.pdf: ns_38_2020_1\n",
      "  39. DataFrame generated for file input_pdf\\2020\\ns-39-2020.pdf: ns_39_2020_1\n",
      "  40. DataFrame generated for file input_pdf\\2020\\ns-40-2020.pdf: ns_40_2020_1\n",
      "  41. DataFrame generated for file input_pdf\\2020\\ns-41-2020.pdf: ns_41_2020_1\n",
      "  42. DataFrame generated for file input_pdf\\2020\\ns-42-2020.pdf: ns_42_2020_1\n",
      "  43. DataFrame generated for file input_pdf\\2020\\ns-43-2020.pdf: ns_43_2020_1\n",
      "  44. DataFrame generated for file input_pdf\\2020\\ns-44-2020.pdf: ns_44_2020_1\n",
      "  45. DataFrame generated for file input_pdf\\2020\\ns-45-2020.pdf: ns_45_2020_1\n",
      "  46. DataFrame generated for file input_pdf\\2020\\ns-46-2020.pdf: ns_46_2020_1\n",
      "  47. DataFrame generated for file input_pdf\\2020\\ns-47-2020.pdf: ns_47_2020_1\n",
      "Processing folder 2021\n",
      "  1. DataFrame generated for file input_pdf\\2021\\ns-01-2021.pdf: ns_01_2021_1\n",
      "  2. DataFrame generated for file input_pdf\\2021\\ns-02-2021.pdf: ns_02_2021_1\n",
      "  3. DataFrame generated for file input_pdf\\2021\\ns-03-2021.pdf: ns_03_2021_1\n",
      "  4. DataFrame generated for file input_pdf\\2021\\ns-04-2021.pdf: ns_04_2021_1\n",
      "  5. DataFrame generated for file input_pdf\\2021\\ns-05-2021.pdf: ns_05_2021_1\n",
      "  6. DataFrame generated for file input_pdf\\2021\\ns-06-2021.pdf: ns_06_2021_1\n",
      "  7. DataFrame generated for file input_pdf\\2021\\ns-07-2021.pdf: ns_07_2021_1\n",
      "  8. DataFrame generated for file input_pdf\\2021\\ns-08-2021.pdf: ns_08_2021_1\n",
      "  9. DataFrame generated for file input_pdf\\2021\\ns-09-2021.pdf: ns_09_2021_1\n",
      "  10. DataFrame generated for file input_pdf\\2021\\ns-10-2021.pdf: ns_10_2021_1\n",
      "  11. DataFrame generated for file input_pdf\\2021\\ns-11-2021.pdf: ns_11_2021_1\n",
      "  12. DataFrame generated for file input_pdf\\2021\\ns-12-2021.pdf: ns_12_2021_1\n",
      "  13. DataFrame generated for file input_pdf\\2021\\ns-13-2021.pdf: ns_13_2021_1\n",
      "  14. DataFrame generated for file input_pdf\\2021\\ns-14-2021.pdf: ns_14_2021_1\n",
      "  15. DataFrame generated for file input_pdf\\2021\\ns-15-2021.pdf: ns_15_2021_1\n",
      "  16. DataFrame generated for file input_pdf\\2021\\ns-16-2021.pdf: ns_16_2021_1\n",
      "  17. DataFrame generated for file input_pdf\\2021\\ns-17-2021.pdf: ns_17_2021_1\n",
      "  18. DataFrame generated for file input_pdf\\2021\\ns-18-2021.pdf: ns_18_2021_1\n",
      "  19. DataFrame generated for file input_pdf\\2021\\ns-19-2021.pdf: ns_19_2021_1\n",
      "  20. DataFrame generated for file input_pdf\\2021\\ns-20-2021.pdf: ns_20_2021_1\n",
      "  21. DataFrame generated for file input_pdf\\2021\\ns-21-2021.pdf: ns_21_2021_1\n",
      "  22. DataFrame generated for file input_pdf\\2021\\ns-22-2021.pdf: ns_22_2021_1\n",
      "  23. DataFrame generated for file input_pdf\\2021\\ns-23-2021.pdf: ns_23_2021_1\n",
      "  24. DataFrame generated for file input_pdf\\2021\\ns-24-2021.pdf: ns_24_2021_1\n",
      "  25. DataFrame generated for file input_pdf\\2021\\ns-25-2021.pdf: ns_25_2021_1\n",
      "  26. DataFrame generated for file input_pdf\\2021\\ns-26-2021.pdf: ns_26_2021_1\n",
      "  27. DataFrame generated for file input_pdf\\2021\\ns-27-2021.pdf: ns_27_2021_1\n",
      "  28. DataFrame generated for file input_pdf\\2021\\ns-28-2021.pdf: ns_28_2021_1\n",
      "  29. DataFrame generated for file input_pdf\\2021\\ns-29-2021.pdf: ns_29_2021_1\n",
      "  30. DataFrame generated for file input_pdf\\2021\\ns-30-2021.pdf: ns_30_2021_1\n",
      "  31. DataFrame generated for file input_pdf\\2021\\ns-31-2021.pdf: ns_31_2021_1\n",
      "  32. DataFrame generated for file input_pdf\\2021\\ns-32-2021.pdf: ns_32_2021_1\n",
      "  33. DataFrame generated for file input_pdf\\2021\\ns-33-2021.pdf: ns_33_2021_1\n",
      "  34. DataFrame generated for file input_pdf\\2021\\ns-34-2021.pdf: ns_34_2021_1\n",
      "  35. DataFrame generated for file input_pdf\\2021\\ns-35-2021.pdf: ns_35_2021_1\n",
      "  36. DataFrame generated for file input_pdf\\2021\\ns-36-2021.pdf: ns_36_2021_1\n",
      "  37. DataFrame generated for file input_pdf\\2021\\ns-37-2021.pdf: ns_37_2021_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  38. DataFrame generated for file input_pdf\\2021\\ns-38-2021.pdf: ns_38_2021_1\n",
      "  39. DataFrame generated for file input_pdf\\2021\\ns-39-2021.pdf: ns_39_2021_1\n",
      "  40. DataFrame generated for file input_pdf\\2021\\ns-40-2021.pdf: ns_40_2021_1\n",
      "  41. DataFrame generated for file input_pdf\\2021\\ns-41-2021.pdf: ns_41_2021_1\n",
      "  42. DataFrame generated for file input_pdf\\2021\\ns-42-2021.pdf: ns_42_2021_1\n",
      "  43. DataFrame generated for file input_pdf\\2021\\ns-43-2021.pdf: ns_43_2021_1\n",
      "  44. DataFrame generated for file input_pdf\\2021\\ns-44-2021.pdf: ns_44_2021_1\n",
      "  45. DataFrame generated for file input_pdf\\2021\\ns-45-2021.pdf: ns_45_2021_1\n",
      "  46. DataFrame generated for file input_pdf\\2021\\ns-46-2021.pdf: ns_46_2021_1\n",
      "Processing folder 2022\n",
      "  1. DataFrame generated for file input_pdf\\2022\\ns-01-2022.pdf: ns_01_2022_1\n",
      "  2. DataFrame generated for file input_pdf\\2022\\ns-02-2022.pdf: ns_02_2022_1\n",
      "  3. DataFrame generated for file input_pdf\\2022\\ns-03-2022.pdf: ns_03_2022_1\n",
      "  4. DataFrame generated for file input_pdf\\2022\\ns-04-2022.pdf: ns_04_2022_1\n",
      "  5. DataFrame generated for file input_pdf\\2022\\ns-05-2022.pdf: ns_05_2022_1\n",
      "  6. DataFrame generated for file input_pdf\\2022\\ns-06-2022.pdf: ns_06_2022_1\n",
      "  7. DataFrame generated for file input_pdf\\2022\\ns-07-2022.pdf: ns_07_2022_1\n",
      "  8. DataFrame generated for file input_pdf\\2022\\ns-08-2022.pdf: ns_08_2022_1\n",
      "  9. DataFrame generated for file input_pdf\\2022\\ns-09-2022.pdf: ns_09_2022_1\n",
      "  10. DataFrame generated for file input_pdf\\2022\\ns-10-2022.pdf: ns_10_2022_1\n",
      "  11. DataFrame generated for file input_pdf\\2022\\ns-11-2022.pdf: ns_11_2022_1\n",
      "  12. DataFrame generated for file input_pdf\\2022\\ns-12-2022.pdf: ns_12_2022_1\n",
      "  13. DataFrame generated for file input_pdf\\2022\\ns-13-2022.pdf: ns_13_2022_1\n",
      "  14. DataFrame generated for file input_pdf\\2022\\ns-14-2022.pdf: ns_14_2022_1\n",
      "  15. DataFrame generated for file input_pdf\\2022\\ns-15-2022.pdf: ns_15_2022_1\n",
      "  16. DataFrame generated for file input_pdf\\2022\\ns-16-2022.pdf: ns_16_2022_1\n",
      "  17. DataFrame generated for file input_pdf\\2022\\ns-17-2022.pdf: ns_17_2022_1\n",
      "  18. DataFrame generated for file input_pdf\\2022\\ns-18-2022.pdf: ns_18_2022_1\n",
      "  19. DataFrame generated for file input_pdf\\2022\\ns-19-2022.pdf: ns_19_2022_1\n",
      "  20. DataFrame generated for file input_pdf\\2022\\ns-20-2022.pdf: ns_20_2022_1\n",
      "  21. DataFrame generated for file input_pdf\\2022\\ns-21-2022.pdf: ns_21_2022_1\n",
      "  22. DataFrame generated for file input_pdf\\2022\\ns-22-2022.pdf: ns_22_2022_1\n",
      "  23. DataFrame generated for file input_pdf\\2022\\ns-23-2022.pdf: ns_23_2022_1\n",
      "  24. DataFrame generated for file input_pdf\\2022\\ns-24-2022.pdf: ns_24_2022_1\n",
      "  25. DataFrame generated for file input_pdf\\2022\\ns-25-2022.pdf: ns_25_2022_1\n",
      "  26. DataFrame generated for file input_pdf\\2022\\ns-26-2022.pdf: ns_26_2022_1\n",
      "  27. DataFrame generated for file input_pdf\\2022\\ns-27-2022.pdf: ns_27_2022_1\n",
      "  28. DataFrame generated for file input_pdf\\2022\\ns-28-2022.pdf: ns_28_2022_1\n",
      "  29. DataFrame generated for file input_pdf\\2022\\ns-29-2022.pdf: ns_29_2022_1\n",
      "  30. DataFrame generated for file input_pdf\\2022\\ns-30-2022.pdf: ns_30_2022_1\n",
      "  31. DataFrame generated for file input_pdf\\2022\\ns-31-2022.pdf: ns_31_2022_1\n",
      "  32. DataFrame generated for file input_pdf\\2022\\ns-32-2022.pdf: ns_32_2022_1\n",
      "  33. DataFrame generated for file input_pdf\\2022\\ns-33-2022.pdf: ns_33_2022_1\n",
      "  34. DataFrame generated for file input_pdf\\2022\\ns-34-2022.pdf: ns_34_2022_1\n",
      "  35. DataFrame generated for file input_pdf\\2022\\ns-35-2022.pdf: ns_35_2022_1\n",
      "  36. DataFrame generated for file input_pdf\\2022\\ns-36-2022.pdf: ns_36_2022_1\n",
      "  37. DataFrame generated for file input_pdf\\2022\\ns-37-2022.pdf: ns_37_2022_1\n",
      "  38. DataFrame generated for file input_pdf\\2022\\ns-38-2022.pdf: ns_38_2022_1\n",
      "  39. DataFrame generated for file input_pdf\\2022\\ns-39-2022.pdf: ns_39_2022_1\n",
      "  40. DataFrame generated for file input_pdf\\2022\\ns-40-2022.pdf: ns_40_2022_1\n",
      "  41. DataFrame generated for file input_pdf\\2022\\ns-41-2022.pdf: ns_41_2022_1\n",
      "  42. DataFrame generated for file input_pdf\\2022\\ns-42-2022.pdf: ns_42_2022_1\n",
      "  43. DataFrame generated for file input_pdf\\2022\\ns-43-2022.pdf: ns_43_2022_1\n",
      "  44. DataFrame generated for file input_pdf\\2022\\ns-44-2022.pdf: ns_44_2022_1\n",
      "Processing folder 2023\n",
      "  1. DataFrame generated for file input_pdf\\2023\\ns-01-2023.pdf: ns_01_2023_1\n",
      "  2. DataFrame generated for file input_pdf\\2023\\ns-02-2023.pdf: ns_02_2023_1\n",
      "  3. DataFrame generated for file input_pdf\\2023\\ns-03-2023.pdf: ns_03_2023_1\n",
      "  4. DataFrame generated for file input_pdf\\2023\\ns-04-2023.pdf: ns_04_2023_1\n",
      "  5. DataFrame generated for file input_pdf\\2023\\ns-05-2023.pdf: ns_05_2023_1\n",
      "  6. DataFrame generated for file input_pdf\\2023\\ns-06-2023.pdf: ns_06_2023_1\n",
      "  7. DataFrame generated for file input_pdf\\2023\\ns-07-2023.pdf: ns_07_2023_1\n",
      "  8. DataFrame generated for file input_pdf\\2023\\ns-08-2023.pdf: ns_08_2023_1\n",
      "  9. DataFrame generated for file input_pdf\\2023\\ns-09-2023.pdf: ns_09_2023_1\n",
      "  10. DataFrame generated for file input_pdf\\2023\\ns-10-2023.pdf: ns_10_2023_1\n",
      "  11. DataFrame generated for file input_pdf\\2023\\ns-11-2023.pdf: ns_11_2023_1\n",
      "  12. DataFrame generated for file input_pdf\\2023\\ns-12-2023.pdf: ns_12_2023_1\n",
      "  13. DataFrame generated for file input_pdf\\2023\\ns-13-2023.pdf: ns_13_2023_1\n",
      "  14. DataFrame generated for file input_pdf\\2023\\ns-14-2023.pdf: ns_14_2023_1\n",
      "  15. DataFrame generated for file input_pdf\\2023\\ns-15-2023.pdf: ns_15_2023_1\n",
      "  16. DataFrame generated for file input_pdf\\2023\\ns-16-2023.pdf: ns_16_2023_1\n",
      "  17. DataFrame generated for file input_pdf\\2023\\ns-17-2023.pdf: ns_17_2023_1\n",
      "  18. DataFrame generated for file input_pdf\\2023\\ns-18-2023.pdf: ns_18_2023_1\n",
      "  19. DataFrame generated for file input_pdf\\2023\\ns-19-2023.pdf: ns_19_2023_1\n",
      "  20. DataFrame generated for file input_pdf\\2023\\ns-20-2023.pdf: ns_20_2023_1\n",
      "  21. DataFrame generated for file input_pdf\\2023\\ns-21-2023.pdf: ns_21_2023_1\n",
      "  22. DataFrame generated for file input_pdf\\2023\\ns-22-2023.pdf: ns_22_2023_1\n",
      "  23. DataFrame generated for file input_pdf\\2023\\ns-23-2023.pdf: ns_23_2023_1\n",
      "  24. DataFrame generated for file input_pdf\\2023\\ns-24-2023.pdf: ns_24_2023_1\n",
      "  25. DataFrame generated for file input_pdf\\2023\\ns-25-2023.pdf: ns_25_2023_1\n",
      "  26. DataFrame generated for file input_pdf\\2023\\ns-26-2023.pdf: ns_26_2023_1\n",
      "  27. DataFrame generated for file input_pdf\\2023\\ns-27-2023.pdf: ns_27_2023_1\n",
      "  28. DataFrame generated for file input_pdf\\2023\\ns-28-2023.pdf: ns_28_2023_1\n",
      "  29. DataFrame generated for file input_pdf\\2023\\ns-29-2023.pdf: ns_29_2023_1\n",
      "  30. DataFrame generated for file input_pdf\\2023\\ns-30-2023.pdf: ns_30_2023_1\n",
      "  31. DataFrame generated for file input_pdf\\2023\\ns-31-2023.pdf: ns_31_2023_1\n",
      "  32. DataFrame generated for file input_pdf\\2023\\ns-32-2023.pdf: ns_32_2023_1\n",
      "  33. DataFrame generated for file input_pdf\\2023\\ns-33-2023.pdf: ns_33_2023_1\n",
      "  34. DataFrame generated for file input_pdf\\2023\\ns-34-2023.pdf: ns_34_2023_1\n",
      "  35. DataFrame generated for file input_pdf\\2023\\ns-35-2023.pdf: ns_35_2023_1\n",
      "  36. DataFrame generated for file input_pdf\\2023\\ns-36-2023.pdf: ns_36_2023_1\n",
      "  37. DataFrame generated for file input_pdf\\2023\\ns-37-2023.pdf: ns_37_2023_1\n",
      "  38. DataFrame generated for file input_pdf\\2023\\ns-38-2023.pdf: ns_38_2023_1\n",
      "  39. DataFrame generated for file input_pdf\\2023\\ns-39-2023.pdf: ns_39_2023_1\n",
      "  40. DataFrame generated for file input_pdf\\2023\\ns-40-2023.pdf: ns_40_2023_1\n",
      "  41. DataFrame generated for file input_pdf\\2023\\ns-41-2023.pdf: ns_41_2023_1\n",
      "  42. DataFrame generated for file input_pdf\\2023\\ns-42-2023.pdf: ns_42_2023_1\n",
      "  43. DataFrame generated for file input_pdf\\2023\\ns-43-2023.pdf: ns_43_2023_1\n",
      "Processing folder 2024\n",
      "  1. DataFrame generated for file input_pdf\\2024\\ns-01-2024.pdf: ns_01_2024_1\n",
      "  2. DataFrame generated for file input_pdf\\2024\\ns-02-2024.pdf: ns_02_2024_1\n",
      "  3. DataFrame generated for file input_pdf\\2024\\ns-03-2024.pdf: ns_03_2024_1\n",
      "  4. DataFrame generated for file input_pdf\\2024\\ns-04-2024.pdf: ns_04_2024_1\n",
      "  5. DataFrame generated for file input_pdf\\2024\\ns-05-2024.pdf: ns_05_2024_1\n",
      "  6. DataFrame generated for file input_pdf\\2024\\ns-06-2024.pdf: ns_06_2024_1\n",
      "  7. DataFrame generated for file input_pdf\\2024\\ns-07-2024.pdf: ns_07_2024_1\n",
      "  8. DataFrame generated for file input_pdf\\2024\\ns-08-2024.pdf: ns_08_2024_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9. DataFrame generated for file input_pdf\\2024\\ns-09-2024.pdf: ns_09_2024_1\n",
      "  10. DataFrame generated for file input_pdf\\2024\\ns-10-2024.pdf: ns_10_2024_1\n",
      "  11. DataFrame generated for file input_pdf\\2024\\ns-11-2024.pdf: ns_11_2024_1\n",
      "  12. DataFrame generated for file input_pdf\\2024\\ns-12-2024.pdf: ns_12_2024_1\n",
      "  13. DataFrame generated for file input_pdf\\2024\\ns-13-2024.pdf: ns_13_2024_1\n",
      "  14. DataFrame generated for file input_pdf\\2024\\ns-14-2024.pdf: ns_14_2024_1\n",
      "  15. DataFrame generated for file input_pdf\\2024\\ns-15-2024.pdf: ns_15_2024_1\n",
      "  16. DataFrame generated for file input_pdf\\2024\\ns-16-2024.pdf: ns_16_2024_1\n",
      "  17. DataFrame generated for file input_pdf\\2024\\ns-17-2024.pdf: ns_17_2024_1\n",
      "  18. DataFrame generated for file input_pdf\\2024\\ns-18-2024.pdf: ns_18_2024_1\n",
      "  19. DataFrame generated for file input_pdf\\2024\\ns-19-2024.pdf: ns_19_2024_1\n",
      "  20. DataFrame generated for file input_pdf\\2024\\ns-20-2024.pdf: ns_20_2024_1\n",
      "  21. DataFrame generated for file input_pdf\\2024\\ns-21-2024.pdf: ns_21_2024_1\n",
      "  22. DataFrame generated for file input_pdf\\2024\\ns-22-2024.pdf: ns_22_2024_1\n",
      "  23. DataFrame generated for file input_pdf\\2024\\ns-23-2024.pdf: ns_23_2024_1\n",
      "  24. DataFrame generated for file input_pdf\\2024\\ns-24-2024.pdf: ns_24_2024_1\n",
      "  25. DataFrame generated for file input_pdf\\2024\\ns-25-2024.pdf: ns_25_2024_1\n",
      "  26. DataFrame generated for file input_pdf\\2024\\ns-26-2024.pdf: ns_26_2024_1\n",
      "  27. DataFrame generated for file input_pdf\\2024\\ns-27-2024.pdf: ns_27_2024_1\n",
      "  28. DataFrame generated for file input_pdf\\2024\\ns-28-2024.pdf: ns_28_2024_1\n",
      "  29. DataFrame generated for file input_pdf\\2024\\ns-29-2024.pdf: ns_29_2024_1\n",
      "  30. DataFrame generated for file input_pdf\\2024\\ns-30-2024.pdf: ns_30_2024_1\n",
      "  31. DataFrame generated for file input_pdf\\2024\\ns-31-2024.pdf: ns_31_2024_1\n",
      "  32. DataFrame generated for file input_pdf\\2024\\ns-32-2024.pdf: ns_32_2024_1\n",
      "  33. DataFrame generated for file input_pdf\\2024\\ns-33-2024.pdf: ns_33_2024_1\n",
      "  34. DataFrame generated for file input_pdf\\2024\\ns-34-2024.pdf: ns_34_2024_1\n",
      "  35. DataFrame generated for file input_pdf\\2024\\ns-35-2024.pdf: ns_35_2024_1\n",
      "  36. DataFrame generated for file input_pdf\\2024\\ns-36-2024.pdf: ns_36_2024_1\n",
      "  37. DataFrame generated for file input_pdf\\2024\\ns-37-2024.pdf: ns_37_2024_1\n",
      "  38. DataFrame generated for file input_pdf\\2024\\ns-38-2024.pdf: ns_38_2024_1\n",
      "  39. DataFrame generated for file input_pdf\\2024\\ns-39-2024.pdf: ns_39_2024_1\n",
      "  40. DataFrame generated for file input_pdf\\2024\\ns-40-2024.pdf: ns_40_2024_1\n",
      "  41. DataFrame generated for file input_pdf\\2024\\ns-41-2024.pdf: ns_41_2024_1\n",
      "  42. DataFrame generated for file input_pdf\\2024\\ns-42-2024.pdf: ns_42_2024_1\n",
      "  43. DataFrame generated for file input_pdf\\2024\\ns-43-2024.pdf: ns_43_2024_1\n",
      "Processing completed for all folders.\n"
     ]
    }
   ],
   "source": [
    "# Set the locale to Spanish\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Dictionary to store generated DataFrames\n",
    "new_dataframes_dict_1 = {}\n",
    "\n",
    "# Path for the processed folders log file\n",
    "record_path = 'dataframes_record/new_processed_folders_1.txt'\n",
    "\n",
    "# Function to correct month names\n",
    "def correct_month_name(month):\n",
    "    months_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Add more mappings as needed for other month names\n",
    "    }\n",
    "    return months_mapping.get(month, month)\n",
    "\n",
    "# Function to register processed folder\n",
    "def register_processed_folder(folder, num_processed_files):\n",
    "    with open(record_path, 'a') as file:\n",
    "        file.write(f\"{folder}:{num_processed_files}\\n\")\n",
    "\n",
    "# Function to check if folder has been processed\n",
    "def folder_processed(folder):\n",
    "    if not os.path.exists(record_path):\n",
    "        return False\n",
    "    with open(record_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(folder):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Function to fetch date from database\n",
    "def get_date(df, engine):\n",
    "    id_ns = df['id_ns'].iloc[0]\n",
    "    year = df['year'].iloc[0]\n",
    "    query = f\"SELECT date FROM dates_growth_rates WHERE id_ns = '{id_ns}' AND year = '{year}';\"\n",
    "    date_result = pd.read_sql(query, engine)\n",
    "    return date_result.iloc[0, 0] if not date_result.empty else None\n",
    "\n",
    "# Function to process PDF file\n",
    "def process_pdf(pdf_path):\n",
    "    new_tables_dict_1 = {}  # Local dictionary for each PDF\n",
    "    table_counter = 1\n",
    "    keyword_count = 0 \n",
    "\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No matches found for id_ns and year in filename:\", filename)\n",
    "        return None, None, None, None, None  # Return None for new_tables_dict_1 as well\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(pdf_path))[0].replace('-', '_')\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            text = page.extract_text()\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                keyword_count += 1\n",
    "                if keyword_count == 1:  # Process only the first occurrence\n",
    "                    tables = tabula.read_pdf(pdf_path, pages=i, multiple_tables=False, stream=True) # Change stream to another option if desired\n",
    "                    for j, table_df in enumerate(tables, start=1):\n",
    "                        dataframe_name = f\"{new_filename}_{keyword_count}\"\n",
    "                        new_tables_dict_1[dataframe_name] = table_df\n",
    "                        table_counter += 1\n",
    "\n",
    "                    break  # Exit loop after finding the first occurrence\n",
    "\n",
    "    return id_ns, year, new_tables_dict_1, keyword_count\n",
    "\n",
    "# Function to process folder\n",
    "def process_folder(folder, engine):\n",
    "    print(f\"Processing folder {os.path.basename(folder)}\")\n",
    "    pdf_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.pdf')]\n",
    "\n",
    "    num_pdfs_processed = 0\n",
    "    num_dataframes_generated = 0\n",
    "\n",
    "    table_counter = 1  # Initialize table counter here\n",
    "    new_tables_dict_1 = {}  # Declare new_tables_dict_1 outside main loop\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        id_ns, year, tables_dict_temp, keyword_count = process_pdf(pdf_file)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for dataframe_name, df in tables_dict_temp.items():\n",
    "                file_name = os.path.splitext(os.path.basename(pdf_file))[0].replace('-', '_')\n",
    "                dataframe_name = f\"{file_name}_{keyword_count}\"\n",
    "                \n",
    "                # Store raw DataFrame in new_tables_dict_1\n",
    "                new_tables_dict_1[dataframe_name] = df.copy()\n",
    "                \n",
    "                # Apply cleaning functions to a copy of the DataFrame\n",
    "                df_clean = df.copy()\n",
    "\n",
    "                if any(col.isdigit() and len(col) == 4 for col in df_clean.columns):\n",
    "                    # If there is at least one column representing a year\n",
    "                    df_clean = swap_nan_se(df_clean)\n",
    "                    df_clean = split_column_by_pattern(df_clean)\n",
    "                    df_clean = drop_rare_caracter_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = relocate_last_columns(df_clean)\n",
    "                    df_clean = replace_first_dot(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = replace_var_perc_first_column(df_clean)\n",
    "                    df_clean = replace_var_perc_last_columns(df_clean)\n",
    "                    df_clean = replace_number_moving_average(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = exchange_values(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = find_year_column(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = convert_float(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = replace_mineria(df_clean)\n",
    "                    df_clean = replace_mining(df_clean)\n",
    "                    df_clean = rounding_values(df_clean, decimals=1)\n",
    "                else:\n",
    "                    # If there are no columns representing years\n",
    "                    df_clean = check_first_row(df_clean)\n",
    "                    df_clean = check_first_row_1(df_clean)\n",
    "                    df_clean = replace_first_row_with_columns(df_clean)\n",
    "                    df_clean = swap_nan_se(df_clean)\n",
    "                    df_clean = split_column_by_pattern(df_clean)\n",
    "                    df_clean = drop_rare_caracter_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = relocate_last_columns(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = replace_var_perc_first_column(df_clean)\n",
    "                    df_clean = replace_var_perc_last_columns(df_clean)\n",
    "                    df_clean = replace_number_moving_average(df_clean)\n",
    "                    df_clean = expand_column(df_clean)\n",
    "                    df_clean = split_values_1(df_clean)\n",
    "                    df_clean = split_values_2(df_clean)\n",
    "                    df_clean = split_values_3(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = exchange_values(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = find_year_column(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = convert_float(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column_1(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column_2(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column_3(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = replace_mineria(df_clean)\n",
    "                    df_clean = replace_mining(df_clean)\n",
    "                    df_clean = rounding_values(df_clean, decimals=1)\n",
    "                \n",
    "                # Add 'year' column to cleaned DataFrame\n",
    "                df_clean.insert(0, 'year', year)\n",
    "                \n",
    "                # Add 'id_ns' column to cleaned DataFrame\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                \n",
    "                # Get corresponding date from database\n",
    "                date = get_date(df_clean, engine)\n",
    "                if date:\n",
    "                    # Add 'date' column to cleaned DataFrame\n",
    "                    df_clean.insert(2, 'date', date)\n",
    "                else:\n",
    "                    print(\"Date not found in database for id_ns:\", id_ns, \"and year:\", year)\n",
    "                \n",
    "                # Store cleaned DataFrame in new_dataframes_dict_1\n",
    "                new_dataframes_dict_1[dataframe_name] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. DataFrame generated for file {pdf_file}: {dataframe_name}')\n",
    "                num_dataframes_generated += 1\n",
    "                table_counter += 1  # Increment table counter here\n",
    "        \n",
    "        num_pdfs_processed += 1  # Increment number of processed PDFs for each PDF in folder\n",
    "\n",
    "    return num_pdfs_processed, num_dataframes_generated, new_tables_dict_1\n",
    "\n",
    "# Function to process folders\n",
    "def process_folders():\n",
    "    pdf_folder = 'input_pdf'\n",
    "    folders = [os.path.join(pdf_folder, d) for d in os.listdir(pdf_folder) if os.path.isdir(os.path.join(pdf_folder, d))]\n",
    "    \n",
    "    new_tables_dict_1 = {}  # Initialize new_tables_dict_1 here\n",
    "    \n",
    "    for folder in folders:\n",
    "        if folder_processed(folder):\n",
    "            print(f\"Folder {folder} has already been processed.\")\n",
    "            continue\n",
    "        \n",
    "        num_pdfs_processed, num_dataframes_generated, tables_dict_temp = process_folder(folder, engine)\n",
    "        \n",
    "        # Update new_tables_dict_1 with values returned from process_folder()\n",
    "        new_tables_dict_1.update(tables_dict_temp)\n",
    "        \n",
    "        register_processed_folder(folder, num_pdfs_processed)\n",
    "\n",
    "        # Ask user if they want to continue with next folder\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Ensure the messagebox is in front\n",
    "        message = f\"Process {folder} complete. Processed {num_pdfs_processed} PDF(s) and generated {num_dataframes_generated} DataFrame(s). Continue with next folder?\"\n",
    "        if not messagebox.askyesno(\"Continue?\", message):\n",
    "            break\n",
    "            \n",
    "    print(\"Processing completed for all folders.\")  # Add a message to indicate completion\n",
    "    \n",
    "    return new_tables_dict_1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    engine = create_sqlalchemy_engine()\n",
    "    new_tables_dict_1 = process_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c63689",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15f0a6ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ns_01_2013_1', 'ns_02_2013_1', 'ns_03_2013_1', 'ns_04_2013_1', 'ns_05_2013_1', 'ns_06_2013_1', 'ns_07_2013_1', 'ns_08_2013_1', 'ns_09_2013_1', 'ns_10_2013_1', 'ns_11_2013_1', 'ns_12_2013_1', 'ns_13_2013_1', 'ns_14_2013_1', 'ns_15_2013_1', 'ns_16_2013_1', 'ns_17_2013_1', 'ns_18_2013_1', 'ns_19_2013_1', 'ns_20_2013_1', 'ns_21_2013_1', 'ns_22_2013_1', 'ns_23_2013_1', 'ns_24_2013_1', 'ns_25_2013_1', 'ns_26_2013_1', 'ns_27_2013_1', 'ns_28_2013_1', 'ns_29_2013_1', 'ns_30_2013_1', 'ns_31_2013_1', 'ns_32_2013_1', 'ns_33_2013_1', 'ns_34_2013_1', 'ns_35_2013_1', 'ns_36_2013_1', 'ns_37_2013_1', 'ns_38_2013_1', 'ns_39_2013_1', 'ns_40_2013_1', 'ns_41_2013_1', 'ns_42_2013_1', 'ns_43_2013_1', 'ns_44_2013_1', 'ns_45_2013_1', 'ns_46_2013_1', 'ns_47_2013_1', 'ns_48_2013_1', 'ns_49_2013_1', 'ns_50_2013_1', 'ns_01_2014_1', 'ns_02_2014_1', 'ns_03_2014_1', 'ns_04_2014_1', 'ns_05_2014_1', 'ns_06_2014_1', 'ns_07_2014_1', 'ns_08_2014_1', 'ns_09_2014_1', 'ns_10_2014_1', 'ns_11_2014_1', 'ns_12_2014_1', 'ns_13_2014_1', 'ns_14_2014_1', 'ns_15_2014_1', 'ns_16_2014_1', 'ns_17_2014_1', 'ns_18_2014_1', 'ns_19_2014_1', 'ns_20_2014_1', 'ns_21_2014_1', 'ns_22_2014_1', 'ns_23_2014_1', 'ns_24_2014_1', 'ns_25_2014_1', 'ns_26_2014_1', 'ns_27_2014_1', 'ns_28_2014_1', 'ns_29_2014_1', 'ns_30_2014_1', 'ns_31_2014_1', 'ns_32_2014_1', 'ns_33_2014_1', 'ns_34_2014_1', 'ns_35_2014_1', 'ns_36_2014_1', 'ns_37_2014_1', 'ns_38_2014_1', 'ns_39_2014_1', 'ns_40_2014_1', 'ns_41_2014_1', 'ns_42_2014_1', 'ns_43_2014_1', 'ns_44_2014_1', 'ns_45_2014_1', 'ns_46_2014_1', 'ns_47_2014_1', 'ns_48_2014_1', 'ns_49_2014_1', 'ns_01_2015_1', 'ns_02_2015_1', 'ns_03_2015_1', 'ns_04_2015_1', 'ns_05_2015_1', 'ns_06_2015_1', 'ns_07_2015_1', 'ns_08_2015_1', 'ns_09_2015_1', 'ns_10_2015_1', 'ns_11_2015_1', 'ns_12_2015_1', 'ns_13_2015_1', 'ns_14_2015_1', 'ns_15_2015_1', 'ns_16_2015_1', 'ns_17_2015_1', 'ns_18_2015_1', 'ns_19_2015_1', 'ns_20_2015_1', 'ns_21_2015_1', 'ns_22_2015_1', 'ns_23_2015_1', 'ns_24_2015_1', 'ns_25_2015_1', 'ns_26_2015_1', 'ns_27_2015_1', 'ns_28_2015_1', 'ns_29_2015_1', 'ns_30_2015_1', 'ns_31_2015_1', 'ns_32_2015_1', 'ns_33_2015_1', 'ns_34_2015_1', 'ns_35_2015_1', 'ns_36_2015_1', 'ns_37_2015_1', 'ns_38_2015_1', 'ns_39_2015_1', 'ns_40_2015_1', 'ns_41_2015_1', 'ns_42_2015_1', 'ns_43_2015_1', 'ns_44_2015_1', 'ns_45_2015_1', 'ns_46_2015_1', 'ns_47_2015_1', 'ns_48_2015_1', 'ns_01_2016_1', 'ns_02_2016_1', 'ns_03_2016_1', 'ns_04_2016_1', 'ns_05_2016_1', 'ns_06_2016_1', 'ns_07_2016_1', 'ns_08_2016_1', 'ns_09_2016_1', 'ns_10_2016_1', 'ns_11_2016_1', 'ns_12_2016_1', 'ns_13_2016_1', 'ns_14_2016_1', 'ns_15_2016_1', 'ns_16_2016_1', 'ns_17_2016_1', 'ns_18_2016_1', 'ns_19_2016_1', 'ns_20_2016_1', 'ns_21_2016_1', 'ns_22_2016_1', 'ns_23_2016_1', 'ns_24_2016_1', 'ns_25_2016_1', 'ns_26_2016_1', 'ns_27_2016_1', 'ns_28_2016_1', 'ns_29_2016_1', 'ns_30_2016_1', 'ns_31_2016_1', 'ns_32_2016_1', 'ns_33_2016_1', 'ns_34_2016_1', 'ns_35_2016_1', 'ns_36_2016_1', 'ns_37_2016_1', 'ns_38_2016_1', 'ns_39_2016_1', 'ns_40_2016_1', 'ns_41_2016_1', 'ns_42_2016_1', 'ns_43_2016_1', 'ns_44_2016_1', 'ns_45_2016_1', 'ns_46_2016_1', 'ns_47_2016_1', 'ns_48_2016_1', 'ns_01_2017_1', 'ns_02_2017_1', 'ns_03_2017_1', 'ns_04_2017_1', 'ns_05_2017_1', 'ns_06_2017_1', 'ns_07_2017_1', 'ns_08_2017_1', 'ns_09_2017_1', 'ns_10_2017_1', 'ns_11_2017_1', 'ns_12_2017_1', 'ns_13_2017_1', 'ns_14_2017_1', 'ns_15_2017_1', 'ns_16_2017_1', 'ns_17_2017_1', 'ns_18_2017_1', 'ns_19_2017_1', 'ns_20_2017_1', 'ns_21_2017_1', 'ns_22_2017_1', 'ns_23_2017_1', 'ns_24_2017_1', 'ns_25_2017_1', 'ns_26_2017_1', 'ns_27_2017_1', 'ns_28_2017_1', 'ns_29_2017_1', 'ns_30_2017_1', 'ns_31_2017_1', 'ns_32_2017_1', 'ns_33_2017_1', 'ns_34_2017_1', 'ns_35_2017_1', 'ns_36_2017_1', 'ns_37_2017_1', 'ns_38_2017_1', 'ns_39_2017_1', 'ns_40_2017_1', 'ns_41_2017_1', 'ns_42_2017_1', 'ns_43_2017_1', 'ns_44_2017_1', 'ns_45_2017_1', 'ns_46_2017_1', 'ns_47_2017_1', 'ns_48_2017_1', 'ns_49_2017_1', 'ns_01_2018_1', 'ns_02_2018_1', 'ns_03_2018_1', 'ns_04_2018_1', 'ns_05_2018_1', 'ns_06_2018_1', 'ns_07_2018_1', 'ns_08_2018_1', 'ns_09_2018_1', 'ns_10_2018_1', 'ns_11_2018_1', 'ns_12_2018_1', 'ns_13_2018_1', 'ns_14_2018_1', 'ns_15_2018_1', 'ns_16_2018_1', 'ns_17_2018_1', 'ns_18_2018_1', 'ns_19_2018_1', 'ns_20_2018_1', 'ns_21_2018_1', 'ns_22_2018_1', 'ns_23_2018_1', 'ns_24_2018_1', 'ns_25_2018_1', 'ns_26_2018_1', 'ns_27_2018_1', 'ns_28_2018_1', 'ns_29_2018_1', 'ns_30_2018_1', 'ns_31_2018_1', 'ns_32_2018_1', 'ns_33_2018_1', 'ns_34_2018_1', 'ns_35_2018_1', 'ns_36_2018_1', 'ns_37_2018_1', 'ns_38_2018_1', 'ns_39_2018_1', 'ns_40_2018_1', 'ns_41_2018_1', 'ns_42_2018_1', 'ns_43_2018_1', 'ns_44_2018_1', 'ns_45_2018_1', 'ns_46_2018_1', 'ns_47_2018_1', 'ns_48_2018_1', 'ns_49_2018_1', 'ns_01_2019_1', 'ns_02_2019_1', 'ns_03_2019_1', 'ns_04_2019_1', 'ns_05_2019_1', 'ns_06_2019_1', 'ns_07_2019_1', 'ns_08_2019_1', 'ns_09_2019_1', 'ns_10_2019_1', 'ns_11_2019_1', 'ns_12_2019_1', 'ns_13_2019_1', 'ns_14_2019_1', 'ns_15_2019_1', 'ns_16_2019_1', 'ns_17_2019_1', 'ns_18_2019_1', 'ns_19_2019_1', 'ns_20_2019_1', 'ns_21_2019_1', 'ns_22_2019_1', 'ns_23_2019_1', 'ns_24_2019_1', 'ns_25_2019_1', 'ns_26_2019_1', 'ns_27_2019_1', 'ns_28_2019_1', 'ns_29_2019_1', 'ns_30_2019_1', 'ns_31_2019_1', 'ns_32_2019_1', 'ns_33_2019_1', 'ns_34_2019_1', 'ns_35_2019_1', 'ns_36_2019_1', 'ns_37_2019_1', 'ns_38_2019_1', 'ns_39_2019_1', 'ns_01_2020_1', 'ns_02_2020_1', 'ns_03_2020_1', 'ns_04_2020_1', 'ns_05_2020_1', 'ns_06_2020_1', 'ns_07_2020_1', 'ns_08_2020_1', 'ns_09_2020_1', 'ns_10_2020_1', 'ns_11_2020_1', 'ns_12_2020_1', 'ns_13_2020_1', 'ns_14_2020_1', 'ns_15_2020_1', 'ns_16_2020_1', 'ns_17_2020_1', 'ns_18_2020_1', 'ns_19_2020_1', 'ns_20_2020_1', 'ns_21_2020_1', 'ns_22_2020_1', 'ns_23_2020_1', 'ns_24_2020_1', 'ns_25_2020_1', 'ns_26_2020_1', 'ns_27_2020_1', 'ns_28_2020_1', 'ns_29_2020_1', 'ns_30_2020_1', 'ns_31_2020_1', 'ns_32_2020_1', 'ns_33_2020_1', 'ns_34_2020_1', 'ns_35_2020_1', 'ns_36_2020_1', 'ns_37_2020_1', 'ns_38_2020_1', 'ns_39_2020_1', 'ns_40_2020_1', 'ns_41_2020_1', 'ns_42_2020_1', 'ns_43_2020_1', 'ns_44_2020_1', 'ns_45_2020_1', 'ns_46_2020_1', 'ns_47_2020_1', 'ns_01_2021_1', 'ns_02_2021_1', 'ns_03_2021_1', 'ns_04_2021_1', 'ns_05_2021_1', 'ns_06_2021_1', 'ns_07_2021_1', 'ns_08_2021_1', 'ns_09_2021_1', 'ns_10_2021_1', 'ns_11_2021_1', 'ns_12_2021_1', 'ns_13_2021_1', 'ns_14_2021_1', 'ns_15_2021_1', 'ns_16_2021_1', 'ns_17_2021_1', 'ns_18_2021_1', 'ns_19_2021_1', 'ns_20_2021_1', 'ns_21_2021_1', 'ns_22_2021_1', 'ns_23_2021_1', 'ns_24_2021_1', 'ns_25_2021_1', 'ns_26_2021_1', 'ns_27_2021_1', 'ns_28_2021_1', 'ns_29_2021_1', 'ns_30_2021_1', 'ns_31_2021_1', 'ns_32_2021_1', 'ns_33_2021_1', 'ns_34_2021_1', 'ns_35_2021_1', 'ns_36_2021_1', 'ns_37_2021_1', 'ns_38_2021_1', 'ns_39_2021_1', 'ns_40_2021_1', 'ns_41_2021_1', 'ns_42_2021_1', 'ns_43_2021_1', 'ns_44_2021_1', 'ns_45_2021_1', 'ns_46_2021_1', 'ns_01_2022_1', 'ns_02_2022_1', 'ns_03_2022_1', 'ns_04_2022_1', 'ns_05_2022_1', 'ns_06_2022_1', 'ns_07_2022_1', 'ns_08_2022_1', 'ns_09_2022_1', 'ns_10_2022_1', 'ns_11_2022_1', 'ns_12_2022_1', 'ns_13_2022_1', 'ns_14_2022_1', 'ns_15_2022_1', 'ns_16_2022_1', 'ns_17_2022_1', 'ns_18_2022_1', 'ns_19_2022_1', 'ns_20_2022_1', 'ns_21_2022_1', 'ns_22_2022_1', 'ns_23_2022_1', 'ns_24_2022_1', 'ns_25_2022_1', 'ns_26_2022_1', 'ns_27_2022_1', 'ns_28_2022_1', 'ns_29_2022_1', 'ns_30_2022_1', 'ns_31_2022_1', 'ns_32_2022_1', 'ns_33_2022_1', 'ns_34_2022_1', 'ns_35_2022_1', 'ns_36_2022_1', 'ns_37_2022_1', 'ns_38_2022_1', 'ns_39_2022_1', 'ns_40_2022_1', 'ns_41_2022_1', 'ns_42_2022_1', 'ns_43_2022_1', 'ns_44_2022_1', 'ns_01_2023_1', 'ns_02_2023_1', 'ns_03_2023_1', 'ns_04_2023_1', 'ns_05_2023_1', 'ns_06_2023_1', 'ns_07_2023_1', 'ns_08_2023_1', 'ns_09_2023_1', 'ns_10_2023_1', 'ns_11_2023_1', 'ns_12_2023_1', 'ns_13_2023_1', 'ns_14_2023_1', 'ns_15_2023_1', 'ns_16_2023_1', 'ns_17_2023_1', 'ns_18_2023_1', 'ns_19_2023_1', 'ns_20_2023_1', 'ns_21_2023_1', 'ns_22_2023_1', 'ns_23_2023_1', 'ns_24_2023_1', 'ns_25_2023_1', 'ns_26_2023_1', 'ns_27_2023_1', 'ns_28_2023_1', 'ns_29_2023_1', 'ns_30_2023_1', 'ns_31_2023_1', 'ns_32_2023_1', 'ns_33_2023_1', 'ns_34_2023_1', 'ns_35_2023_1', 'ns_36_2023_1', 'ns_37_2023_1', 'ns_38_2023_1', 'ns_39_2023_1', 'ns_40_2023_1', 'ns_41_2023_1', 'ns_42_2023_1', 'ns_43_2023_1', 'ns_01_2024_1', 'ns_02_2024_1', 'ns_03_2024_1', 'ns_04_2024_1', 'ns_05_2024_1', 'ns_06_2024_1', 'ns_07_2024_1', 'ns_08_2024_1', 'ns_09_2024_1', 'ns_10_2024_1', 'ns_11_2024_1', 'ns_12_2024_1', 'ns_13_2024_1', 'ns_14_2024_1', 'ns_15_2024_1', 'ns_16_2024_1', 'ns_17_2024_1', 'ns_18_2024_1', 'ns_19_2024_1', 'ns_20_2024_1', 'ns_21_2024_1', 'ns_22_2024_1', 'ns_23_2024_1', 'ns_24_2024_1', 'ns_25_2024_1', 'ns_26_2024_1', 'ns_27_2024_1', 'ns_28_2024_1', 'ns_29_2024_1', 'ns_30_2024_1', 'ns_31_2024_1', 'ns_32_2024_1', 'ns_33_2024_1', 'ns_34_2024_1', 'ns_35_2024_1', 'ns_36_2024_1', 'ns_37_2024_1', 'ns_38_2024_1', 'ns_39_2024_1', 'ns_40_2024_1', 'ns_41_2024_1', 'ns_42_2024_1', 'ns_43_2024_1'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables_dict_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329629e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_dataframes_dict_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6cc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_1['ns_43_2024_1'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ecc28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_1 = new_dataframes_dict_1['ns_43_2024_1']\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee02360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[(df_1['sectores_economicos'] == 'agropecuario') | (df_1['economic_sectors'] == 'agriculture and livestock')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0eb2f",
   "metadata": {},
   "source": [
    "<div id=\"3-2-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3f814",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.2.2.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    <span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">Table 2.</span> Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419acde4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "The basic criterion to start extracting tables is to use keywords (sufficient condition). I mean, tables containing the following keywords meet the requirements to be extracted.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b455f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to search in the page text\n",
    "keywords = [\"ECONOMIC SECTORS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86af3dc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"font-size: 24px; color: rgb(255, 32, 78); font-weight: bold;\">&#9888;</span>\n",
    "    <span style=\"font-family: PT Serif Pro Book; color: black; font-size: 16px;\">\n",
    "        Please check that the flat file <b>\"ns_dates.csv\"</b> is updated with the dates, years and ids for the newly downloaded PDF <span style=\"font-size: 24px;\">&#128462;</span> (WR). That file is located in the <code>ns_dates</code> folder and is uploaded to SQL from the jupyeter notebook <code>aux_files_to_sql.ipynb</code>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the locale to Spanish\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Dictionary to store generated DataFrames\n",
    "new_dataframes_dict_2 = {}\n",
    "\n",
    "# Path for the processed folders log file\n",
    "record_path = 'dataframes_record/new_processed_folders_2.txt'\n",
    "\n",
    "# Function to correct month names\n",
    "def correct_month_name(month):\n",
    "    months_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Add more mappings as needed for other month names\n",
    "    }\n",
    "    return months_mapping.get(month, month)\n",
    "\n",
    "# Function to register processed folder\n",
    "def register_processed_folder(folder, num_processed_files):\n",
    "    with open(record_path, 'a') as file:\n",
    "        file.write(f\"{folder}:{num_processed_files}\\n\")\n",
    "        \n",
    "# Function to check if folder has been processed\n",
    "def folder_processed(folder):\n",
    "    if not os.path.exists(record_path):\n",
    "        return False\n",
    "    with open(record_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(folder):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Function to fetch date from database\n",
    "def get_date(df, engine):\n",
    "    id_ns = df['id_ns'].iloc[0]\n",
    "    year = df['year'].iloc[0]\n",
    "    query = f\"SELECT date FROM dates_growth_rates WHERE id_ns = '{id_ns}' AND year = '{year}';\"\n",
    "    date_result = pd.read_sql(query, engine)\n",
    "    return date_result.iloc[0, 0] if not date_result.empty else None\n",
    "\n",
    "# Function to process PDF file\n",
    "def process_pdf(pdf_path):\n",
    "    tables_dict_2 = {}  # Local dictionary for each PDF\n",
    "    table_counter = 1\n",
    "    keyword_count = 0 \n",
    "\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No matches found for id_ns and year in filename:\", filename)\n",
    "        return None, None, None, None\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(pdf_path))[0].replace('-', '_')\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            text = page.extract_text()\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                keyword_count += 1\n",
    "                if keyword_count == 2:\n",
    "                    tables = tabula.read_pdf(pdf_path, pages=i, multiple_tables=False)\n",
    "                    for j, table_df in enumerate(tables, start=1):\n",
    "                        dataframe_name = f\"{new_filename}_{keyword_count}\"\n",
    "                        tables_dict_2[dataframe_name] = table_df\n",
    "                        table_counter += 1\n",
    "\n",
    "    return id_ns, year, tables_dict_2, keyword_count\n",
    "\n",
    "\n",
    "def process_folder(folder, engine):\n",
    "    print(f\"Processing folder {os.path.basename(folder)}\")\n",
    "    pdf_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.pdf')]\n",
    "\n",
    "    num_pdfs_processed = 0\n",
    "    num_dataframes_generated = 0\n",
    "\n",
    "    table_counter = 1  # Initialize table counter here\n",
    "    tables_dict_2 = {}  # Declare tables_dict outside main loop\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        id_ns, year, tables_dict_temp, keyword_count = process_pdf(pdf_file)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for dataframe_name, df in tables_dict_temp.items():\n",
    "                file_name = os.path.splitext(os.path.basename(pdf_file))[0].replace('-', '_')\n",
    "                dataframe_name = f\"{file_name}_{keyword_count}\"\n",
    "\n",
    "                # Store raw DataFrame in tables_dict\n",
    "                tables_dict_2[dataframe_name] = df.copy()\n",
    "\n",
    "                # Apply 20 lines of cleaning functions to a copy of the DataFrame\n",
    "                df_clean = df.copy()\n",
    "                if df_clean.iloc[0, 0] is np.nan:\n",
    "                    # Apply 20 lines of cleaning\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = separate_years(df_clean)\n",
    "                    df_clean = relocate_roman_numerals(df_clean)\n",
    "                    df_clean = extract_mixed_values(df_clean)\n",
    "                    df_clean = replace_first_row_nan(df_clean)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = drop_nan_row(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = split_values(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = roman_arabic(df_clean)\n",
    "                    df_clean = fix_duplicates(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = convert_float(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = replace_mineria(df_clean)\n",
    "                    df_clean = replace_mining(df_clean)\n",
    "                    df_clean = rounding_values(df_clean, decimals=1)\n",
    "                else:\n",
    "                    # Apply 15 lines of cleaning\n",
    "                    df_clean = exchange_roman_nan(df_clean)\n",
    "                    df_clean = exchange_columns(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = last_column_es(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = roman_arabic(df_clean)\n",
    "                    df_clean = fix_duplicates(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = convert_float(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = replace_mineria(df_clean)\n",
    "                    df_clean = replace_mining(df_clean)\n",
    "                    df_clean = rounding_values(df_clean, decimals=1)\n",
    "\n",
    "                # Add 'year' column to cleaned DataFrame\n",
    "                df_clean.insert(0, 'year', year)\n",
    "                \n",
    "                # Add 'id_ns' column to cleaned DataFrame\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                \n",
    "                # Get corresponding date from database\n",
    "                date = get_date(df_clean, engine)\n",
    "                if date:\n",
    "                    # Add 'date' column to cleaned DataFrame\n",
    "                    df_clean.insert(2, 'date', date)\n",
    "                else:\n",
    "                    print(\"Date not found in database for id_ns:\", id_ns, \"and year:\", year)\n",
    "\n",
    "                # Store cleaned DataFrame in new_dataframes_dict\n",
    "                new_dataframes_dict_2[dataframe_name] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. DataFrame generated for file {pdf_file}: {dataframe_name}')\n",
    "                num_dataframes_generated += 1\n",
    "                table_counter += 1  # Increment table counter here\n",
    "                    \n",
    "        num_pdfs_processed += 1  # Increment number of PDFs processed for each PDF in folder\n",
    "\n",
    "    return num_pdfs_processed, num_dataframes_generated, tables_dict_2\n",
    "        \n",
    "def process_folders():\n",
    "    pdf_folder = 'input_pdf'\n",
    "    folders = [os.path.join(pdf_folder, d) for d in os.listdir(pdf_folder) if os.path.isdir(os.path.join(pdf_folder, d))]\n",
    "\n",
    "    tables_dict_2 = {}  # Initialize tables_dict here\n",
    "    \n",
    "    for folder in folders:\n",
    "        if folder_processed(folder):\n",
    "            print(f\"Folder {folder} has already been processed.\")\n",
    "            continue\n",
    "        \n",
    "        num_pdfs_processed, num_dataframes_generated, tables_dict_temp = process_folder(folder, engine)\n",
    "        \n",
    "        # Update tables_dict with values returned from process_folder()\n",
    "        tables_dict_2.update(tables_dict_temp)\n",
    "        \n",
    "        register_processed_folder(folder, num_pdfs_processed)\n",
    "        \n",
    "        # Ask user if they want to continue with next folder\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Ensure the messagebox is in front\n",
    "        message = f\"Process {folder} complete. Processed {num_pdfs_processed} PDF(s) and generated {num_dataframes_generated} dataframes. Continue with next folder?\"\n",
    "        if not messagebox.askyesno(\"Continue?\", message):\n",
    "            break\n",
    "            \n",
    "    print(\"Processing completed for all folders.\")  # Add a message to indicate completion\n",
    "\n",
    "    return tables_dict_2  # Return tables_dict at the end of the function\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    engine = create_sqlalchemy_engine() # Creates the SQL connection to merge the date, year and id from a SQL database to dataframes\n",
    "    tables_dict_2 = process_folders()  # Capture the returned value from process_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77706358",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272f429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tables_dict_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a46bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_dataframes_dict_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict_2['ns_43_2024_2'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframes_dict_2['ns_43_2024_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d97b8",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217183e7",
   "metadata": {},
   "source": [
    "<div id=\"4\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8989d65",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Real-time data of Peru's GDP growth rates</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a04abd",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "This section creates the GDP growth rate vintages for Peru using <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 1</a> and <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 2</a>, which were extracted and cleaned in the previous section. Each table from each WR (PDF <span style=\"font-size: 24px;\">&#128462;</span>) was extracted and cleaned individually in the previous section. Here, we will concatenate all the tables for a specific economic sector, thus creating a vintage dataset of (real) GDP growth by economic sector from <b>2013</b> to <b>2024</b>.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9ebdf",
   "metadata": {},
   "source": [
    "<div id=\"select_sector\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ec604",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #00414C; color: white; padding: 10px;\">\n",
    "<h1><span style = \"color: #15F5BA; font-family: 'PT Serif Pro Book'; color: dark;\">$\\bullet$</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Select <code>sector_economico</code> and <code>economic_sector</code></span></h1>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb3ca8",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "When executing the following code, a window will be displayed with options in <b>Spanish</b> and <b>English</b> to select <b>economic sectors</b>. Choose them to concatenate Peru GDP growth rates (annual, quarterly or monthly) by sector.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to display the window and capture the selected values\n",
    "selected_spanish, selected_english, sector = show_option_window()\n",
    "\n",
    "# Display the selected values\n",
    "print(f\"You have selected sector = {sector}, selected_spanish = {selected_spanish}, and selected_english = {selected_english}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e461ff",
   "metadata": {},
   "source": [
    "<div id=\"select_freq\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e08e39",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #00414C; color: white; padding: 10px;\">\n",
    "<h1><span style = \"color: #15F5BA; font-family: 'PT Serif Pro Book'; color: dark;\">$\\bullet$</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Select <code>frequency</code></span></h1>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to show the popup window\n",
    "frequency = show_frequency_window()\n",
    "print(\"Selected frequency:\", frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ff093",
   "metadata": {},
   "source": [
    "<div id=\"counter\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542e449",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #00414C; color: white; padding: 10px;\">\n",
    "<h1><span style = \"color: #15F5BA; font-family: 'PT Serif Pro Book'; color: dark;\">$\\bullet$</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Set counter (dataframe name suffix)</span></h1>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ce0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to set the counter\n",
    "if frequency == \"monthly\":\n",
    "    counter = 1\n",
    "elif frequency == \"quarterly\":\n",
    "    counter = 2\n",
    "elif frequency == \"annual\":\n",
    "    counter = 2\n",
    "else:\n",
    "    counter = None \n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6ca98",
   "metadata": {},
   "source": [
    "<div id=\"4-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b142e",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Growth rates datasets concatenation for all frequencies\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically construct the function name and dictionary name\n",
    "function_name = f\"concatenate_{frequency}_df\"\n",
    "dataframe_dict_name = f\"new_dataframes_dict_{counter}\"\n",
    "\n",
    "# Check that both the function and dictionary exist in the global scope\n",
    "if function_name in globals() and dataframe_dict_name in globals():\n",
    "    # Call the function using its reference from globals()\n",
    "    globals()[f\"new_{sector}_{frequency}_growth_rates\"] = globals()[function_name](\n",
    "        globals()[dataframe_dict_name], selected_spanish, selected_english\n",
    "    )\n",
    "else:\n",
    "    print(f\"Error: {function_name} or {dataframe_dict_name} does not exist in the global scope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "globals()[f\"new_{sector}_{frequency}_growth_rates\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e88875",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80c892",
   "metadata": {},
   "source": [
    "<div id=\"5\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2418a10",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">5.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Uploading data to SQL</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f132e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "Finally, we upload all the datasets generated in this jupyter notebook to the <code>'gdp_revisions_datasets'</code> database of <code>PostgresSQL</code>.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c65736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_sqlalchemy_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24528012",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "Loading\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f624ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()[f\"new_{sector}_{frequency}_growth_rates\"].to_sql(f'new_{sector}_{frequency}_growth_rates', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707afbf5",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 20px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#select_sector\" style=\"color: rgb(255, 32, 78); text-decoration: none;\"></a>\n",
    "    </span> \n",
    "    <a href=\"#select_sector\" style=\"color: rgb(255, 32, 78); text-decoration: none;\">Back to select sectors.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6d115",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 20px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#select_freq\" style=\"color: rgb(255, 32, 78); text-decoration: none;\"></a>\n",
    "    </span> \n",
    "    <a href=\"#select_freq\" style=\"color: rgb(255, 32, 78); text-decoration: none;\">Back to select frequency.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd18b4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc920fd",
   "metadata": {},
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eae11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18533f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdp_revisions",
   "language": "python",
   "name": "gdp_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
