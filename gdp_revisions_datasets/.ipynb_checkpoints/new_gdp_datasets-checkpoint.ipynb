{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127505dd-d5cc-4829-9190-6314ae54b3ca",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; color: #292929;\">\n",
    "  <h1 style=\"margin-bottom: 10px;\">New GDP Real-Time Dataset</h1>\n",
    "  <div style=\"height: 2px; width: 90%; margin: 0 auto; background-color: #292929;\"></div>\n",
    "  <h2>Documentation</h2>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b19f9-8caf-48a1-ac70-90ff99489590",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-right: 40px;\">\n",
    "  <span style=\"display: inline-block; margin-right: 10px;\">\n",
    "    <a href=\"https://github.com/JasonCruz18\" target=\"_blank\">\n",
    "      <img src=\"https://cdn.jsdelivr.net/gh/devicons/devicon/icons/github/github-original.svg\" alt=\"GitHub\" style=\"width: 24px;\">\n",
    "    </a>\n",
    "  </span>\n",
    "  <span style=\"display: inline-block;\">\n",
    "    <a href=\"mailto:jj.cruza@up.edu.pe\">\n",
    "      <img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4e/Mail_%28iOS%29.svg\" alt=\"Email\" style=\"width: 24px;\">\n",
    "    </a>\n",
    "  </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96357866-58f8-4d8b-b25a-b9b145465322",
   "metadata": {},
   "source": [
    "**Author:** Jason Cruz  \n",
    "**Last updated:** 08/13/2025  \n",
    "**Python version:** 3.12  \n",
    "**Project:** Rationality and Nowcasting on Peruvian GDP Revisions  \n",
    "\n",
    "---\n",
    "## ðŸ“Œ Summary\n",
    "This notebook documents the step-by-step **construction of datasets** for analyzing **Peruvian GDP revisions** from 2013â€“2024.  \n",
    "It covers:\n",
    "1. **Data acquisition** from the Central Reserve Bank of Peru's Weekly Reports (PDF).\n",
    "2. **Data cleaning** and extraction of GDP tables.\n",
    "3. **Creation of real-time GDP vintages**.\n",
    "4. **Preparation of the final revisions dataset**.\n",
    "5. **Export to SQL** for further analysis.\n",
    "\n",
    "ðŸŒ **Main Data Source:** [BCRP Weekly Report](https://www.bcrp.gob.pe/publicaciones/nota-semanal.html) (ðŸ“° WR, from here on)  \n",
    "Any questions or issues regarding the coding, please email [Jason ðŸ“¨](mailto:jj.cruza@alum.up.edu.pe)  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907046a",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac40ed0",
   "metadata": {},
   "source": [
    "If you don't have the libraries below, please use the following code (as example) to install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c73193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d47b8-5873-426b-b739-e1fc05dcf8e5",
   "metadata": {},
   "source": [
    "Check out Python information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55588d8e-8df5-406a-8644-e67ff1dcbc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ Python Information\n",
      "  Version  : 3.12.1\n",
      "  Compiler : MSC v.1916 64 bit (AMD64)\n",
      "  Build    : ('main', 'Jan 19 2024 15:44:08')\n",
      "  OS       : Windows 10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"ðŸ Python Information\")\n",
    "print(f\"  Version  : {sys.version.split()[0]}\")\n",
    "print(f\"  Compiler : {platform.python_compiler()}\")\n",
    "print(f\"  Build    : {platform.python_build()}\")\n",
    "print(f\"  OS       : {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214f5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PDF downloader\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os  # For file and directory manipulation, for interacting with the operating system\n",
    "import random  # To generate random numbers\n",
    "from selenium import webdriver  # For automating web browsers\n",
    "from selenium.webdriver.common.by import By  # To locate elements on a webpage\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # To wait until certain conditions are met on a webpage.\n",
    "from selenium.webdriver.support import expected_conditions as EC  # To define expected conditions\n",
    "from selenium.common.exceptions import StaleElementReferenceException  # To handle exceptions related to elements on the webpage that are no longer available.\n",
    "import pygame # Allows you to handle graphics, sounds and input events.\n",
    "from webdriver_manager.chrome import ChromeDriverManager # To avoid compatibility issues with the ChromeDrive version of ChromeDrive\n",
    "\n",
    "import shutil # Used for high-level file operations, such as copying, moving, renaming, and deleting files and directories.\n",
    "\n",
    "\n",
    "# 2. Generate PDF input with key tables\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import fitz  # This library is used for working with PDF documents, including reading, writing, and modifying PDFs (PyMuPDF).\n",
    "import tkinter as tk  # This library is used for creating graphical user interfaces (GUIs) in Python.\n",
    "\n",
    "\n",
    "# 3. Data cleaning\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 3.1. A brief documentation on issus in the table information of the PDFs\n",
    "\n",
    "from PIL import Image  # Used for opening, manipulating, and saving image files.\n",
    "import matplotlib.pyplot as plt  # Used for creating static, animated, and interactive visualizations.\n",
    "\n",
    "# 3.2. Extracting tables and data cleanup\n",
    "\n",
    "import pdfplumber  # For extracting text and metadata from PDF files\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import unicodedata  # For manipulating Unicode data\n",
    "import re  # For regular expressions operations\n",
    "from datetime import datetime  # For working with dates and times\n",
    "import locale  # For locale-specific formatting of numbers, dates, and currencies\n",
    "\n",
    "# 3.2.1. Table 1. Extraction and cleaning of data from tables on monthly real GDP growth rates.\n",
    "\n",
    "import tabula  # Used to extract tables from PDF files into pandas DataFrames\n",
    "from tkinter import Tk, messagebox, TOP, YES, NO  # Used for creating graphical user interfaces\n",
    "from sqlalchemy import create_engine  # Used for connecting to and interacting with SQL databases\n",
    "\n",
    "# 3.2.2. Table 2. Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.\n",
    "\n",
    "import roman\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# 4. Real-time data of Peru's GDP growth rates\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import psycopg2  # For interacting with PostgreSQL databases\n",
    "from sqlalchemy import create_engine, text  # For creating and executing SQL queries using SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ef8f8",
   "metadata": {},
   "source": [
    "## âš™ï¸ Initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5afa5-c77a-4bd4-a0f4-509e2095728d",
   "metadata": {},
   "source": [
    "Before preprocessing new GDP releases data, we will:\n",
    "\n",
    "* **Create necessary folders** for storing inputs, outputs, logs, and screenshots.\n",
    "* **Connect to the PostgreSQL database** containing GDP revisions datasets.\n",
    "* **Import helper functions** from `new_gdp_datasets_functions.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc1c80",
   "metadata": {},
   "source": [
    "**Create necessary folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbb4b638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ digital_pdf created\n",
      "ðŸ“‚ digital_pdf\\raw_pdf created\n",
      "ðŸ“‚ digital_pdf\\input_pdf created\n",
      "ðŸ“‚ record created\n",
      "ðŸ“‚ alert_track created\n"
     ]
    }
   ],
   "source": [
    "# Define base folder for saving all digital PDFs\n",
    "digital_pdf = 'digital_pdf'\n",
    "\n",
    "# Define subfolder for saving the original PDFs as downloaded from the BCRP website\n",
    "raw_pdf = os.path.join(digital_pdf, 'raw_pdf')\n",
    "\n",
    "# Define subfolder for saving reduced PDFs containing only selected pages with GDP growth tables (monthly, quarterly, and annual frequencies)\n",
    "input_pdf = os.path.join(digital_pdf, 'input_pdf')\n",
    "\n",
    "# Define folder for saving .txt files with download and dataframe record\n",
    "record = 'record'\n",
    "\n",
    "# Define folder for saving warning bells. This is for download notifications (see section 1).\n",
    "alert_track = 'alert_track'\n",
    "\n",
    "# Create all required folders (if they do not already exist) and confirm creation\n",
    "for folder in [digital_pdf, raw_pdf, input_pdf, record, alert_track]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    print(f\"ðŸ“‚ {folder} created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36f49d-15ba-481e-b16b-0ae56d5d0c12",
   "metadata": {},
   "source": [
    "**Connect to the PostgreSQL database**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdd5dc",
   "metadata": {},
   "source": [
    "The following function will establish a connection to the `gdp_revisions_datasets` database in `PostgreSQL`. The **input data** used in this jupyter notebook will be loaded from this `PostgreSQL` database, and similarly, all **output data** generated by this jupyter notebook will be stored in that database. Ensure that you set the necessary parameters to access the server once you have obtained the required permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e982f83",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Tip:** To request permissions, please email [Jason ðŸ“¨](mailto:jj.cruza@alum.up.edu.pe)  \n",
    "> âš ï¸ **Warning:** Make sure you have set your SQL credentials as environment variables before proceeding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3b60634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sqlalchemy_engine(database=\"gdp_revisions_datasets\", port=5432):\n",
    "    \"\"\"\n",
    "    Create an SQLAlchemy engine to connect to the PostgreSQL database.\n",
    "    \n",
    "    Environment Variables Required:\n",
    "        CIUP_SQL_USER: SQL username\n",
    "        CIUP_SQL_PASS: SQL password\n",
    "        CIUP_SQL_HOST: SQL host address\n",
    "\n",
    "    Args:\n",
    "        database (str): Name of the database. Default is 'gdp_revisions_datasets'.\n",
    "        port (int): Port number. Default is 5432.\n",
    "\n",
    "    Returns:\n",
    "        engine (sqlalchemy.engine.Engine): SQLAlchemy engine object.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If required environment variables are missing.\n",
    "\n",
    "    Example:\n",
    "        engine = create_sqlalchemy_engine()\n",
    "    \"\"\"\n",
    "    user = os.environ.get('CIUP_SQL_USER')\n",
    "    password = os.environ.get('CIUP_SQL_PASS')\n",
    "    host = os.environ.get('CIUP_SQL_HOST')\n",
    "\n",
    "    if not all([host, user, password]):\n",
    "        raise ValueError(\"âŒ Missing environment variables: CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS\")\n",
    "\n",
    "    connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    print(f\"ðŸ”— Connected to PostgreSQL database: {database} at {host}:{port}\")\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a76e853-8dc9-45e6-9f30-cde33dd3966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Connected to PostgreSQL database: gdp_revisions_datasets at localhost:5432\n"
     ]
    }
   ],
   "source": [
    "engine = create_sqlalchemy_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b819ed7-9661-4efd-972e-b38e78f48ae3",
   "metadata": {},
   "source": [
    "**Import helper functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a836f",
   "metadata": {},
   "source": [
    "> âš ï¸ Please, check the script `new_gdp_datasets_functions.py` which contains all the functions required by this _jupyter notebook_. The functions there are ordered according to the sections of this jupyter notebok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d1e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_gdp_datasets_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f3192",
   "metadata": {},
   "source": [
    "## 1. PDF Downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98250a64",
   "metadata": {},
   "source": [
    "Our main source for data collection is the [BCRP Weekly Report](https://www.bcrp.gob.pe/publicaciones/nota-semanal.html). The weekly report is a periodic (weekly) publication of the BCRP in compliance with article 84 of the Peruvian Constitution and articles 2 and 74 of the BCRP's organic law, which include, among its functions, the periodic publication of the main national macroeconomic statistics.\n",
    "    \n",
    "Our project requires the publication of **two tables**: the table of monthly growth rates of real GDP (12-month percentage changes), and the table of quarterly (annual) growth rates of real GDP. These tables are referred to as **Table 1** and **Table 2**, respectively, throughout this jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d95c04",
   "metadata": {},
   "source": [
    "### Scraper bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcc4e3",
   "metadata": {},
   "source": [
    "This section automates the download of the **BCRP Weekly Report PDFs** directly from the official BCRP website.\n",
    "\n",
    "**What it does:**\n",
    "1. Opens the official BCRP Weekly Report page.\n",
    "2. Finds and collects all PDF links.\n",
    "3. Downloads them in chronological order (oldest to newest).\n",
    "4. Optionally plays a notification sound every N downloads.\n",
    "5. Organizes downloaded PDFs into year-based folders.\n",
    "\n",
    "> ðŸ’¡ If a CAPTCHA appears, solve it manually in the browser window and re-run the cell.\n",
    "\n",
    "> ðŸ” This script uses webdriver-manager to automatically handle browser drivers (default: Chrome), so you DO NOT need to manually download ChromeDriver, GeckoDriver, etc. If you want to change browser for your replication, modify the 'browser' parameter in init_driver().\n",
    "\n",
    "> ðŸŽµ Place your own MP3 file in `alert_track` folder for download notifications. Recommended free sources (CC0/public domain):\n",
    ">  - Pixabay Audio: https://pixabay.com/music/\n",
    ">  - FreeSound: https://freesound.org/\n",
    ">  - FreePD: https://freepd.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee0199bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¥ Starting PDF Downloader for BCRP Weekly Reports...\n",
      "\n",
      "ðŸŒ BCRP site opened successfully.\n",
      "ðŸ”Ž Found 153 WR on page (one per month).\n",
      "\n",
      "1. âœ… Downloaded: ns-04-2013.pdf\n",
      "â³ Waiting 7.29 seconds...\n",
      "2. âœ… Downloaded: ns-08-2013.pdf\n",
      "â³ Waiting 9.33 seconds...\n",
      "3. âœ… Downloaded: ns-12-2013.pdf\n",
      "â³ Waiting 8.19 seconds...\n",
      "4. âœ… Downloaded: ns-16-2013.pdf\n",
      "â³ Waiting 8.17 seconds...\n",
      "5. âœ… Downloaded: ns-21-2013.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "â¸ï¸ Continue? (y = yes, any other key = stop):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Waiting 8.25 seconds...\n",
      "6. âœ… Downloaded: ns-25-2013.pdf\n",
      "â³ Waiting 9.10 seconds...\n",
      "7. âœ… Downloaded: ns-29-2013.pdf\n",
      "â³ Waiting 5.47 seconds...\n",
      "8. âœ… Downloaded: ns-33-2013.pdf\n",
      "â³ Waiting 7.38 seconds...\n",
      "9. âœ… Downloaded: ns-37-2013.pdf\n",
      "â³ Waiting 5.77 seconds...\n",
      "10. âœ… Downloaded: ns-42-2013.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "â¸ï¸ Continue? (y = yes, any other key = stop):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Waiting 6.33 seconds...\n",
      "11. âœ… Downloaded: ns-46-2013.pdf\n",
      "â³ Waiting 7.03 seconds...\n",
      "12. âœ… Downloaded: ns-50-2013.pdf\n",
      "â³ Waiting 6.40 seconds...\n",
      "13. âœ… Downloaded: ns-04-2014.pdf\n",
      "â³ Waiting 9.25 seconds...\n",
      "14. âœ… Downloaded: ns-08-2014.pdf\n",
      "â³ Waiting 8.16 seconds...\n",
      "15. âœ… Downloaded: ns-12-2014.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "â¸ï¸ Continue? (y = yes, any other key = stop):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Waiting 5.22 seconds...\n",
      "16. âœ… Downloaded: ns-15-2014.pdf\n",
      "â³ Waiting 6.65 seconds...\n",
      "17. âœ… Downloaded: ns-20-2014.pdf\n",
      "â³ Waiting 7.23 seconds...\n",
      "18. âœ… Downloaded: ns-24-2014.pdf\n",
      "â³ Waiting 6.45 seconds...\n",
      "19. âœ… Downloaded: ns-28-2014.pdf\n",
      "â³ Waiting 6.15 seconds...\n",
      "20. âœ… Downloaded: ns-32-2014.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "â¸ï¸ Continue? (y = yes, any other key = stop):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ Download limit of 20 new PDFs reached.\n",
      "\n",
      "ðŸ‘‹ Browser closed.\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "Total monthly links kept: 153\n",
      "Newly downloaded: 20\n"
     ]
    }
   ],
   "source": [
    "# Run the function to start the scraper bot\n",
    "download_pdfs(\n",
    "    bcrp_url = \"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\",\n",
    "    raw_pdf_folder = raw_pdf,\n",
    "    download_record_folder = record,\n",
    "    alert_track_folder = alert_track,\n",
    "    max_downloads = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168995d",
   "metadata": {},
   "source": [
    "Probably the ðŸ“° WR were downloaded in a single folder, but we would like the WR to be sorted by years. The following code sorts the PDFs into subfolders (years) for us by placing each WR according to the year of its publication. This happens in the **\"blink of an eye\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee016a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the directory\n",
    "files = os.listdir(raw_pdf)\n",
    "\n",
    "# Call the function to organize files\n",
    "organize_files_by_year(raw_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d24d2",
   "metadata": {},
   "source": [
    "## 2. Generate PDF input with key tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666f310",
   "metadata": {},
   "source": [
    "Now that we have downloaded the ðŸ“° WR from the Central Bank, we should know that each of these files has more than 100 pages, but not all of them contain the information required for this project.\n",
    "\n",
    "All we really want is a couple of pages from each ðŸ“° WR, one for **Table 1** (monthly real GDP growth) and one for **Table 2** (annual and quarterly real GDP growth). The code below is executed to maintain the **two key pages** with both tables of each PDF plus the cover page that contains the information that helps us identify one ðŸ“° WR from another such as its date of publication and serial number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30e931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Folder paths\n",
    "raw_pdf = 'raw_pdf'\n",
    "input_pdf = 'input_pdf'\n",
    "input_pdf_record_folder = 'record'\n",
    "input_pdf_record_txt = 'input_pdf_record.txt'\n",
    "\n",
    "# Keywords to identify key pages\n",
    "keywords = [\"ECONOMIC SECTORS\"]\n",
    "\n",
    "# Read previously processed PDFs\n",
    "input_pdf_files = read_input_pdf_files(input_pdf_record_folder, input_pdf_record_txt)\n",
    "\n",
    "# Main loop over yearly folders\n",
    "for folder in sorted(os.listdir(raw_pdf)):\n",
    "    folder_path = os.path.join(raw_pdf, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nðŸ“‚ Processing folder: {folder}\")\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "    if not pdf_files:\n",
    "        continue\n",
    "\n",
    "    new_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    # tqdm progress bar with clean format (no ?PDF/s)\n",
    "    for filename in tqdm(pdf_files, desc=f\"Processing PDFs in {folder}\", unit=\"PDF\",\n",
    "                         bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt}\"):\n",
    "        pdf_file = os.path.join(folder_path, filename)\n",
    "        if filename in input_pdf_files:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        pages_with_keywords = search_keywords(pdf_file, keywords)\n",
    "        num_pages = trim_pdf(pdf_file, pages_with_keywords, output_folder=input_pdf)\n",
    "\n",
    "        if num_pages > 0:\n",
    "            input_pdf_files.add(filename)\n",
    "            new_count += 1\n",
    "\n",
    "    # Update the record file\n",
    "    write_input_pdf_files(input_pdf_files, input_pdf_record_folder, input_pdf_record_txt)\n",
    "\n",
    "    # Folder summary\n",
    "    print(f\"âœ… Trimmed PDFs saved in '{input_pdf}' ({new_count} new, {skipped_count} skipped)\")\n",
    "\n",
    "    # Ask user if they want to continue to the next folder\n",
    "    if not ask_continue_input(f\"Do you want to continue to the next folder after '{folder}'?\"):\n",
    "        print(\"ðŸ›‘ Process stopped by user.\")\n",
    "        break\n",
    "\n",
    "print(\"\\nðŸŽ‰ PDF input generation completed for all folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643079a",
   "metadata": {},
   "source": [
    "Again, probably the WR (PDF files, now of few pages) were stored in disorder in the `input_pdf` folder. The following code sorts the PDFs into subfolders (years) by placing each WR (which now includes only the key tables) according to the year of its publication. This happens in the **\"blink of an eye\"**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the directory\n",
    "files = os.listdir(input_pdf)\n",
    "\n",
    "# Call the function to organize files\n",
    "organize_files_by_year(input_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e1e92",
   "metadata": {},
   "source": [
    "## 3. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100857d4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "Since we already have the PDFs <span style=\"font-size: 24px;\">&#128462;</span> with just the tables required for this project, we can start extracting them. Then we can proceed with data cleaning.\n",
    "</p>  \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357318e8",
   "metadata": {},
   "source": [
    "### 3.2 Extracting tables and data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea58b1c",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "The main library used for extracting tables from PDFs <span style=\"font-size: 24px;\">&#128462;</span> is <code>pdfplumber</code>. You can review the official documentation by clicking <a href=\"https://github.com/jsvine/pdfplumber\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">here</a>.\n",
    "</p>\n",
    "    \n",
    "<p>     \n",
    "    The functions in <b>Section 3</b> of the <code>\"new_gdp_datasets_functions.py\"</code> script were built to deal with each of these issues. An interesting exercise is to compare the original tables (the ones in the PDF <span style=\"font-size: 24px;\">&#128462;</span>) and the cleaned tables (by the cleanup codes below). Thus, the cleanup codes for <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 1</a> and <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 2</a> generates two dictionaries, the first one stores the raw tables; that is, the original tables from the PDF <span style=\"font-size: 24px;\">&#128462;</span> extracted by the <code>pdfplumber</code> library, while the second dictionary stores the fully cleaned tables.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070fb47",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "    The code iterates through each PDF <span style=\"font-size: 24px;\">&#128462;</span> and extracts the two required tables from each. The extracted information is then transformed into dataframes and the columns and values are cleaned up to conform to Python conventions (pythonic).\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3334671",
   "metadata": {},
   "source": [
    "<div id=\"3-2-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8436c2d1",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.2.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    <span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">Table 1.</span> Extraction and cleaning of data from tables on monthly real GDP growth rates.\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65139cc2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "The basic criterion to start extracting tables is to use keywords (sufficient condition). I mean, tables containing the following keywords meet the requirements to be extracted.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77729e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to search in the page text\n",
    "keywords = [\"ECONOMIC SECTORS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4ac96",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"font-size: 24px; color: rgb(255, 32, 78); font-weight: bold;\">&#9888;</span>\n",
    "    <span style=\"font-family: PT Serif Pro Book; color: black; font-size: 16px;\">\n",
    "        Please check that the flat file <b>\"ns_dates.csv\"</b> is updated with the dates, years and ids for the newly downloaded PDF <span style=\"font-size: 24px;\">&#128462;</span> (WR). That file is located in the <b>\"ns_dates\"</b> folder and is uploaded to SQL from the jupyeter notebook <code>aux_files_to_sql.ipynb</code>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f028da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9. DataFrame generated for file input_pdf\\2024\\ns-09-2024.pdf: ns_09_2024_1\n",
      "  10. DataFrame generated for file input_pdf\\2024\\ns-10-2024.pdf: ns_10_2024_1\n",
      "  11. DataFrame generated for file input_pdf\\2024\\ns-11-2024.pdf: ns_11_2024_1\n",
      "  12. DataFrame generated for file input_pdf\\2024\\ns-12-2024.pdf: ns_12_2024_1\n",
      "  13. DataFrame generated for file input_pdf\\2024\\ns-13-2024.pdf: ns_13_2024_1\n",
      "  14. DataFrame generated for file input_pdf\\2024\\ns-14-2024.pdf: ns_14_2024_1\n",
      "  15. DataFrame generated for file input_pdf\\2024\\ns-15-2024.pdf: ns_15_2024_1\n",
      "  16. DataFrame generated for file input_pdf\\2024\\ns-16-2024.pdf: ns_16_2024_1\n",
      "  17. DataFrame generated for file input_pdf\\2024\\ns-17-2024.pdf: ns_17_2024_1\n",
      "  18. DataFrame generated for file input_pdf\\2024\\ns-18-2024.pdf: ns_18_2024_1\n",
      "  19. DataFrame generated for file input_pdf\\2024\\ns-19-2024.pdf: ns_19_2024_1\n",
      "  20. DataFrame generated for file input_pdf\\2024\\ns-20-2024.pdf: ns_20_2024_1\n",
      "  21. DataFrame generated for file input_pdf\\2024\\ns-21-2024.pdf: ns_21_2024_1\n",
      "  22. DataFrame generated for file input_pdf\\2024\\ns-22-2024.pdf: ns_22_2024_1\n",
      "  23. DataFrame generated for file input_pdf\\2024\\ns-23-2024.pdf: ns_23_2024_1\n",
      "  24. DataFrame generated for file input_pdf\\2024\\ns-24-2024.pdf: ns_24_2024_1\n",
      "  25. DataFrame generated for file input_pdf\\2024\\ns-25-2024.pdf: ns_25_2024_1\n",
      "  26. DataFrame generated for file input_pdf\\2024\\ns-26-2024.pdf: ns_26_2024_1\n",
      "  27. DataFrame generated for file input_pdf\\2024\\ns-27-2024.pdf: ns_27_2024_1\n",
      "  28. DataFrame generated for file input_pdf\\2024\\ns-28-2024.pdf: ns_28_2024_1\n",
      "  29. DataFrame generated for file input_pdf\\2024\\ns-29-2024.pdf: ns_29_2024_1\n",
      "  30. DataFrame generated for file input_pdf\\2024\\ns-30-2024.pdf: ns_30_2024_1\n",
      "  31. DataFrame generated for file input_pdf\\2024\\ns-31-2024.pdf: ns_31_2024_1\n",
      "  32. DataFrame generated for file input_pdf\\2024\\ns-32-2024.pdf: ns_32_2024_1\n",
      "  33. DataFrame generated for file input_pdf\\2024\\ns-33-2024.pdf: ns_33_2024_1\n",
      "  34. DataFrame generated for file input_pdf\\2024\\ns-34-2024.pdf: ns_34_2024_1\n",
      "  35. DataFrame generated for file input_pdf\\2024\\ns-35-2024.pdf: ns_35_2024_1\n",
      "  36. DataFrame generated for file input_pdf\\2024\\ns-36-2024.pdf: ns_36_2024_1\n",
      "  37. DataFrame generated for file input_pdf\\2024\\ns-37-2024.pdf: ns_37_2024_1\n",
      "  38. DataFrame generated for file input_pdf\\2024\\ns-38-2024.pdf: ns_38_2024_1\n",
      "  39. DataFrame generated for file input_pdf\\2024\\ns-39-2024.pdf: ns_39_2024_1\n",
      "  40. DataFrame generated for file input_pdf\\2024\\ns-40-2024.pdf: ns_40_2024_1\n",
      "  41. DataFrame generated for file input_pdf\\2024\\ns-41-2024.pdf: ns_41_2024_1\n",
      "  42. DataFrame generated for file input_pdf\\2024\\ns-42-2024.pdf: ns_42_2024_1\n",
      "  43. DataFrame generated for file input_pdf\\2024\\ns-43-2024.pdf: ns_43_2024_1\n",
      "Processing completed for all folders.\n"
     ]
    }
   ],
   "source": [
    "# Set the locale to Spanish\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Dictionary to store generated DataFrames\n",
    "new_dataframes_dict_1 = {}\n",
    "\n",
    "# Path for the processed folders log file\n",
    "record_path = 'dataframes_record_folder/new_processed_folders_1.txt'\n",
    "\n",
    "# Function to correct month names\n",
    "def correct_month_name(month):\n",
    "    months_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Add more mappings as needed for other month names\n",
    "    }\n",
    "    return months_mapping.get(month, month)\n",
    "\n",
    "# Function to register processed folder\n",
    "def register_processed_folder(folder, num_processed_files):\n",
    "    with open(record_path, 'a') as file:\n",
    "        file.write(f\"{folder}:{num_processed_files}\\n\")\n",
    "\n",
    "# Function to check if folder has been processed\n",
    "def folder_processed(folder):\n",
    "    if not os.path.exists(record_path):\n",
    "        return False\n",
    "    with open(record_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(folder):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Function to fetch date from database\n",
    "def get_date(df, engine):\n",
    "    id_ns = df['id_ns'].iloc[0]\n",
    "    year = df['year'].iloc[0]\n",
    "    query = f\"SELECT date FROM dates_growth_rates WHERE id_ns = '{id_ns}' AND year = '{year}';\"\n",
    "    date_result = pd.read_sql(query, engine)\n",
    "    return date_result.iloc[0, 0] if not date_result.empty else None\n",
    "\n",
    "# Function to process PDF file\n",
    "def process_pdf(pdf_path):\n",
    "    new_tables_dict_1 = {}  # Local dictionary for each PDF\n",
    "    table_counter = 1\n",
    "    keyword_count = 0 \n",
    "\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No matches found for id_ns and year in filename:\", filename)\n",
    "        return None, None, None, None, None  # Return None for new_tables_dict_1 as well\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(pdf_path))[0].replace('-', '_')\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            text = page.extract_text()\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                keyword_count += 1\n",
    "                if keyword_count == 1:  # Process only the first occurrence\n",
    "                    tables = tabula.read_pdf(pdf_path, pages=i, multiple_tables=False, stream=True) # Change stream to another option if desired\n",
    "                    for j, table_df in enumerate(tables, start=1):\n",
    "                        dataframe_name = f\"{new_filename}_{keyword_count}\"\n",
    "                        new_tables_dict_1[dataframe_name] = table_df\n",
    "                        table_counter += 1\n",
    "\n",
    "                    break  # Exit loop after finding the first occurrence\n",
    "\n",
    "    return id_ns, year, new_tables_dict_1, keyword_count\n",
    "\n",
    "# Function to process folder\n",
    "def process_folder(folder, engine):\n",
    "    print(f\"Processing folder {os.path.basename(folder)}\")\n",
    "    pdf_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.pdf')]\n",
    "\n",
    "    num_pdfs_processed = 0\n",
    "    num_dataframes_generated = 0\n",
    "\n",
    "    table_counter = 1  # Initialize table counter here\n",
    "    new_tables_dict_1 = {}  # Declare new_tables_dict_1 outside main loop\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        id_ns, year, tables_dict_temp, keyword_count = process_pdf(pdf_file)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for dataframe_name, df in tables_dict_temp.items():\n",
    "                file_name = os.path.splitext(os.path.basename(pdf_file))[0].replace('-', '_')\n",
    "                dataframe_name = f\"{file_name}_{keyword_count}\"\n",
    "                \n",
    "                # Store raw DataFrame in new_tables_dict_1\n",
    "                new_tables_dict_1[dataframe_name] = df.copy()\n",
    "                \n",
    "                # Apply cleaning functions to a copy of the DataFrame\n",
    "                df_clean = df.copy()\n",
    "\n",
    "                if any(col.isdigit() and len(col) == 4 for col in df_clean.columns):\n",
    "                    # If there is at least one column representing a year\n",
    "                    df_clean = swap_nan_se(df_clean)\n",
    "                    df_clean = split_column_by_pattern(df_clean)\n",
    "                    df_clean = drop_rare_caracter_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = relocate_last_columns(df_clean)\n",
    "                    df_clean = replace_first_dot(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = replace_var_perc_first_column(df_clean)\n",
    "                    df_clean = replace_var_perc_last_columns(df_clean)\n",
    "                    df_clean = replace_number_moving_average(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = exchange_values(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = find_year_column(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = convert_float(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = replace_mineria(df_clean)\n",
    "                    df_clean = replace_mining(df_clean)\n",
    "                    df_clean = rounding_values(df_clean, decimals=1)\n",
    "                else:\n",
    "                    # If there are no columns representing years\n",
    "                    df_clean = check_first_row(df_clean)\n",
    "                    df_clean = check_first_row_1(df_clean)\n",
    "                    df_clean = replace_first_row_with_columns(df_clean)\n",
    "                    df_clean = swap_nan_se(df_clean)\n",
    "                    df_clean = split_column_by_pattern(df_clean)\n",
    "                    df_clean = drop_rare_caracter_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = relocate_last_columns(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = replace_var_perc_first_column(df_clean)\n",
    "                    df_clean = replace_var_perc_last_columns(df_clean)\n",
    "                    df_clean = replace_number_moving_average(df_clean)\n",
    "                    df_clean = expand_column(df_clean)\n",
    "                    df_clean = split_values_1(df_clean)\n",
    "                    df_clean = split_values_2(df_clean)\n",
    "                    df_clean = split_values_3(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = exchange_values(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = find_year_column(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = get_months_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = convert_float(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column_1(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column_2(df_clean)\n",
    "                    df_clean = replace_nan_with_previous_column_3(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = replace_mineria(df_clean)\n",
    "                    df_clean = replace_mining(df_clean)\n",
    "                    df_clean = rounding_values(df_clean, decimals=1)\n",
    "                \n",
    "                # Add 'year' column to cleaned DataFrame\n",
    "                df_clean.insert(0, 'year', year)\n",
    "                \n",
    "                # Add 'id_ns' column to cleaned DataFrame\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                \n",
    "                # Get corresponding date from database\n",
    "                date = get_date(df_clean, engine)\n",
    "                if date:\n",
    "                    # Add 'date' column to cleaned DataFrame\n",
    "                    df_clean.insert(2, 'date', date)\n",
    "                else:\n",
    "                    print(\"Date not found in database for id_ns:\", id_ns, \"and year:\", year)\n",
    "                \n",
    "                # Store cleaned DataFrame in new_dataframes_dict_1\n",
    "                new_dataframes_dict_1[dataframe_name] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. DataFrame generated for file {pdf_file}: {dataframe_name}')\n",
    "                num_dataframes_generated += 1\n",
    "                table_counter += 1  # Increment table counter here\n",
    "        \n",
    "        num_pdfs_processed += 1  # Increment number of processed PDFs for each PDF in folder\n",
    "\n",
    "    return num_pdfs_processed, num_dataframes_generated, new_tables_dict_1\n",
    "\n",
    "# Function to process folders\n",
    "def process_folders():\n",
    "    pdf_folder = 'input_pdf'\n",
    "    folders = [os.path.join(pdf_folder, d) for d in os.listdir(pdf_folder) if os.path.isdir(os.path.join(pdf_folder, d))]\n",
    "    \n",
    "    new_tables_dict_1 = {}  # Initialize new_tables_dict_1 here\n",
    "    \n",
    "    for folder in folders:\n",
    "        if folder_processed(folder):\n",
    "            print(f\"Folder {folder} has already been processed.\")\n",
    "            continue\n",
    "        \n",
    "        num_pdfs_processed, num_dataframes_generated, tables_dict_temp = process_folder(folder, engine)\n",
    "        \n",
    "        # Update new_tables_dict_1 with values returned from process_folder()\n",
    "        new_tables_dict_1.update(tables_dict_temp)\n",
    "        \n",
    "        register_processed_folder(folder, num_pdfs_processed)\n",
    "\n",
    "        # Ask user if they want to continue with next folder\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Ensure the messagebox is in front\n",
    "        message = f\"Process {folder} complete. Processed {num_pdfs_processed} PDF(s) and generated {num_dataframes_generated} DataFrame(s). Continue with next folder?\"\n",
    "        if not messagebox.askyesno(\"Continue?\", message):\n",
    "            break\n",
    "            \n",
    "    print(\"Processing completed for all folders.\")  # Add a message to indicate completion\n",
    "    \n",
    "    return new_tables_dict_1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    engine = create_sqlalchemy_engine()\n",
    "    new_tables_dict_1 = process_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c63689",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0a6ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_tables_dict_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329629e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_dataframes_dict_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6cc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tables_dict_1['ns_43_2024_1'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ecc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = new_dataframes_dict_1['ns_43_2024_1']\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee02360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[(df_1['sectores_economicos'] == 'agropecuario') | (df_1['economic_sectors'] == 'agriculture and livestock')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0eb2f",
   "metadata": {},
   "source": [
    "<div id=\"3-2-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3f814",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">3.2.2.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    <span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">Table 2.</span> Extraction and cleaning of data from tables on quarterly and annual real GDP growth rates.\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419acde4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "The basic criterion to start extracting tables is to use keywords (sufficient condition). I mean, tables containing the following keywords meet the requirements to be extracted.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b455f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to search in the page text\n",
    "keywords = [\"ECONOMIC SECTORS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86af3dc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"font-size: 24px; color: rgb(255, 32, 78); font-weight: bold;\">&#9888;</span>\n",
    "    <span style=\"font-family: PT Serif Pro Book; color: black; font-size: 16px;\">\n",
    "        Please check that the flat file <b>\"ns_dates.csv\"</b> is updated with the dates, years and ids for the newly downloaded PDF <span style=\"font-size: 24px;\">&#128462;</span> (WR). That file is located in the <code>ns_dates</code> folder and is uploaded to SQL from the jupyeter notebook <code>aux_files_to_sql.ipynb</code>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9. DataFrame generated for file input_pdf\\2024\\ns-09-2024.pdf: ns_09_2024_2\n",
      "  10. DataFrame generated for file input_pdf\\2024\\ns-10-2024.pdf: ns_10_2024_2\n",
      "  11. DataFrame generated for file input_pdf\\2024\\ns-11-2024.pdf: ns_11_2024_2\n",
      "  12. DataFrame generated for file input_pdf\\2024\\ns-12-2024.pdf: ns_12_2024_2\n",
      "  13. DataFrame generated for file input_pdf\\2024\\ns-13-2024.pdf: ns_13_2024_2\n",
      "  14. DataFrame generated for file input_pdf\\2024\\ns-14-2024.pdf: ns_14_2024_2\n",
      "  15. DataFrame generated for file input_pdf\\2024\\ns-15-2024.pdf: ns_15_2024_2\n",
      "  16. DataFrame generated for file input_pdf\\2024\\ns-16-2024.pdf: ns_16_2024_2\n",
      "  17. DataFrame generated for file input_pdf\\2024\\ns-17-2024.pdf: ns_17_2024_2\n",
      "  18. DataFrame generated for file input_pdf\\2024\\ns-18-2024.pdf: ns_18_2024_2\n",
      "  19. DataFrame generated for file input_pdf\\2024\\ns-19-2024.pdf: ns_19_2024_2\n",
      "  20. DataFrame generated for file input_pdf\\2024\\ns-20-2024.pdf: ns_20_2024_2\n",
      "  21. DataFrame generated for file input_pdf\\2024\\ns-21-2024.pdf: ns_21_2024_2\n",
      "  22. DataFrame generated for file input_pdf\\2024\\ns-22-2024.pdf: ns_22_2024_2\n",
      "  23. DataFrame generated for file input_pdf\\2024\\ns-23-2024.pdf: ns_23_2024_2\n",
      "  24. DataFrame generated for file input_pdf\\2024\\ns-24-2024.pdf: ns_24_2024_2\n",
      "  25. DataFrame generated for file input_pdf\\2024\\ns-25-2024.pdf: ns_25_2024_2\n",
      "  26. DataFrame generated for file input_pdf\\2024\\ns-26-2024.pdf: ns_26_2024_2\n",
      "  27. DataFrame generated for file input_pdf\\2024\\ns-27-2024.pdf: ns_27_2024_2\n",
      "  28. DataFrame generated for file input_pdf\\2024\\ns-28-2024.pdf: ns_28_2024_2\n",
      "  29. DataFrame generated for file input_pdf\\2024\\ns-29-2024.pdf: ns_29_2024_2\n",
      "  30. DataFrame generated for file input_pdf\\2024\\ns-30-2024.pdf: ns_30_2024_2\n",
      "  31. DataFrame generated for file input_pdf\\2024\\ns-31-2024.pdf: ns_31_2024_2\n",
      "  32. DataFrame generated for file input_pdf\\2024\\ns-32-2024.pdf: ns_32_2024_2\n",
      "  33. DataFrame generated for file input_pdf\\2024\\ns-33-2024.pdf: ns_33_2024_2\n",
      "  34. DataFrame generated for file input_pdf\\2024\\ns-34-2024.pdf: ns_34_2024_2\n",
      "  35. DataFrame generated for file input_pdf\\2024\\ns-35-2024.pdf: ns_35_2024_2\n",
      "  36. DataFrame generated for file input_pdf\\2024\\ns-36-2024.pdf: ns_36_2024_2\n",
      "  37. DataFrame generated for file input_pdf\\2024\\ns-37-2024.pdf: ns_37_2024_2\n",
      "  38. DataFrame generated for file input_pdf\\2024\\ns-38-2024.pdf: ns_38_2024_2\n",
      "  39. DataFrame generated for file input_pdf\\2024\\ns-39-2024.pdf: ns_39_2024_2\n",
      "  40. DataFrame generated for file input_pdf\\2024\\ns-40-2024.pdf: ns_40_2024_2\n",
      "  41. DataFrame generated for file input_pdf\\2024\\ns-41-2024.pdf: ns_41_2024_2\n",
      "  42. DataFrame generated for file input_pdf\\2024\\ns-42-2024.pdf: ns_42_2024_2\n",
      "  43. DataFrame generated for file input_pdf\\2024\\ns-43-2024.pdf: ns_43_2024_2\n",
      "Processing completed for all folders.\n"
     ]
    }
   ],
   "source": [
    "# Set the locale to Spanish\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Dictionary to store generated DataFrames\n",
    "new_dataframes_dict_2 = {}\n",
    "\n",
    "# Path for the processed folders log file\n",
    "record_path = 'dataframes_record_folder/new_processed_folders_2.txt'\n",
    "\n",
    "# Function to correct month names\n",
    "def correct_month_name(month):\n",
    "    months_mapping = {\n",
    "        'setiembre': 'septiembre',\n",
    "        # Add more mappings as needed for other month names\n",
    "    }\n",
    "    return months_mapping.get(month, month)\n",
    "\n",
    "# Function to register processed folder\n",
    "def register_processed_folder(folder, num_processed_files):\n",
    "    with open(record_path, 'a') as file:\n",
    "        file.write(f\"{folder}:{num_processed_files}\\n\")\n",
    "        \n",
    "# Function to check if folder has been processed\n",
    "def folder_processed(folder):\n",
    "    if not os.path.exists(record_path):\n",
    "        return False\n",
    "    with open(record_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(folder):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Function to fetch date from database\n",
    "def get_date(df, engine):\n",
    "    id_ns = df['id_ns'].iloc[0]\n",
    "    year = df['year'].iloc[0]\n",
    "    query = f\"SELECT date FROM dates_growth_rates WHERE id_ns = '{id_ns}' AND year = '{year}';\"\n",
    "    date_result = pd.read_sql(query, engine)\n",
    "    return date_result.iloc[0, 0] if not date_result.empty else None\n",
    "\n",
    "# Function to process PDF file\n",
    "def process_pdf(pdf_path):\n",
    "    new_tables_dict_2 = {}  # Local dictionary for each PDF\n",
    "    table_counter = 1\n",
    "    keyword_count = 0 \n",
    "\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No matches found for id_ns and year in filename:\", filename)\n",
    "        return None, None, None, None\n",
    "\n",
    "    new_filename = os.path.splitext(os.path.basename(pdf_path))[0].replace('-', '_')\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            text = page.extract_text()\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                keyword_count += 1\n",
    "                if keyword_count == 2:\n",
    "                    tables = tabula.read_pdf(pdf_path, pages=i, multiple_tables=False)\n",
    "                    for j, table_df in enumerate(tables, start=1):\n",
    "                        dataframe_name = f\"{new_filename}_{keyword_count}\"\n",
    "                        new_tables_dict_2[dataframe_name] = table_df\n",
    "                        table_counter += 1\n",
    "\n",
    "    return id_ns, year, new_tables_dict_2, keyword_count\n",
    "\n",
    "\n",
    "def process_folder(folder, engine):\n",
    "    print(f\"Processing folder {os.path.basename(folder)}\")\n",
    "    pdf_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.pdf')]\n",
    "\n",
    "    num_pdfs_processed = 0\n",
    "    num_dataframes_generated = 0\n",
    "\n",
    "    table_counter = 1  # Initialize table counter here\n",
    "    new_tables_dict_2 = {}  # Declare tables_dict outside main loop\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        id_ns, year, tables_dict_temp, keyword_count = process_pdf(pdf_file)\n",
    "\n",
    "        if tables_dict_temp:\n",
    "            for dataframe_name, df in tables_dict_temp.items():\n",
    "                file_name = os.path.splitext(os.path.basename(pdf_file))[0].replace('-', '_')\n",
    "                dataframe_name = f\"{file_name}_{keyword_count}\"\n",
    "\n",
    "                # Store raw DataFrame in tables_dict\n",
    "                new_tables_dict_2[dataframe_name] = df.copy()\n",
    "\n",
    "                # Apply 20 lines of cleaning functions to a copy of the DataFrame\n",
    "                df_clean = df.copy()\n",
    "                if df_clean.iloc[0, 0] is np.nan:\n",
    "                    # Apply 20 lines of cleaning\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = separate_years(df_clean)\n",
    "                    df_clean = relocate_roman_numerals(df_clean)\n",
    "                    df_clean = extract_mixed_values(df_clean)\n",
    "                    df_clean = replace_first_row_nan(df_clean)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = drop_nan_row(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = split_values(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = roman_arabic(df_clean)\n",
    "                    df_clean = fix_duplicates(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = convert_float(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = replace_mineria(df_clean)\n",
    "                    df_clean = replace_mining(df_clean)\n",
    "                    df_clean = rounding_values(df_clean, decimals=1)\n",
    "                else:\n",
    "                    # Apply 15 lines of cleaning\n",
    "                    df_clean = exchange_roman_nan(df_clean)\n",
    "                    df_clean = exchange_columns(df_clean)\n",
    "                    df_clean = drop_nan_columns(df_clean)\n",
    "                    df_clean = remove_digit_slash(df_clean)\n",
    "                    df_clean = last_column_es(df_clean)\n",
    "                    df_clean = swap_first_second_row(df_clean)\n",
    "                    df_clean = drop_nan_rows(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    year_columns = extract_years(df_clean)\n",
    "                    df_clean = separate_text_digits(df_clean)\n",
    "                    df_clean = roman_arabic(df_clean)\n",
    "                    df_clean = fix_duplicates(df_clean)\n",
    "                    df_clean = relocate_last_column(df_clean)\n",
    "                    df_clean = clean_first_row(df_clean)\n",
    "                    df_clean = get_quarters_sublist_list(df_clean, year_columns)\n",
    "                    df_clean = first_row_columns(df_clean)\n",
    "                    df_clean = clean_columns_values(df_clean)\n",
    "                    df_clean = reset_index(df_clean)\n",
    "                    df_clean = convert_float(df_clean)\n",
    "                    df_clean = replace_set_sep(df_clean)\n",
    "                    df_clean = spaces_se_es(df_clean)\n",
    "                    df_clean = replace_services(df_clean)\n",
    "                    df_clean = replace_mineria(df_clean)\n",
    "                    df_clean = replace_mining(df_clean)\n",
    "                    df_clean = rounding_values(df_clean, decimals=1)\n",
    "\n",
    "                # Add 'year' column to cleaned DataFrame\n",
    "                df_clean.insert(0, 'year', year)\n",
    "                \n",
    "                # Add 'id_ns' column to cleaned DataFrame\n",
    "                df_clean.insert(1, 'id_ns', id_ns)\n",
    "                \n",
    "                # Get corresponding date from database\n",
    "                date = get_date(df_clean, engine)\n",
    "                if date:\n",
    "                    # Add 'date' column to cleaned DataFrame\n",
    "                    df_clean.insert(2, 'date', date)\n",
    "                else:\n",
    "                    print(\"Date not found in database for id_ns:\", id_ns, \"and year:\", year)\n",
    "\n",
    "                # Store cleaned DataFrame in new_dataframes_dict\n",
    "                new_dataframes_dict_2[dataframe_name] = df_clean\n",
    "\n",
    "                print(f'  {table_counter}. DataFrame generated for file {pdf_file}: {dataframe_name}')\n",
    "                num_dataframes_generated += 1\n",
    "                table_counter += 1  # Increment table counter here\n",
    "                    \n",
    "        num_pdfs_processed += 1  # Increment number of PDFs processed for each PDF in folder\n",
    "\n",
    "    return num_pdfs_processed, num_dataframes_generated, new_tables_dict_2\n",
    "        \n",
    "def process_folders():\n",
    "    pdf_folder = 'input_pdf'\n",
    "    folders = [os.path.join(pdf_folder, d) for d in os.listdir(pdf_folder) if os.path.isdir(os.path.join(pdf_folder, d))]\n",
    "\n",
    "    new_tables_dict_2 = {}  # Initialize tables_dict here\n",
    "    \n",
    "    for folder in folders:\n",
    "        if folder_processed(folder):\n",
    "            print(f\"Folder {folder} has already been processed.\")\n",
    "            continue\n",
    "        \n",
    "        num_pdfs_processed, num_dataframes_generated, tables_dict_temp = process_folder(folder, engine)\n",
    "        \n",
    "        # Update tables_dict with values returned from process_folder()\n",
    "        new_tables_dict_2.update(tables_dict_temp)\n",
    "        \n",
    "        register_processed_folder(folder, num_pdfs_processed)\n",
    "        \n",
    "        # Ask user if they want to continue with next folder\n",
    "        root = Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)  # Ensure the messagebox is in front\n",
    "        message = f\"Process {folder} complete. Processed {num_pdfs_processed} PDF(s) and generated {num_dataframes_generated} dataframes. Continue with next folder?\"\n",
    "        if not messagebox.askyesno(\"Continue?\", message):\n",
    "            break\n",
    "            \n",
    "    print(\"Processing completed for all folders.\")  # Add a message to indicate completion\n",
    "\n",
    "    return new_tables_dict_2  # Return tables_dict at the end of the function\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    engine = create_sqlalchemy_engine() # Creates the SQL connection to merge the date, year and id from a SQL database to dataframes\n",
    "    new_tables_dict_2 = process_folders()  # Capture the returned value from process_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77706358",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272f429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_tables_dict_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a46bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_dataframes_dict_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tables_dict_2['ns_43_2024_2'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframes_dict_2['ns_43_2024_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d97b8",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217183e7",
   "metadata": {},
   "source": [
    "<div id=\"4\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8989d65",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Real-time data of Peru's GDP growth rates</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a04abd",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "This section creates the GDP growth rate vintages for Peru using <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 1</a> and <a href=\"#3-2-1\" style=\"color: rgb(0, 153, 123); font-size: 16px;\">Table 2</a>, which were extracted and cleaned in the previous section. Each table from each WR (PDF <span style=\"font-size: 24px;\">&#128462;</span>) was extracted and cleaned individually in the previous section. Here, we will concatenate all the tables for a specific economic sector, thus creating a vintage dataset of (real) GDP growth by economic sector from <b>2013</b> to <b>2024</b>.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2971c7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    <span style=\"font-size: 24px; color: #FFA823; font-weight: bold;\">&#9888;</span>\n",
    "As preferred or as appropriate, you can create the data manually, step by step, or focus on specific sectors or frequencies. Alternatively, you can choose a more efficient or automated approach by generating the data for all sectors and frequencies simultaneously.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed3953",
   "metadata": {},
   "source": [
    "<div id=\"4-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ee7f3",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Manual process of data creation in real time: sector by sector and frequency by frequency.\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef0a33",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    With this method you can create and inspect the dataset sector by sector and frequency by frequency. This is useful if you want to create data only for particular sectors and frequencies.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9ebdf",
   "metadata": {},
   "source": [
    "<div id=\"select_sector\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ec604",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #00414C; color: white; padding: 10px;\">\n",
    "<h1><span style = \"color: #15F5BA; font-family: 'PT Serif Pro Book'; color: dark;\">$\\bullet$</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Select <code>sector_economico</code> and <code>economic_sector</code></span></h1>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb3ca8",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color:dark; font-size:16px\">\n",
    "<p>     \n",
    "When executing the following code, a window will be displayed with options in <b>Spanish</b> and <b>English</b> to select <b>economic sectors</b>. Choose them to concatenate Peru GDP growth rates (annual, quarterly or monthly) by sector.\n",
    "</p>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to display the window and capture the selected values\n",
    "selected_spanish, selected_english, sector = show_option_window()\n",
    "\n",
    "# Display the selected values\n",
    "print(f\"You have selected sector = {sector}, selected_spanish = {selected_spanish}, and selected_english = {selected_english}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d69443",
   "metadata": {},
   "source": [
    "<div id=\"select_freq\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33746e78",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #00414C; color: white; padding: 10px;\">\n",
    "<h1><span style = \"color: #15F5BA; font-family: 'PT Serif Pro Book'; color: dark;\">$\\bullet$</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Select <code>frequency</code></span></h1>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6652f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to show the popup window\n",
    "frequency = show_frequency_window()\n",
    "print(\"Selected frequency:\", frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4800d",
   "metadata": {},
   "source": [
    "<div id=\"counter\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f78b39d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #00414C; color: white; padding: 10px;\">\n",
    "<h1><span style = \"color: #15F5BA; font-family: 'PT Serif Pro Book'; color: dark;\">$\\bullet$</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Set counter (dataframe name suffix)</span></h1>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to set the counter\n",
    "if frequency == \"monthly\":\n",
    "    counter = 1\n",
    "elif frequency == \"quarterly\":\n",
    "    counter = 2\n",
    "elif frequency == \"annual\":\n",
    "    counter = 2\n",
    "else:\n",
    "    counter = None \n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6ca98",
   "metadata": {},
   "source": [
    "<div id=\"4-1-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b142e",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.1.1.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Growth rates datasets concatenation for all frequencies\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically construct the function name and dictionary name\n",
    "function_name = f\"concatenate_{frequency}_df\"\n",
    "dataframe_dict_name = f\"new_dataframes_dict_{counter}\"\n",
    "\n",
    "# Check that both the function and dictionary exist in the global scope\n",
    "if function_name in globals() and dataframe_dict_name in globals():\n",
    "    # Call the function using its reference from globals()\n",
    "    globals()[f\"new_{sector}_{frequency}_growth_rates\"] = globals()[function_name](\n",
    "        globals()[dataframe_dict_name], selected_spanish, selected_english\n",
    "    )\n",
    "else:\n",
    "    print(f\"Error: {function_name} or {dataframe_dict_name} does not exist in the global scope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "globals()[f\"new_{sector}_{frequency}_growth_rates\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4f057",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f60c74",
   "metadata": {},
   "source": [
    "<div id=\"4-1-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2418a10",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.1.2.</span> <span style = \"color: dark; font-family: PT Serif Pro Book;\">Uploading data to SQL</span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c80fc4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "Finally, we upload all the datasets generated in this jupyter notebook to the <code>'gdp_revisions_datasets'</code> database of <code>PostgresSQL</code>.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_sqlalchemy_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be882f24",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "Loading\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623149b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()[f\"new_{sector}_{frequency}_growth_rates\"].to_sql(f'new_{sector}_{frequency}_growth_rates', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60157eab",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 20px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#select_sector\" style=\"color: rgb(255, 32, 78); text-decoration: none;\">â®</a>\n",
    "    </span> \n",
    "    <a href=\"#select_sector\" style=\"color: rgb(255, 32, 78); text-decoration: none;\">Back to select sectors.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6d115",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 20px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#select_freq\" style=\"color: rgb(255, 32, 78); text-decoration: none;\">â®</a>\n",
    "    </span> \n",
    "    <a href=\"#select_freq\" style=\"color: rgb(255, 32, 78); text-decoration: none;\">Back to select frequency.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a58528",
   "metadata": {},
   "source": [
    "<div id=\"4-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cac195",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: PT Serif Pro Book;\">4.2.</span>\n",
    "    <span style = \"color: dark; font-family: PT Serif Pro Book;\">\n",
    "    Automatic data creation process in real time: all sectors and frequencies at the same time.\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d138910",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    With this method you can create the dataset for all sectors and all frequencies at the same time. This is more efficient if the goal is to generate all possible combinations of datasets for <code>sector</code> and <code>frequency</code> (without excluding any sector or frequency).\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b2bfe",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    List of frequencies to be used to create concatenated datasets\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = [\n",
    "        \"monthly\", \n",
    "        \"quarterly\",\n",
    "        \"annual\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbac57",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    Function to process growth rates datasets: concatenate and load to SQL\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_datasets_to_sql(sector, frequency):\n",
    "    # Initialize counter for loaded DataFrames\n",
    "    total_loaded = 0\n",
    "\n",
    "    # Set counter based on frequency\n",
    "    if frequency == \"monthly\":\n",
    "        counter = 1\n",
    "    elif frequency in [\"quarterly\", \"annual\"]:\n",
    "        counter = 2\n",
    "    else:\n",
    "        print(f\"Unknown frequency: {frequency}\")\n",
    "        return None\n",
    "\n",
    "    # Dynamically build function and dictionary names\n",
    "    function_name = f\"concatenate_{frequency}_df\"\n",
    "    dataframe_dict_name = f\"new_dataframes_dict_{counter}\"\n",
    "\n",
    "    if function_name in globals() and dataframe_dict_name in globals():\n",
    "        # Generate the DataFrame\n",
    "        df_name = f\"new_{sector}_{frequency}_growth_rates\"\n",
    "        globals()[df_name] = globals()[function_name](\n",
    "            globals()[dataframe_dict_name], option_mapping[sector][0], option_mapping[sector][1]\n",
    "        )\n",
    "\n",
    "        # Load to SQL\n",
    "        engine = create_sqlalchemy_engine()\n",
    "        globals()[df_name].to_sql(df_name, engine, index=False, if_exists='replace')\n",
    "\n",
    "        return globals()[df_name]\n",
    "    else:\n",
    "        print(f\"Error: {function_name} or {dataframe_dict_name} does not exist in the global scope.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20687226",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'PT Serif Pro Book'; color: dark; font-size:16px\">\n",
    "    Run the function to create concatenated datasets for all sectors and frequencies and load to SQL\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b0581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counter\n",
    "processed_datasets = 0\n",
    "\n",
    "# Process all combinations\n",
    "for sector in option_mapping.keys():\n",
    "    for frequency in frequencies:\n",
    "        print(f\"Processing {sector} - {frequency}\")\n",
    "        df = process_new_datasets_to_sql(sector, frequency)\n",
    "        if df is not None:\n",
    "            display(df.head(10))  # Display the first 10 rows\n",
    "            processed_datasets += 1  # Increment counter\n",
    "\n",
    "# Display total number of processed datasets\n",
    "print(f\"Total datasets processed: {processed_datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd18b4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: rgb(255, 32, 78); font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: rgb(0, 153, 123); text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448c020",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 16px; background-color: #F5F5F5; padding: 18px; line-height: 1.5; font-family: 'PT Serif Pro Book';\">\n",
    "    <span style=\"font-size: 24px; color: #FFA823; font-weight: bold;\">&#9888;</span>\n",
    "    Once you have all the datasets generated by this script (<code>new_gdp_datasets.ipynb</code>) you can concatenate with those generated in the script <code>old_gdp_datasets.ipynb</code>. <b>Section 6</b> of the script <code>aux_files_to_sql.ipynb</code> concatenates both <b>new</b> and <b>old</b> datasets for <b>all sectors</b> and <b>all frequencies</b>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc920fd",
   "metadata": {},
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eae11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb22e11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdp_revisions",
   "language": "python",
   "name": "gdp_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
