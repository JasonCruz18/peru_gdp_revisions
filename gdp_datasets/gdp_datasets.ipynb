{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd9a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a055b3b9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter'; color: rgb(0, 65, 75);\">\n",
    "    <h1>\n",
    "    GDP Revisions Datasets\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd88a4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter'; color: rgb(0, 65, 75);\">\n",
    "    <h4>\n",
    "        Documentation\n",
    "        <br>\n",
    "        ____________________\n",
    "            </br>\n",
    "    </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72f519",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color: dark;\">\n",
    "    This \n",
    "    <span style=\"color: rgb(61, 48, 162);\">jupyter notebook</span>\n",
    "    provides a step-by-step guide to <b>data building</b> regarding the project <b>'Revisiones y sesgos en las estimaciones preliminares del PBI en el Per√∫'</b>. The guide covers downloading PDF files containing tables with information on annual, quarterly, and monthly Peru's GDP growth rates (including sectoral GDP) and extracting this information into SQL tables. These data sets will be used for data analysis.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d22837",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'charter'; color: rgb(0, 65, 75);\">\n",
    "    Jason Cruz\n",
    "    <br>\n",
    "    <a href=\"mailto:jj.cruza@up.edu.pe\" style=\"color: rgb(0, 153, 123)\">\n",
    "        jj.cruza@up.edu.pe\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28540df2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Times New Roman; text-align: left; color: rgb(61, 48, 162)\">The provided outline is functional. Use the buttons to enhance the experience of this script.<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc04c9",
   "metadata": {},
   "source": [
    "<div id=\"outilne\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0fa13",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #141414; padding: 10px;\">\n",
    "<h2 style=\"text-align: left; font-family: 'charter'; color: #E0E0E0;\">\n",
    "    Outline\n",
    "    </h2>\n",
    "    <br>\n",
    "    <a href=\"#1\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        1. PDF Downloader</a>\n",
    "    <br>\n",
    "    <a href=\"#2\" style=\"color: #687EFF; font-size: 18px;\">\n",
    "        2. Extracting Tables (and data cleaning)</a>\n",
    "    <br>\n",
    "    <a href=\"#2-1\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        2.1. 'pdfplumber' demo.</a>\n",
    "    <br>\n",
    "    <a href=\"#2-1-1\" style=\"color: #E0E0E0; font-size: 12px;\">\n",
    "        2.1.1. What data would we get if we used the default settings?.</a>   \n",
    "    <br>\n",
    "    <a href=\"#2-1-2\" style=\"color: #E0E0E0; font-size: 12px;\">\n",
    "        2.1.2. Using custom '.extract_table' settings.</a>\n",
    "    <br> \n",
    "    <a href=\"#2-2\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        2.2. Extracting tables and generating dataframes (includes data cleanup).</a>\n",
    "    <br>\n",
    "    <a href=\"#3\" style=\"color: #687EFF; font-size: 18px;\">3. SQL Tables</a>\n",
    "    <br>\n",
    "    <a href=\"#3-1\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.1. Annual Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-2\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.2. Quarterly Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-3\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.3. Monthly Concatenation.</a>\n",
    "    <br>\n",
    "    <a href=\"#3-4\" style=\"color: rgb(0, 153, 123); font-size: 12px;\">\n",
    "        3.4. Loading SQL.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90323106",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    Any questions or issues regarding the coding, please <a href=\"mailto:jj.cruza@alum.up.edu.pe\" style=\"color: rgb(0, 153, 123)\">email Jason Cruz\n",
    "    </a>.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac40ed0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    If you don't have the libraries below, please use the following code (as example) to install the required libraries.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c73193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907046a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    <h2>\n",
    "    Libraries\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214f5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Downloader\n",
    "\n",
    "import os  # for file and directory manipulation\n",
    "import random  # to generate random numbers\n",
    "import time  # to manage time and take breaks in the script\n",
    "import requests  # to make HTTP requests to web servers\n",
    "from selenium import webdriver  # for automating web browsers\n",
    "from selenium.webdriver.common.by import By  # to locate elements on a webpage\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # to wait until certain conditions are met on a webpage.\n",
    "from selenium.webdriver.support import expected_conditions as EC  # to define expected conditions\n",
    "from selenium.common.exceptions import StaleElementReferenceException  # To handle exceptions related to elements on the webpage that are no longer available.\n",
    "\n",
    "\n",
    "# Extracting Tables (and data cleaning)\n",
    "\n",
    "import pdfplumber  # for extracting text and metadata from PDF files\n",
    "import pandas as pd  # for data manipulation and analysis\n",
    "import os  # for interacting with the operating system\n",
    "import unicodedata  # for manipulating Unicode data\n",
    "import re  # for regular expressions operations\n",
    "from datetime import datetime  # for working with dates and times\n",
    "import locale  # for locale-specific formatting of numbers, dates, and currencies\n",
    "\n",
    "\n",
    "# SQL tables\n",
    "\n",
    "import psycopg2  # for interacting with PostgreSQL databases\n",
    "from sqlalchemy import create_engine, text  # for creating and executing SQL queries using SQLAlchemy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ef8f8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'charter'; color: dark;\">\n",
    "    <h2>\n",
    "    Initial set-up\n",
    "    </h2>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc1c80",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\"> The next 3 code lines will create folders in your current path, call them to import and export your outputs. <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8899a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to download PDF files\n",
    "\n",
    "raw_pdf = 'raw_pdf' # to save raw data (.pdf).\n",
    "if not os.path.exists(raw_pdf):\n",
    "    os.mkdir(raw_pdf) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26b4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path to save text file with the names of already downloaded files\n",
    "\n",
    "download_record = 'download_record'\n",
    "if not os.path.exists(download_record):\n",
    "    os.mkdir(download_record) # to create the folder (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7cf410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba6e051c",
   "metadata": {},
   "source": [
    "<div id=\"1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f3192",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: charter;\">1.</span> <span style = \"color: dark; font-family: charter;\">PDF Downloader</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98250a64",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Our main source for data collection is the <a href=\"https://www.bcrp.gob.pe/\" style=\"color: rgb(0, 153, 123)\">BCRP's web page</a> (.../publicaciones/nota-semanal). The BCRP publishes \"Notas Semanales\", documents that contain, among other information, tables of GDP and sectoral GDP growth rate values for annual, quarterly and monthly frequencies.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3a388",
   "metadata": {},
   "source": [
    "-- (pending) Selenium tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcc4e3",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    The provided code will download all the 'Notas Semanales' files in PDF format from this web page.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e812cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the BCRP URL\n",
    "bcrp_url = \"https://www.bcrp.gob.pe/publicaciones/nota-semanal.html\"  # Never replace this URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269c03d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    The provided code will download all the 'Notas Semanales' files in PDF format from this web page.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e685a8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site opened successfully\n",
      "1. The file ns-01-2013.pdf has already been downloaded previously. Skipping...\n",
      "2. The file ns-02-2013.pdf has already been downloaded previously. Skipping...\n",
      "3. The file ns-03-2013.pdf has already been downloaded previously. Skipping...\n",
      "4. The file ns-04-2013.pdf has already been downloaded previously. Skipping...\n",
      "5. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-05-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-05-2013.pdf\n",
      "Batch 1 of 5 completed\n",
      "Waiting randomly for 8.94 seconds\n",
      "6. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-06-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-06-2013.pdf\n",
      "Waiting randomly for 6.90 seconds\n",
      "7. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-07-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-07-2013.pdf\n",
      "Waiting randomly for 9.94 seconds\n",
      "8. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-08-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-08-2013.pdf\n",
      "Waiting randomly for 9.13 seconds\n",
      "9. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-09-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-09-2013.pdf\n",
      "Waiting randomly for 6.86 seconds\n",
      "10. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-10-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-10-2013.pdf\n",
      "Batch 2 of 5 completed\n",
      "Waiting randomly for 5.06 seconds\n",
      "11. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-11-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-11-2013.pdf\n",
      "Waiting randomly for 8.77 seconds\n",
      "12. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-12-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-12-2013.pdf\n",
      "Waiting randomly for 6.66 seconds\n",
      "13. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-13-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-13-2013.pdf\n",
      "Waiting randomly for 7.22 seconds\n",
      "14. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-14-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-14-2013.pdf\n",
      "Waiting randomly for 8.98 seconds\n",
      "15. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-15-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-15-2013.pdf\n",
      "Batch 3 of 5 completed\n",
      "Waiting randomly for 6.77 seconds\n",
      "16. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-16-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-16-2013.pdf\n",
      "Waiting randomly for 9.07 seconds\n",
      "17. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-17-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-17-2013.pdf\n",
      "Waiting randomly for 5.23 seconds\n",
      "18. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-18-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-18-2013.pdf\n",
      "Waiting randomly for 5.78 seconds\n",
      "19. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-19-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-19-2013.pdf\n",
      "Waiting randomly for 6.60 seconds\n",
      "20. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-20-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-20-2013.pdf\n",
      "Batch 4 of 5 completed\n",
      "Waiting randomly for 9.82 seconds\n",
      "21. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-21-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-21-2013.pdf\n",
      "Waiting randomly for 9.72 seconds\n",
      "22. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-22-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-22-2013.pdf\n",
      "Waiting randomly for 8.72 seconds\n",
      "23. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-23-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-23-2013.pdf\n",
      "Waiting randomly for 7.70 seconds\n",
      "24. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-24-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-24-2013.pdf\n",
      "Waiting randomly for 9.04 seconds\n",
      "25. New URL: https://www.bcrp.gob.pe/docs/Publicaciones/Nota-Semanal/2013/ns-25-2013.pdf\n",
      "PDF downloaded successfully at: raw_pdf\\ns-25-2013.pdf\n",
      "Batch 5 of 5 completed\n",
      "Waiting randomly for 9.91 seconds\n",
      "All downloads completed (25 in total)\n"
     ]
    }
   ],
   "source": [
    "# List to keep track of successfully downloaded files\n",
    "downloaded_files = []\n",
    "\n",
    "# Folder where downloaded PDF files will be saved\n",
    "raw_pdf = \"raw_pdf\"  # Replace with the actual path\n",
    "\n",
    "# Folder where the download record file will be saved\n",
    "download_record = \"download_record\"  # Replace with the actual path\n",
    "\n",
    "# Load the list of previously downloaded files if it exists\n",
    "if os.path.exists(os.path.join(download_record, \"downloaded_files.txt\")):\n",
    "    with open(os.path.join(download_record, \"downloaded_files.txt\"), \"r\") as f:\n",
    "        downloaded_files = f.read().splitlines()\n",
    "\n",
    "# Web driver setup\n",
    "driver_path = os.environ.get('driver_path')\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "def random_wait(min_time, max_time):\n",
    "    wait_time = random.uniform(min_time, max_time)\n",
    "    print(f\"Waiting randomly for {wait_time:.2f} seconds\")\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "def download_pdf(pdf_link):\n",
    "    # Click the link using JavaScript\n",
    "    driver.execute_script(\"arguments[0].click();\", pdf_link)\n",
    "\n",
    "    # Wait for the new page to fully open (adjust timing as necessary)\n",
    "    wait.until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "    # Switch to the new window or tab\n",
    "    windows = driver.window_handles\n",
    "    driver.switch_to.window(windows[1])\n",
    "\n",
    "    # Get the current URL (may vary based on site-specific logic)\n",
    "    new_url = driver.current_url\n",
    "    print(f\"{download_counter}. New URL: {new_url}\")\n",
    "\n",
    "    # Get the file name from the URL\n",
    "    file_name = new_url.split(\"/\")[-1]\n",
    "\n",
    "    # Form the full destination path\n",
    "    destination_path = os.path.join(raw_pdf, file_name)\n",
    "\n",
    "    # Download the PDF\n",
    "    response = requests.get(new_url, stream=True)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Save the PDF content to the local file\n",
    "        with open(destination_path, 'wb') as pdf_file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                pdf_file.write(chunk)\n",
    "\n",
    "        print(f\"PDF downloaded successfully at: {destination_path}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error downloading the PDF. Response code: {response.status_code}\")\n",
    "\n",
    "    # Close the new window or tab\n",
    "    driver.close()\n",
    "\n",
    "    # Switch back to the main window\n",
    "    driver.switch_to.window(windows[0])\n",
    "\n",
    "# Number of downloads per batch\n",
    "downloads_per_batch = 5\n",
    "# Total number of downloads\n",
    "total_downloads = 25\n",
    "\n",
    "try:\n",
    "    # Open the test page\n",
    "    driver.get(bcrp_url)\n",
    "    print(\"Site opened successfully\")\n",
    "\n",
    "    # Wait for the container area to be present\n",
    "    wait = WebDriverWait(driver, 60)\n",
    "    container_area = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"rightside\"]')))\n",
    "\n",
    "    # Get all the links within the container area\n",
    "    pdf_links = container_area.find_elements(By.XPATH, './/a')\n",
    "\n",
    "    # Reverse the order of links\n",
    "    pdf_links = list(reversed(pdf_links))\n",
    "\n",
    "    # Initialize download counter\n",
    "    download_counter = 0\n",
    "\n",
    "    # Iterate over reversed links and download PDFs in batches\n",
    "    for pdf_link in pdf_links:\n",
    "        download_counter += 1\n",
    "\n",
    "        # Get the file name from the URL\n",
    "        new_url = pdf_link.get_attribute(\"href\")\n",
    "        file_name = new_url.split(\"/\")[-1]\n",
    "\n",
    "        # Check if the file has already been downloaded\n",
    "        if file_name in downloaded_files:\n",
    "            print(f\"{download_counter}. The file {file_name} has already been downloaded previously. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Try to download the file\n",
    "        try:\n",
    "            download_pdf(pdf_link)\n",
    "\n",
    "            # Update the list of downloaded files\n",
    "            downloaded_files.append(file_name)\n",
    "\n",
    "            # Save the file name in the record\n",
    "            with open(os.path.join(download_record, \"downloaded_files.txt\"), \"a\") as f:\n",
    "                f.write(file_name + \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading the file {file_name}: {str(e)}\")\n",
    "\n",
    "        # If the download count reaches a multiple of batch size, notify\n",
    "        if download_counter % downloads_per_batch == 0:\n",
    "            print(f\"Batch {download_counter // downloads_per_batch} of {downloads_per_batch} completed\")\n",
    "\n",
    "        # Random wait before the next iteration\n",
    "        random_wait(5, 10)\n",
    "\n",
    "        # If total downloads reached, break out of loop\n",
    "        if download_counter == total_downloads:\n",
    "            print(f\"All downloads completed ({total_downloads} in total)\")\n",
    "            break\n",
    "\n",
    "except StaleElementReferenceException:\n",
    "    print(\"StaleElementReferenceException occurred. Retrying...\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser when finished\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045beb7b",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a384d2",
   "metadata": {},
   "source": [
    "<div id=\"2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e1e92",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.</span> <span style = \"color: dark; font-family: charter;\">Extracting Tables (and data cleaning)</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ed1db",
   "metadata": {},
   "source": [
    "<div id=\"2-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af999f",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.1.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        pdfplumber\n",
    "    </span> \n",
    "    demo\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebeb3a",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Import\n",
    "    <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        pdfplumber\n",
    "    </span>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42e7dac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This library version is: 0.10.4\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "print(f'This library version is: {pdfplumber.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b481d4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Load the PDF\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84bb31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(\".\\\\ns-10-2013.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad376ace",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Get the page 82\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_82 = pdf.pages[81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33facebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the page to a higher resolution image (e.g., 300 DPI).\n",
    "image = p_82.to_image(resolution=300)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629ee4d",
   "metadata": {},
   "source": [
    "<div id=\"2-1-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49929c",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.1.1.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    What data would we get if we used the default settings?\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ea8f5e",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    We can check by using <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        PageImage.debug_tablefinder()\n",
    "    </span>:\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd775601",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.reset().debug_tablefinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ae18d",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    The default settings correctly identify the table's vertical demarcations, but don't capture the horizontal demarcations between each group of five states/territories. So:\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b70f0",
   "metadata": {},
   "source": [
    "<div id=\"2-1-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8947130a",
   "metadata": {},
   "source": [
    "<h3><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.1.2.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Using custom <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        <b>.extract_table\n",
    "            </b>\n",
    "    </span>'s settings\n",
    "    </span>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c473c71b",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    <ul>\n",
    "        <li>Because the columns are separated by lines, we use <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        vertical_strategy=\"lines\"\n",
    "    </span>.\n",
    "            </li>\n",
    "        <li>Because the rows are, primarily, separated by gutters between the text, we use <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        horizontal_strategy=\"text\"\n",
    "    </span>.\n",
    "            <li>To snap together a handful of the gutters at the top which aren't fully flush with one another, we use <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        snap_y_tolerance\n",
    "    </span>which snaps horizontal lines within a certain distance to the same vertical alignment.\n",
    "                </li>\n",
    "        <li>And because the left and right-hand extremities of the text aren't quite flush with the vertical lines, we use <span style=\"background-color: #f2f2f2; font-family: Courier New;\">\n",
    "        \"intersection_tolerance\": 15\n",
    "    </span>.\n",
    "            </li>\n",
    "        </ul>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_settings = {\n",
    "    \"vertical_strategy\": \"lines\", \n",
    "    \"horizontal_strategy\": \"lines\",\n",
    "    \"explicit_vertical_lines\": [],\n",
    "    \"explicit_horizontal_lines\": [],\n",
    "    \"snap_tolerance\": 3,\n",
    "    \"snap_x_tolerance\": 3,\n",
    "    \"snap_y_tolerance\": 3,\n",
    "    \"join_tolerance\": 3,\n",
    "    \"join_x_tolerance\": 3,\n",
    "    \"join_y_tolerance\": 3,\n",
    "    \"edge_min_length\": 3,\n",
    "    \"min_words_vertical\": 3,\n",
    "    \"min_words_horizontal\": 1,\n",
    "    \"text_keep_blank_chars\": False,\n",
    "    \"text_tolerance\": 3,\n",
    "    \"text_x_tolerance\": 3,\n",
    "    \"text_y_tolerance\": 3,\n",
    "    \"intersection_tolerance\": 3,\n",
    "    \"intersection_x_tolerance\": 3,\n",
    "    \"intersection_y_tolerance\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c25c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.reset().debug_tablefinder(table_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2093bf",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5a767",
   "metadata": {},
   "source": [
    "<div id=\"2-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357318e8",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">2.2.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Extracting tables and generating dataframes (includes data cleanup)\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100857d4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    We would like to get specific tables: information on GDP growth rates with annual, quarterly and monthly frequency. We don't need other tables also related to GDP that don't meet these requirements. Extraction will be easier if we use keywords.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77729e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to search in the page text\n",
    "keywords = [\"PRODUCTO BRUTO INTERNO\", \"SECTORES ECON√ìMICOS\", \"PBI\", \"GDP\", \"Variaciones\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070fb47",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    The code iterates through each PDF and extracts the two required tables from each. The extracted information is then transformed into dataframes and the columns and values are cleaned up to conform to Python conventions (pythonic).\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c81935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the locale to Spanish\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "# Function to process a PDF file and generate corresponding DataFrames\n",
    "def process_pdf(pdf_path):\n",
    "    # Dictionary to store tables that meet the criteria\n",
    "    tables_dict = {}\n",
    "\n",
    "    # Counter to assign names to tables\n",
    "    table_counter = 1\n",
    "\n",
    "    # Get id_ns and year from the PDF filename\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    id_ns_year_matches = re.findall(r'ns-(\\d+)-(\\d{4})', filename)\n",
    "    if id_ns_year_matches:\n",
    "        id_ns, year = id_ns_year_matches[0]\n",
    "    else:\n",
    "        print(\"No matches found for id_ns and year in filename:\", filename)\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Replace hyphens with underscores in the filename\n",
    "    new_filename = filename.replace('-', '_')\n",
    "\n",
    "    # Date extracted from the first text of the first page\n",
    "    date = None\n",
    "\n",
    "    # Open the PDF file\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Iterate through the pages of the PDF\n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            # Extract text from the page\n",
    "            text = page.extract_text()\n",
    "            if i == 1:\n",
    "                # Get the date from the first text of the first page\n",
    "                match = re.search(r'(\\d{1,2}\\s+de\\s+\\w+\\s+de\\s+\\d{4})', text)\n",
    "                if match:\n",
    "                    # Convert the date string to a datetime object with the desired format\n",
    "                    date = datetime.strptime(match.group(0), '%d de %B de %Y')\n",
    "\n",
    "            # Check if all keywords are present in the page text\n",
    "            if all(keyword in text for keyword in keywords):\n",
    "                # Extract all tables from the page and add them to the tables dictionary\n",
    "                for j, table in enumerate(page.extract_tables(), start=1):\n",
    "                    tables_dict[f\"table_{table_counter}\"] = table\n",
    "                    table_counter += 1\n",
    "\n",
    "    # Process each table in the dictionary\n",
    "    all_dataframes = {}\n",
    "    for table_name, table in tables_dict.items():\n",
    "        # Process the sublists to create DataFrames\n",
    "        for i, sublist in enumerate(table):\n",
    "            # Check if it's the first or second sublist\n",
    "            if i < 2:\n",
    "                # Apply space replacement around the hyphen only for the first and second sublist\n",
    "                for j, item in enumerate(sublist):\n",
    "                    if isinstance(item, str):\n",
    "                        table[i][j] = re.sub(r'\\s*-\\s*', '-', item)\n",
    "\n",
    "        # Process the first sublist to define the DataFrame columns\n",
    "        columns = table[0]\n",
    "\n",
    "        # List to store DataFrames of each sublist\n",
    "        dfs_temp = []\n",
    "\n",
    "        # Process the remaining sublists to create DataFrames\n",
    "        for sublist in table[1:]:\n",
    "            # Check if the sublist element is None\n",
    "            if sublist is not None:\n",
    "                # Iterate over sublist elements and split them by \"\\n\"\n",
    "                elements = [elem.split('\\n') if elem is not None else [''] for elem in sublist]\n",
    "                # Transpose the list to group elements of the same position into sublists\n",
    "                rows = zip(*elements)\n",
    "                # Convert rows to a Pandas DataFrame\n",
    "                df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "                # Split observations into multiple columns, excluding 'SECTORES ECON√ìMICOS' and 'ECONOMIC SECTORS' columns\n",
    "                columns_to_split = [col for col in df.columns if col not in ['SECTORES ECON√ìMICOS', 'ECONOMIC SECTORS']]\n",
    "                for col in columns_to_split:\n",
    "                    df_temp = df[col].str.split(expand=True)\n",
    "                    for i in range(len(df_temp.columns)):\n",
    "                        df[f\"{col}_{i+1}\"] = df_temp[i]\n",
    "\n",
    "                # Remove the original columns that were split\n",
    "                df = df.drop(columns=columns_to_split)\n",
    "\n",
    "                # Add the DataFrame to the temporary DataFrame list\n",
    "                dfs_temp.append(df)\n",
    "\n",
    "        # Concatenate temporary DataFrames into one\n",
    "        df_final_temp = pd.concat(dfs_temp, ignore_index=True)\n",
    "\n",
    "        # Rename columns containing '_'\n",
    "        new_names = {col: col.split('_')[0] for col in df_final_temp.columns if '_' in col}\n",
    "        df_final_temp.rename(columns=new_names, inplace=True)\n",
    "\n",
    "        # Iterate over columns and remove leading underscores if necessary\n",
    "        df_final_temp.columns = [col[1:] if col.startswith('_') else col for col in df_final_temp.columns]\n",
    "\n",
    "        # Remove spaces before and after hyphens in column names\n",
    "        df_final_temp.columns = [col.strip().replace(' - ', '-') for col in df_final_temp.columns]\n",
    "\n",
    "        # Replace empty columns\n",
    "        columns = list(df_final_temp.columns)\n",
    "        for i, column in enumerate(columns):\n",
    "            if column.strip() == '':\n",
    "                # Find the left column name containing a year\n",
    "                left_name = next((col for col in reversed(columns[:i]) if col.isdigit()), None)\n",
    "\n",
    "                # Find the right column name containing a year\n",
    "                right_name = next((col for col in columns[i + 1:] if col.isdigit()), None)\n",
    "\n",
    "                if left_name and not right_name:\n",
    "                    # If there's a 4-digit number to the left and no more columns to the right\n",
    "                    df_final_temp.rename(columns={column: left_name}, inplace=True)\n",
    "                elif not left_name and right_name:\n",
    "                    # If there's a 4-digit number to the right and no more columns to the left\n",
    "                    right_year = int(right_name)\n",
    "                    df_final_temp.rename(columns={column: str(right_year - 1)}, inplace=True)\n",
    "                elif left_name and right_name:\n",
    "                    # If there are column names to the left and right containing 4-digit numbers\n",
    "                    left_year = int(left_name)\n",
    "                    df_final_temp.rename(columns={column: str(left_year)}, inplace=True)\n",
    "\n",
    "        # Replace column names with prefixes from the first row\n",
    "        new_names = df_final_temp.iloc[0].apply(lambda x: x.strip() if isinstance(x, str) else x).astype(str) + '_' + df_final_temp.columns\n",
    "        df_final_temp.columns = new_names\n",
    "\n",
    "        # Remove the first row of the DataFrame, as its values were used as prefixes for columns\n",
    "        df_final_temp = df_final_temp.drop(0)\n",
    "\n",
    "        # Convert all columns to lowercase\n",
    "        df_final_temp.columns = map(str.lower, df_final_temp.columns)\n",
    "\n",
    "        # Remove periods from column names\n",
    "        df_final_temp.columns = df_final_temp.columns.str.replace('.', '')\n",
    "\n",
    "        # Remove special characters and accents from column names\n",
    "        df_final_temp.columns = [unicodedata.normalize('NFKD', col).encode('ASCII', 'ignore').decode('utf-8') for col in df_final_temp.columns]\n",
    "\n",
    "        # Replace 'ano' with 'year' in all columns\n",
    "        df_final_temp.columns = [col.replace('ano', 'year') for col in df_final_temp.columns]\n",
    "\n",
    "        # Replace empty spaces between words with '_'\n",
    "        df_final_temp.columns = [col.replace(' ', '_') for col in df_final_temp.columns]\n",
    "\n",
    "        # Replace hyphens with underscores\n",
    "        df_final_temp.columns = [col.replace('-', '_') for col in df_final_temp.columns]\n",
    "\n",
    "        # Implementation of line that renames columns\n",
    "        df_final_temp.columns = [col[1:] if col.startswith('_') else col for col in df_final_temp.columns]\n",
    "\n",
    "        # Replace commas with periods in values of all columns except 'sectores_economicos' and 'economic_sectors'\n",
    "        for col in df_final_temp.columns:\n",
    "            if col not in ['sectores_economicos', 'economic_sectors']:\n",
    "                df_final_temp[col] = df_final_temp[col].apply(lambda x: str(x).replace(',', '.') if isinstance(x, (int, float, str)) else x)\n",
    "\n",
    "        # Convert columns to float type\n",
    "        for col in df_final_temp.columns:\n",
    "            if col not in ['sectores_economicos', 'economic_sectors']:\n",
    "                if isinstance(df_final_temp[col], pd.Series):\n",
    "                    df_final_temp[col] = pd.to_numeric(df_final_temp[col], errors='coerce')\n",
    "\n",
    "        # Get object (text string) type columns\n",
    "        text_columns = df_final_temp.select_dtypes(include='object').columns\n",
    "\n",
    "        # Iterate over text columns and remove accents\n",
    "        for col in text_columns:\n",
    "            df_final_temp[col] = df_final_temp[col].apply(lambda x: unicodedata.normalize('NFKD', x).encode('ASCII', 'ignore').decode('utf-8') if isinstance(x, str) else x)\n",
    "\n",
    "        # Convert all text strings to lowercase\n",
    "        for col in text_columns:\n",
    "            df_final_temp[col] = df_final_temp[col].str.lower()\n",
    "\n",
    "        # Define function to remove numbers and special characters\n",
    "        def remove_special_characters(text):\n",
    "            return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "        # Apply function to 'sectores_economicos' and 'economic_sectors' columns\n",
    "        df_final_temp['sectores_economicos'] = df_final_temp['sectores_economicos'].apply(\n",
    "            remove_special_characters)\n",
    "        df_final_temp['economic_sectors'] = df_final_temp['economic_sectors'].apply(\n",
    "            remove_special_characters)\n",
    "\n",
    "        # Add new columns id_ns, year, and date to DataFrames\n",
    "        df_final_temp['year'] = year\n",
    "        df_final_temp['id_ns'] = id_ns\n",
    "        df_final_temp['date'] = date\n",
    "\n",
    "        # Reorganize columns to place year, id_ns, and date at the beginning\n",
    "        column_order = ['year', 'id_ns', 'date'] + [col for col in df_final_temp.columns if col not in ['id_ns', 'year', 'date']]\n",
    "        df_final_temp = df_final_temp[column_order]\n",
    "\n",
    "        # Convert id_ns and year columns to integer type\n",
    "        df_final_temp['year'] = df_final_temp['year'].astype(int)\n",
    "        df_final_temp['id_ns'] = df_final_temp['id_ns'].astype(int)\n",
    "\n",
    "        # Save the final DataFrame with a unique name in the dictionary\n",
    "        df_name = f\"{os.path.splitext(new_filename)[0]}_{table_name.split('_')[1]}\"\n",
    "        all_dataframes[df_name] = df_final_temp\n",
    "\n",
    "    # Return the results\n",
    "    return all_dataframes, year, id_ns, date\n",
    "\n",
    "# Iterate over the PDF files in the folder\n",
    "\n",
    "# Initialize a counter\n",
    "file_counter = 0\n",
    "\n",
    "# Dictionary to store all generated dataframes\n",
    "all_dataframes = {}\n",
    "\n",
    "# Iterate over the PDF files in the folder\n",
    "for filename in os.listdir(raw_pdf):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_counter += 1\n",
    "        pdf_file = os.path.join(raw_pdf, filename)\n",
    "        print(f\"Processing file {file_counter}: {pdf_file}\")\n",
    "        generated_dataframes, year, id_ns, date = process_pdf(pdf_file)\n",
    "        print(\"Generated DataFrames:\")\n",
    "        for df_name in generated_dataframes.keys():\n",
    "            print(df_name)\n",
    "        # Add the generated dataframes to the general dictionary\n",
    "        all_dataframes.update(generated_dataframes)\n",
    "\n",
    "# Use the all_dataframes dictionary as needed\n",
    "\n",
    "        #print(\"year:\", year)\n",
    "        #print(\"id_ns:\", id_ns)\n",
    "        #print(\"date:\", date)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef487354",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataframes['ns_10_2013_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d97b8",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217183e7",
   "metadata": {},
   "source": [
    "<div id=\"3\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8989d65",
   "metadata": {},
   "source": [
    "<h1><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.</span> <span style = \"color: dark; font-family: charter;\">SQL Tables</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a04abd",
   "metadata": {},
   "source": [
    "<div style=\"font-family: charter; text-align: left; color:dark\">\n",
    "    Finally, after obtaining and cleaning all the necessary data, we can create the three most important datasets to store realeses, vintages, and revisions. These datasets will be stored as tables in SQL and can be loaded into any software or programming language.\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6ca98",
   "metadata": {},
   "source": [
    "<div id=\"3-1\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b142e",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.1.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Annual Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb99fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the names of dataframes that meet the criterion of ending in '_2'\n",
    "dataframes_ending_with_2 = []\n",
    "\n",
    "# List to store the names of dataframes to be concatenated\n",
    "dataframes_to_concatenate = []\n",
    "\n",
    "# Iterate over the dataframe names in the all_dataframes dictionary\n",
    "for df_name in all_dataframes.keys():\n",
    "    # Check if the dataframe name ends with '_2' and add it to the corresponding list\n",
    "    if df_name.endswith('_2'):\n",
    "        dataframes_ending_with_2.append(df_name)\n",
    "        dataframes_to_concatenate.append(all_dataframes[df_name])\n",
    "\n",
    "# Print the names of dataframes that meet the criterion of ending in '_2'\n",
    "print(\"DataFrames ending with '_2' that will be concatenated:\")\n",
    "for df_name in dataframes_ending_with_2:\n",
    "    print(df_name)\n",
    "\n",
    "# Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "if dataframes_to_concatenate:\n",
    "    # Concatenate only rows that meet the specified conditions\n",
    "    gdp_annual_growth_rates = pd.concat([df[(df['sectores_economicos'] == 'pbi') | (df['economic_sectors'] == 'gdp')] \n",
    "                                for df in dataframes_to_concatenate \n",
    "                                if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                ignore_index=True)\n",
    "\n",
    "    # Keep only columns that start with 'year' and the 'id_ns', 'year', and 'date' columns\n",
    "    columns_to_keep = ['id_ns', 'year', 'date'] + [col for col in gdp_annual_growth_rates.columns if col.startswith('year')]\n",
    "\n",
    "    # Drop unwanted columns\n",
    "    gdp_annual_growth_rates = gdp_annual_growth_rates[columns_to_keep]\n",
    "    \n",
    "    # Remove duplicate columns if any\n",
    "    gdp_annual_growth_rates = gdp_annual_growth_rates.loc[:,~gdp_annual_growth_rates.columns.duplicated()]\n",
    "\n",
    "    # Print the number of rows in the concatenated dataframe\n",
    "    print(\"Number of rows in the concatenated dataframe:\", len(gdp_annual_growth_rates))\n",
    "else:\n",
    "    print(\"No dataframes were found to concatenate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_annual_growth_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f8d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61bbaac2",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d9bac",
   "metadata": {},
   "source": [
    "<div id=\"3-2\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50087e95",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.2.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Quarterly Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe25cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List to store the names of dataframes that meet the criterion of ending in '_2'\n",
    "dataframes_ending_with_2 = []\n",
    "\n",
    "# List to store the names of dataframes to be concatenated\n",
    "dataframes_to_concatenate = []\n",
    "\n",
    "# Iterate over the dataframe names in the all_dataframes dictionary\n",
    "for df_name in all_dataframes.keys():\n",
    "    # Check if the dataframe name ends with '_2' and add it to the corresponding list\n",
    "    if df_name.endswith('_2'):\n",
    "        dataframes_ending_with_2.append(df_name)\n",
    "        dataframes_to_concatenate.append(all_dataframes[df_name])\n",
    "\n",
    "# Print the names of dataframes that meet the criterion of ending in '_2'\n",
    "print(\"DataFrames ending with '_2' that will be concatenated:\")\n",
    "for df_name in dataframes_ending_with_2:\n",
    "    print(df_name)\n",
    "\n",
    "# Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "if dataframes_to_concatenate:\n",
    "    # Concatenate only rows that meet the specified conditions\n",
    "    gdp_quarterly_growth_rates = pd.concat([df[(df['sectores_economicos'] == 'pbi') | (df['economic_sectors'] == 'gdp')] \n",
    "                                for df in dataframes_to_concatenate \n",
    "                                if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                ignore_index=True)\n",
    "\n",
    "    # Keep all columns except those starting with 'year_', in addition to the 'id_ns', 'year', and 'date' columns\n",
    "    columns_to_keep = ['year', 'id_ns', 'date'] + [col for col in gdp_quarterly_growth_rates.columns if not col.startswith('year_')]\n",
    "\n",
    "    # Select unwanted columns\n",
    "    gdp_quarterly_growth_rates = gdp_quarterly_growth_rates[columns_to_keep]\n",
    "\n",
    "    # Drop the 'sectores_economicos' and 'economic_sectors' columns\n",
    "    gdp_quarterly_growth_rates.drop(columns=['sectores_economicos', 'economic_sectors'], inplace=True)\n",
    "\n",
    "    # Remove duplicate columns if any\n",
    "    gdp_quarterly_growth_rates = gdp_quarterly_growth_rates.loc[:,~gdp_quarterly_growth_rates.columns.duplicated()]\n",
    "\n",
    "    # Print the number of rows in the concatenated dataframe\n",
    "    print(\"Number of rows in the concatenated dataframe:\", len(gdp_quarterly_growth_rates))\n",
    "else:\n",
    "    print(\"No dataframes were found to concatenate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_quarterly_growth_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d99fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24dd0fc0",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca271f",
   "metadata": {},
   "source": [
    "<div id=\"3-3\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453bb965",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.3.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Monthly Concatenation\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List to store the names of dataframes that meet the criterion of ending in '_1'\n",
    "dataframes_ending_with_1 = []\n",
    "\n",
    "# List to store the names of dataframes to be concatenated\n",
    "dataframes_to_concatenate = []\n",
    "\n",
    "# Iterate over the dataframe names in the all_dataframes dictionary\n",
    "for df_name in all_dataframes.keys():\n",
    "    # Check if the dataframe name ends with '_1' and add it to the corresponding list\n",
    "    if df_name.endswith('_1'):\n",
    "        dataframes_ending_with_1.append(df_name)\n",
    "        dataframes_to_concatenate.append(all_dataframes[df_name])\n",
    "\n",
    "# Print the names of dataframes that meet the criterion of ending with '_1'\n",
    "print(\"DataFrames ending with '_1' that will be concatenated:\")\n",
    "for df_name in dataframes_ending_with_1:\n",
    "    print(df_name)\n",
    "\n",
    "# Concatenate all dataframes in the 'dataframes_to_concatenate' list\n",
    "if dataframes_to_concatenate:\n",
    "    # Concatenate only rows that meet the specified conditions\n",
    "    gdp_monthly_growth_rates = pd.concat([df[(df['sectores_economicos'] == 'pbi') | (df['economic_sectors'] == 'gdp')] \n",
    "                                for df in dataframes_to_concatenate \n",
    "                                if 'sectores_economicos' in df.columns and 'economic_sectors' in df.columns], \n",
    "                                ignore_index=True)\n",
    "\n",
    "    # Keep all columns except those starting with 'year_', in addition to the 'id_ns', 'year', and 'date' columns\n",
    "    columns_to_keep = ['year', 'id_ns', 'date'] + [col for col in gdp_monthly_growth_rates.columns if not col.startswith('year_')]\n",
    "\n",
    "    # Select unwanted columns\n",
    "    gdp_monthly_growth_rates = gdp_monthly_growth_rates[columns_to_keep]\n",
    "\n",
    "    # Drop the 'sectores_economicos' and 'economic_sectors' columns\n",
    "    gdp_monthly_growth_rates.drop(columns=['sectores_economicos', 'economic_sectors'], inplace=True)\n",
    "\n",
    "    # Remove duplicate columns if any\n",
    "    gdp_monthly_growth_rates = gdp_monthly_growth_rates.loc[:,~gdp_monthly_growth_rates.columns.duplicated()]\n",
    "    \n",
    "    # Drop columns with at least two underscores in their names\n",
    "    columns_to_drop = [col for col in gdp_monthly_growth_rates.columns if col.count('_') >= 2]\n",
    "    gdp_monthly_growth_rates.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    # Print the number of rows in the concatenated dataframe\n",
    "    print(\"Number of rows in the concatenated dataframe:\", len(gdp_monthly_growth_rates))\n",
    "else:\n",
    "    print(\"No dataframes were found to concatenate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_monthly_growth_rates['date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7880a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79df4c62",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda85bba",
   "metadata": {},
   "source": [
    "<div id=\"3-4\">\n",
    "   <!-- Contenido de la celda de destino -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d3554",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: rgb(0, 65, 75); font-family: charter;\">3.4.</span>\n",
    "    <span style = \"color: dark; font-family: charter;\">\n",
    "    Loading SQL\n",
    "    </span>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Get environment variables\n",
    "user = os.environ.get('CIUP_SQL_USER')\n",
    "password = os.environ.get('CIUP_SQL_PASS')\n",
    "host = os.environ.get('CIUP_SQL_HOST')\n",
    "port = 5432\n",
    "database = 'gdp_revisions_datasets'\n",
    "\n",
    "# Check if all environment variables are defined\n",
    "if not all([host, user, password]):\n",
    "    raise ValueError(\"Some environment variables are missing (CIUP_SQL_HOST, CIUP_SQL_USER, CIUP_SQL_PASS)\")\n",
    "\n",
    "# Create connection string\n",
    "connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# gdp_monthly_growth_rates is the DataFrame you want to save to the database\n",
    "gdp_annual_growth_rates.to_sql('gdp_annual_growth_rates', engine, index=False, if_exists='replace')\n",
    "gdp_quarterly_growth_rates.to_sql('gdp_quarterly_growth_rates', engine, index=False, if_exists='replace')\n",
    "gdp_monthly_growth_rates.to_sql('gdp_monthly_growth_rates', engine, index=False, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132cb43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15cd18b4",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(61, 48, 162); font-size: 12px;\">\n",
    "    Back to the\n",
    "    <a href=\"#outilne\" style=\"color: #687EFF;\">\n",
    "    outline.\n",
    "    </a>\n",
    "    <div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc920fd",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eae11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf105ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdp_revisions",
   "language": "python",
   "name": "gdp_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
